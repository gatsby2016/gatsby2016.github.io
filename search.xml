<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[2022支离破碎2023会好吗]]></title>
    <url>%2F2022%2F12%2F31%2F2022%E6%94%AF%E7%A6%BB%E7%A0%B4%E7%A2%8E2023%E4%BC%9A%E5%A5%BD%E5%90%97%2F</url>
    <content type="text"><![CDATA[2022年该是特别难忘的一年，我想，大家都是这个感触。 去年在年终总结里写道—— 尽管这样，疫情始终没有完全消散，一直在国内各地零星爆发，反反复复。我们疫苗两针打了，德尔塔变异来了，加强针又打了，眼下小孩子们两针疫苗都打完了，奥秘克戎又来了。而我们，也已经开始逐渐适应、习惯乃至麻木。也许过不多久，新闻报道会说世卫组织宣布新冠疫情彻底消失。那时我们会喜会乐还是无动于衷？不过倒也不必想这么远，因为谁知道呢，或许他们宣布的是新冠疫情将“流感化”，将长期存在呢？ 没想到一语成谶。 家里父母姐上周就已经阳了。眼下我对象27号也刚阳，一周转阴期还没过。所幸目前亲友都没有进一步严重的症状。值得更幸的是，我还坚挺。大家对我的评论普遍是，挺到现在挺不容易的。确实，还能坚持多久呢？ 但是就这样一轮快速感染高峰后，未来就会好么？我想起前两天我对象发烧时她不爱测体温，她说害怕看到体温，怕烧太高了。我说你这不是掩耳盗铃么？不测体温，烧着谁也不知道对吧，总是烧着，也不太可能闭着眼睛睡一觉明天就生龙活虎了吧？硬挺？体格还没强壮到那地步啊。啥也不做，明天的情况只会更糟啊。还是得有点干预措施。 Anyway，还是希望未来我们都身体健康，平安。 还是说回我自己。 对我来说，今年“特别之处”在支离破碎。这种支离破碎，换个意思说是毫无收获。我觉得这么总结没有问题—— 一二月，处在上班下班等放春节假的焦急节奏中。结果，在放假前一天等到了Omicron病毒疫情突袭杭州。当时犹豫再三反复斟酌抉择，最后决定不回家了。由此，26年来第一次在外地过春节。不过不凄凉，爸妈第一时间寄来两箱坚守物资，和对象两人大年夜搞了六菜一汤年夜饭。饭后我向她普及北方大年夜传统——包饺子喽。非常有意义的一次“驻外过年”。 三四月，工作之余准备博考面试和报名的紧张。终于，莫名其妙就上岸了。侥幸，幸运，运气……加一丁丁点的付出。直到后来正式名单公示，我心里一直都忐忑不安，因为非常出乎意料，也怕历史再一次重演。 五六七八月，正式确定后辞职开始一段自由时光。大部分时间无效地混迹于滨江图书馆，你说看书吧，没看几本，不成系统。你说看paper吧，没看几篇，浅尝辄止。总之，那一阵，心浮气躁，又有点心高气傲。直到现在，还是没有完全从那种困境中完全走出来。或者换句话说，这后面的半年，算是白瞎折腾了。 九十十一月，就是一眨眼的工夫。从杭州到天津，从天津到南开，……（省略三个月），从南开到天津，又从天津回杭州。就是这么闭环。要知道，这次说的在学校，就是真正的在学校。三个月没出校园，不知万国之都天津什么繁华都市容貌。但“扎根校园”，确实是让人耳目一新——校园风貌，令人顿生宽广胸怀；体育娱乐活动，强筋健骨而又丰富多彩；授课教学，令人受益匪浅学有所思；同学新友，热情大方博学多闻；院系教师，和蔼可亲循循善诱。这些都给人鼓舞，让人倍感自豪；我想，这些也许足够我平衡其他为数不多让人感到失落的地方了。 尽管短短三个月，但是在学校内认识了很多很多人，也发生了很多很多事，更有一些无法言说的或抱怨或退缩的情绪恣意流淌。也许2023年回到学校，它们还会扑面而来。但在2022年最后一天我读到这样一段话—— 回避问题和逃避痛苦的倾向，是人类心理疾病的根源。逃避痛苦的替代品最终带给人的痛苦，甚至比所逃避的痛苦更为强烈。如果不顾一切地逃避问题和痛苦，不仅错失了解决问题、推动心灵成长的契机，而且还会使我们患上心理疾病。 所以大胆估计，2023年，还会是直面问题，直面痛苦的一年。要想一切法子，直面问题并解决它。 临近十二月，倒没啥可以回顾，无非是在月初疫情完全放开前被封在学校宿舍几天，期间做了回志愿者为学校出点力。然后就慌促回杭，在彻底放开后的焦虑情绪中，苟在房间，写着学期课程作业。时而遗憾自己在有些事情上“看到，却没做到”，时而担忧未来何去何从。然后在不知不觉中，finish了我的2022。 Anyway，2022好也罢差也罢，也就这么过去了。翻看相册，还是有很多开心的、甜蜜的、珍贵的故事令人感动，给人力量，无法一一言表，只能放在心间珍藏。往者不可追，希望接下来的2023，更完整，有收获。 以及，看到，并做到。]]></content>
  </entry>
  <entry>
    <title><![CDATA[读入门级明史]]></title>
    <url>%2F2022%2F08%2F25%2F%E8%AF%BB%E5%85%A5%E9%97%A8%E7%BA%A7%E6%98%8E%E5%8F%B2%2F</url>
    <content type="text"><![CDATA[最近着实有些嚣张了——无业游民一个，天天混迹于杭州图书馆，趿拉着拖鞋，虽未蓬头垢面，但也是头发凌乱了。而最关键的是，我并不像图书馆里的大部分人一样用功自修，天天尽读闲散书籍。以至于入门专业知识抛诸脑后，学术技术知识遗忘殆尽。 而今天，做了这样一件事——花了一整天的时间读了《一读就懂的中国史——明朝》，像看故事，从朱元璋元末起义开始，到1644年崇祯帝自缢。近300年间，明帝国起起伏伏——有永乐盛世，仁宣之治，嘉靖中兴，隆庆开关，万历中兴，也有“清君侧”靖难之役，土木堡之变，夺门之变，壬寅宫变， 更有宦官干政，权臣乱朝纲，边境进犯，农民起义，藩王谋反，东林党争……一句话，实在难以说得了整个明朝大小事件，一天，也遑论读完明朝历史，只能说是简单粗略地了解了。 我无意要说明史，不敢班门弄斧，更没那本领。 我想说的是，我发现的历史的特点—— 让人回味无穷。因为历史就在那里，伴随着那段时间的流淌在某个空间发生了。然后就永远定格了。我们可以说这是客观的，历史就是客观事实。相反，人作为独立的个体，每个人在客体前则会呈现主观的思维。巧妙就在这里：当我们去读历史，那些已发生的事实就呈现在我们面前，我们自然而然对事件、人物产生主观情感，或敬佩，或憎恶，或同情，或悲悯……也会有评判：或善或恶，忠良奸佞…… 尤其是，一个人总不会永远善良，也不会永远坏，人是立体的。祖训祖制也不一定金科玉律，捧为圭臬，也可能是沉重包袱，时代是变化的。所以，这就导致历史，在空间层面，对于每个读者，都让他们能回味无穷；在时间层面，让读者在生活的每个阶段，总能回味无穷。 让人意犹未尽。历史不是小说故事，不会因为读完了故事，它就结束了再也没有了，进而只能靠自我揣测自我想象。相反，这本历史书虽读毕，可是我知道，关于这段历史，还有更多更丰富的记忆等着我去开启，就像在海边捡拾贝壳，捡到了一个，就知道在这片海滩还有更多贝壳，而且可能永远也无法捡拾完。历史的这个特点真是让人欲罢不能，你越去了解，它就越丰富，越令你着迷，就越促使你去找更多书更多资料去探究更多。可当你探究了很久很多之后，回头一看，好像也并不是非探不可的事情。但你又细细想来，那种了解历史时一边感到充实一边感到好奇的感觉，总不能辜负，总值得付出。 这两个特点不是历史本身固有的，历史本身似乎也谈不上什么特点，无非就是客观、厚重。但我以为，历史最有意义的，恰恰是它与读者碰撞交流互动后散发出来的芳香——让人回味无穷，让人意犹未尽。这才是历史的特点，鲜明而独特。]]></content>
  </entry>
  <entry>
    <title><![CDATA[曾经的友谊地久天长]]></title>
    <url>%2F2022%2F08%2F23%2F%E6%9B%BE%E7%BB%8F%E7%9A%84%E5%8F%8B%E8%B0%8A%E5%9C%B0%E4%B9%85%E5%A4%A9%E9%95%BF%2F</url>
    <content type="text"><![CDATA[这是一个反思自省认识的帖，想了想不该写，但还是写了。 关于真挚的情感，关于以前的友谊，也关于认识世界。 最近接二连三的朋友间拜访与被拜访，开始让我意识到人生就是一条河流，看起来无声无息，实则暗潮涌动。 上学时，包括毕业后一年，都很天真。总觉得和一个人在一起玩的很好的话，这种感情应该会一直持续下去。哪怕后来我们分开去追求自己的人生，好多年不见，但总会在闲暇时找机会见上一面，并且一旦见上一面便会格外珍惜，以前无话不说的感觉就会回来。这种状态是最理想的，当然，这种状态的前提是始终互相关心，互相挂念，互相重视。 但这终归是理想的，理想就意味着，不现实。 先说一件我自己没做好的事情—— 大学时期我有一个很好的朋友，两人关系很好，简直是如胶似漆。但是好景不长，我谈了女友，沉迷恋爱，和他的交往自然就少了，而且很奇怪他那时也极力避免和我们俩接触，这就导致友谊很快降温。后来他参加工作，我继续读书，过年时一次微信交流中，他向我半坦白半征求地说到他的性取向问题以及他的感情问题。我当时自然想到他以前对我是否也是这种感情，但想到我恋爱后他就远离我们了，所以我还是把他当成很好的朋友，后来也在一起吃过饭。但这件事和我女友说了之后她就非常反对我再和他交往，虽然我进退两难，但还是因为一次pdd助力的小问题，彻底加速了我和他的破裂。至今未再联系。 我很难再想象他当时对我的爱情也好友谊也罢，现在一旦想起，总归是心灰意冷、失落吧。 就像现在我仅剩不多的某些挚友中，对我的状态。谈不上彻底破裂，心灰意冷，但总归不那么畅快。事情看起来也很简单，似乎可有可无。 我也一度在想，是不是我哪里没做好？是我太热情？太直接？还是我太敏感？ 但我的热情，是我对待挚友的一贯性格。我的直接，也是我对以前的彼此无话不说的状态的延续。至于我的敏感，恰恰是我对挚友的行为无法理解后不得不做的小心推测。 我也在想象哪几种可能—— 是因为挚友都有了伴侣，开始照顾家庭进而需要兼顾女友的感受？或者是新时代男青年要把伴侣的要求作为第一要务？ 是因为挚友有了新的环境新的好友，抑或有了更成熟的交际圈而对以前鄙夷？ 还是因为大家有了更成熟的心灵更伟大的事业而乏于人际琐碎繁文缛节？ 茫然，感慨，令人唏嘘。 人生真是一条河流，暗潮涌动下，永远奔腾向前。 后来我转念一想，世界尽管是这样，但也有我的问题。不卑不亢而言，我总是太卑，该是这而为人所不齿所不交吧。]]></content>
  </entry>
  <entry>
    <title><![CDATA[读莫言的《生死疲劳》]]></title>
    <url>%2F2022%2F08%2F05%2F%E8%AF%BB%E8%8E%AB%E8%A8%80%E7%9A%84%E7%94%9F%E6%AD%BB%E7%96%B2%E5%8A%B3%2F</url>
    <content type="text"><![CDATA[前段时间在图书馆一口气读完了余华的《活着》，当时即泪眼婆娑，感叹福贵一家命运的悲惨。起初以为是那个时代特有的苦难，后来一想，哪怕现在，有些悲痛也从未离去，只是我们未曾得知。 最近，先是有一气没一气的读《生死疲劳》，结果越读越起劲，最后干脆一不做二不休地，在图书馆分多口气读完了。 如果说，《活着》是直面现实的悲惨，那《生死疲劳》，则是在幽默戏谑中，面对人生与时代的跌宕起伏。跨越半个世纪的长度，地主西门闹先后投胎为驴牛猪狗猴人，从一个六道轮回的角度，揭开个体与群体的生活的面纱。当一切落幕，终究，“一切来自土地的都将回归土地”。 令我最为唏嘘的，是这样一件事—— 蓝解放人到中年，已位居副县长职位。不知何故，竟与年仅20岁的庞春苗一见钟情，坠入爱河，甘愿为爱情牺牲所有。他主动放弃了前途，连名声也不要了。甚至，家庭孩子也甘愿抛弃，主动提出离婚。妻子黄开放半辈子为他操持，难道最后却要落得如此结果？于是她放下狠话：除非我死，否则你休想离婚！ 放弃了。蓝解放彻底放弃了与世俗观念的挣扎。与庞春苗从山东高密乡私奔到西安，爱情支撑着两人互相扶持彼此慰藉。 这是浪漫的旅程也是苦难的历程；这是无耻的行径也是高尚的行为；这是退却也是进攻；这是投降也是抵抗；这是示弱也是示威；这是挑战也是妥协。莫言如是评价。 然而最终，命运还是成全了他们——没过几年，黄开放患上癌症，命不久矣。蓝解放庞春苗两人因此踏上了返乡的火车。回到西门屯，开放躺在床上，解放春苗，泪流满面。开放淡淡说：“我没有几天熬头了，你们也不用东躲西藏了……也是我糊涂，当初为什么不成全了你们呢……” 漫漫一生，临了皆风轻云淡。黄开放当初心底那样的倔强与狠劲，最后，被时间抹平了，也被濒死掩埋了。莫言写道是人将死恩仇并泯。 那个时代，也未曾让人感到安慰半分，不过是个你方唱罢我登场的舞台。也正是它，造就了山东高密乡所有出场的未出场的面孔后人生的起起伏伏、草野浮沉。而那段故事，又何止发生在山东高密呢？ 所以，来自大地的我们，虽终将回归大地，但过好一生这个命题，究竟是由自身命运还是所处的时代所指导呢？ 在我对人生与时代慨叹之后，让我继续回忆叙述一段往事—— 高二那年，也就是莫言在领到诺贝尔文学奖奖状之后，莫言的作品在门口书摊大火。出于尊敬，我买了他的一本现在忘记具体叫什么名字的小说，在课堂间隙认真拜读。读了约莫几页，一处大段露骨描写，“大作家果然不慎避讳，甚至文采斐然”，我想。继续读，约莫几页，又是一处，“应该是故事情节发展需要”，我抿嘴点头。继续读，又来，“这情节发展节奏真快，可是描述是不是太多了些？大作家都这样？”。索性，一目十行，快速往后翻，好家伙，那男女欢愉之事描写真是马不停蹄映入我眼帘。很快，一本书翻完了。露骨描写的内容对我的心灵冲击较大，于是我骂了一句狗屎小说后，将书扔到了垃圾桶。 好巧不巧，不知道是怎么搞的，这本书被教语文的卢老师捡到了。然后他在课堂上问是谁把书扔了？我举手站起。问我为什么把书扔了？我如实回答：不知道书里写的什么乱七八糟。虽然我那时候本来就不让老师省心，但这事卢老师没有批评我，只是意味深长地对我们说：可以理解啊，你们还小，有些内容看不懂很正常；再一个，你们每个人都有自己的阅读喜好，不强求。 奇怪，上述文字的感觉竟有点《生死疲劳》文字的幽默样式了。可见，读书的作用性和文字的影响力有多大。当然，这种作用范围也许仅仅体现在文字上，也或许能对其他创作风格形成影响。有机会可以一试。说远了。 那本书的内容我现在已经忘了一干二净。也可能当时就没记住什么。但是卢老师的这番话却始终很清晰。这话听着让人感动，受益匪浅，我至今也很敬佩他。 但我说这事的目的不是重温感动，而是说这本书让我形成了对莫言的不好印象。加之他那模样，使我经常将他与《熔炉》里的那个垃圾校长联系到一块。 所以，当我在那件事情的十年后鼓起勇气拿起他的《生死疲劳》并几口气读完后，我就在想，呵，看来这老头子也不是只会重墨描写那些男男女女的事情的。 我开始对他表示钦佩。心想这是否就是魔幻现实主义的手法？把苍白的历史纳以生命，用笔锋勾勒，旋即变成了血淋淋的现实。]]></content>
  </entry>
  <entry>
    <title><![CDATA[人类简史这本书中的全新观点]]></title>
    <url>%2F2022%2F08%2F02%2F%E4%BA%BA%E7%B1%BB%E7%AE%80%E5%8F%B2%E8%BF%99%E6%9C%AC%E4%B9%A6%E4%B8%AD%E7%9A%84%E5%85%A8%E6%96%B0%E8%A7%82%E7%82%B9%2F</url>
    <content type="text"><![CDATA[《人类简史》这本书的观点对我来说简直是颠覆性的。 读完这本书已经有两周，虽说读的时候就已经被部分观点所冲击，但读完之后，那种游离社会之外、满眼虚空、不知人生当为何物的感觉更为强烈，尤其在搭配着我“自我闭关、社会隔离”、并且先读完《三体》的这段时间。直到两周之后的现在，当我企图去写些感受感悟的时候，我一度不知从何而起。一来，我的思维的确还处于混乱的状态，尚未形成清晰脉络，触及现实种种，皆持质疑之态；二来，即便动笔，所写的内容涉及的名词、概念从何而来？是否经得起推敲，经得起深究？ 这种影响真是深刻的，巨大的。 读三体，我仿佛遨游到太阳系银河系之外，与宇宙不同的文明接触、碰撞、交流。人类渺小不值一提。读人类简史，我又回归人类演化发展、寻求人类本质。一边见证人类从几百万年前进化至今，经历认知革命、农业革命、工业革命、科技革命，一边又慨叹人类实体虽在，命运却也虚无，社会、秩序、理念、思想不过都是虚构想象，内心撕裂却无力，只好立足当下。回头看生活种种，竟不知以前观点为何为何。震撼之中，深感颠覆。 想象所构建的秩序帝国、资本与科学生命的意义不过是认识你自己]]></content>
      <categories>
        <category>life</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[离职感言与钱]]></title>
    <url>%2F2022%2F06%2F14%2F%E7%A6%BB%E8%81%8C%E6%84%9F%E8%A8%80%E4%B8%8E%E9%92%B1%2F</url>
    <content type="text"><![CDATA[钱是高尚的，还是粗鄙的？ 关注钱本就是个无可厚非的事情，不谈钱谈理想，不是在扯淡就是在忽悠人。对于绝大部分人来说，钱都是通过工作所得。所以，想说一说钱与工作。 其实，钱不应该是我们工作的全部，因为薪酬都是和直接职级挂钩的，所以我们更应该关注我们的职级，我们目前在这个岗位处于什么层次。只有职级上来了，工资、钱自然而然都会跟上来。那这时候问题就变成了，如何提高我们的职级。很显然，对于私企民营企业外企来说（这里抛开国企、事业性单位以及相关政务系统，掺杂因素太复杂，而且我也没有切身体会），只有一个衡量标准：你的贡献。所有企业，要在市场生存下来，都需要盈利。这也就解释了为什么所有公司的口号、愿景、企业价值观，都在讲高效、质量、产品价值，因为在残酷的市场竞争下，最终目标都要盈利。而你的贡献，就在直接间接的支撑公司去盈利。贡献又可分为两个方面（虽然这两个方面通常是相互依存的）：其一，是你已经作出的贡献；这个是实打实可见的；其二，是基于你的能力，你的潜力，判断你可以做出贡献。举个例子，企业进行应届生校招，工作匹配度通常不需要太大，工作经历也不需要相关，他们只是综合你的学生经历判断你的潜力，至于其他，都可以在工作环境中培养；而企业进行社招，通常看工作匹配度和工作经历，因为他们需要判断你之前做出了什么贡献，以及来了之后能否快速产出？就像企业内部职级晋升一样，着重看你做了哪些贡献，然后研判你的能力和潜力。而这能力往往无法定量化，所以最终评判重点还是落到了你之前的贡献中。可是转念一想，员工之所以能产出贡献，不正是因为员工的能力和潜力么？所以，想加薪，别做梦。先提高自己的职级再说，这关键就是要提高自身的能力。技术出身的人很容易想，那我苦练技术，多多跟进前沿，提高自身能力。是对的，但是到一定阶段就不太对。因为量变的累积会产生质变。当你技术职级越来越高后，说明你的技术能力受到了公司的认可。这个时候，公司不再满足于你一个人的贡献，毕竟一个人的能力是有限的，而且某一方面的贡献仅靠你一个人也是危险的。他希望你的能力能分享出去传递下去，通过你一己之力，指数发散，带动更多人做出贡献。也就是质变到管理人才岗位（这个管理不是纯管理，而是类似技术经理研发主管这种性质）。这一步，其实是技术累积的必然，但也是技术累积的黑洞。一旦你到达这个位置，一方面，你阶跃到一个更高的维度，在这个维度下，你势必又要从零开始往上走；另一方面，你已有的技术累积的维度能量会被慢慢消耗，因为这个维度下有更多的低职级低维度员工等待你去管理和培养。质变这一步很关键。如果质变成功，恭喜！开启新的人生阶段。否则，随着年龄到某个数字，你就会被优化掉或者被边缘化。为什么？假如你技术不到位，是不是该优化给年轻人机会？假如你技术到位但是没质变到管理，那你的技术相比薪酬而言性价比已经不高。假如质变到管理岗位但是失败了（啥叫失败，就是你主导负责的几个项目接连都流了或严重延期或上线后出现严重风险），你说那还有戏么？总不能再下放做技术？总而言之，这才是程序员残酷的35岁守恒定律。所以，努力去追逐职级上升吧。每一步都要加油啊。 所以，钱是高尚的，还是粗鄙的？当你追逐上升，就是高尚的；当你追逐钱，就是粗鄙的。]]></content>
  </entry>
  <entry>
    <title><![CDATA[谈一点想说的]]></title>
    <url>%2F2022%2F04%2F22%2F%E8%B0%88%E4%B8%80%E7%82%B9%E6%83%B3%E8%AF%B4%E7%9A%84%2F</url>
    <content type="text"><![CDATA[以前我们说：群众的眼睛是雪亮的。现在可能不是这样，相反，群众的眼睛太容易被蒙蔽了。 两个问题一个是舆论。舆论真是一把双刃剑，好的，它形成了一定力量的对上监督；坏的，信息真真假假难以分辨，太容易被灌输。 另一个是上海，上海这次的疫情现状真的魔幻，很难不让人大跌眼镜。这次的确折射很多治理问题，从上到下，有问题没有，绝对有，并且很多。这次疫情消停后必须对上海来一套大调整。接着说，但是目前，这些问题被舆论放大后，呈现到了群众的视野中，尤其是疫情严防死守直接导致个人利害关系的群众，直接点燃了大家的情绪；非利益关系的心里可能也有疑惑，或者几句抱怨，或者动摇。这种疑惑和情绪在大家心里扎根，是非常非常危险的。这不是说，上海疫情控制住就完事了的情绪，这次可以是放大上海疫情乱象，下一次就有其他乱象被夸张，多来几次，对我们社会来说十分危险。当然我们国家确实还是有一些小毛病乃至更严重一点的问题存在的。小毛病任何一个社会都会有，更严重的，是经济、社会、时代高速发展的偶然或必然，这些背离宗旨的，他们一定会被解决也一定要被解决。但是个案问题极易被“有心人”利用进行煽动，而大家还不自知。这十分危险，这是关系到整个国家整个体制的。可能大家会觉得言重了，一点不为过。永远不要低估每一位人民群众生命个体在社会主义中国的重要性，以及，永远不要低估国内伪高精分子敌特分子隐藏分子的隐蔽性、煽动性和持续性。 应该说，我们国家，目前国内外都处在非常非常关键的时期。而且又有半年后的换届时机。这个时候，一定要谨慎再谨慎，坚定再坚定，作战打仗，军心一定要稳。只要我们内部是铁板一块，外部就不能奈我何。]]></content>
  </entry>
  <entry>
    <title><![CDATA[Thesis for my master degree]]></title>
    <url>%2F2022%2F04%2F20%2FThesis-for-my-master-degree%2F</url>
    <content type="text"><![CDATA[Hi, my master thesis has been uploaded for backup. See HERE. It can also be accessed on CNKI.NET]]></content>
      <tags>
        <tag>master thesis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[主流车道线数据集的车道线评估方法]]></title>
    <url>%2F2022%2F02%2F09%2F%E4%B8%BB%E6%B5%81%E8%BD%A6%E9%81%93%E7%BA%BF%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E8%BD%A6%E9%81%93%E7%BA%BF%E8%AF%84%E4%BC%B0%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[Tusimple数据 (point-based评估) 车道线上点的accuracy及车道线的FP FN rate 评估代码 基本思路模型预测输出车道线$L_{set}$，对于第$a$条车道线，我们按$y$值从大到小排序，假设为$L^{xy}_a=\{(x_0, y_0),…,(x_t, y_t)\}$；进行二次采样并输出y_sample$=\{Y_0,…, Y_k\}$下对应的$x$轴坐标$L^x_a = \{\dot{x}_0,…, \dot{x}_k\}$。确定所有预测车道线$L^x_{set}=\{L^x_0, …, L^x_n\}$，然后$L^x_{set}$与第$p$个GT车道线$L^{gt}_p$进行逐一比较，确定当前GT车道线$L^{gt}_p$的$\{acc^0_{p}, …, acc^n_{p}\}$。 其中，当前图像上预测车道线数量为$n$。$L^{xy}_a$为初始预测的某一车道线。y_sample为Tusimple数据集预定义的$y$轴方向上的采样点集合，共有$k$个采样位置。二次采样方式可以是重采样或离散采样，根据$L^{xy}_a$确定。二次采样时，对于y_sample区间内，$\{y_0,…,y_t\}$区间未包含部分对应的$x$坐标置为$-2$。而$\{y_0,…,y_t\}$区间超过y_sample区间部分直接忽略不计 PS: $L^{xy}_a$非必要，仅针对模型不按y_sample输出车道线点的情况。如果模型直接输出$L^x_a$则不需要二次采样。 车道线$L^x_a$与第p个GT车道线$L^{gt}_p$计算$acc^a_{p}$过程：def line_accuracy(pred, gt, thresh): # thresh阈值根据车道线斜率确定，斜率越大，thresh越大 pred = np.array([p if p &gt;= 0 else -100 for p in pred]) gt = np.array([g if g &gt;= 0 else -100 for g in gt]) return np.sum(np.where(np.abs(pred - gt) &lt; thresh, 1., 0.)) / len(gt) A point is correct when the difference between a ground-truth and predicted point is less than a certain threshold. 进一步地，根据GT车道线$L^{gt}_p$与所有预测车道线的$\{acc^0_{p}, …, acc^n_{p}\}$确定最佳匹配，假设为$acc^a_{p}$，判断其是否大于pt_thresh = 0.85，进而确定TP or FN +1，单图像因此可以确定FN rate and FP rate(这里FP rate与通常定义的FP rate不一致)。 fp, fn = 0., 0.matched = 0.for x_gts, thresh in zip(gt, threshs): accs = [LaneEval.line_accuracy(np.array(x_preds), np.array(x_gts), thresh) for x_preds in pred] max_acc = np.max(accs) if len(accs) &gt; 0 else 0. if max_acc &lt; LaneEval.pt_thresh: # pt_thresh = 0.85 fn += 1 else: matched += 1 line_accs.append(max_acc)fp = len(pred) - matchedif len(gt) &gt; 4 and fn &gt; 0: fn -= 1 最后，所有样本的mean(单图像上所有GT车道线的mean($\{acc^0_{p}, …, acc^n_{p}\}$))，作为整个数据的车道线点的accuracy。所有样本的mean(FP rate)和mean(FN rate)作为整个数据的车道线FP FN rate。 All the three reported metrics reported as the average across all images of the average of each image. CULane and CurveLanes数据 (region-based评估) 车道线precision, recall and F1 score 评估代码 基本思路将单图像GT和预测车道线扩展为宽度为30的线区域，然后逐一计算pred和gt的IoU。进行最佳匹配。 判断其IoU大于某个阈值（定义0.3为loose、0.5为strict）视作一个TP，进而也确定FN及FP，对数据集内所有图像遍历并累积TP、FP、FN，最后确定整个数据的precision、recall、F1 score。 vector&lt;vector&lt;double&gt; &gt; similarity(anno_lanes.size(), vector&lt;double&gt;(detect_lanes.size(), 0));for(int i=0; i&lt;anno_lanes.size(); i++)&#123; const vector&lt;Point2f&gt; &amp;curr_anno_lane = anno_lanes[i]; for(int j=0; j&lt;detect_lanes.size(); j++) &#123; const vector&lt;Point2f&gt; &amp;curr_detect_lane = detect_lanes[j]; similarity[i][j] = lane_compare-&gt;get_lane_similarity(curr_anno_lane, curr_detect_lane); &#125;&#125;double LaneCompare::get_lane_similarity(const vector&lt;Point2f&gt; &amp;lane1, const vector&lt;Point2f&gt; &amp;lane2)&#123;Mat im1 = Mat::zeros(im_height, im_width, CV_8UC1);Mat im2 = Mat::zeros(im_height, im_width, CV_8UC1);// draw lines on im1 and im2//......Scalar color_white = Scalar(1);//扩展点集为图像上'线区域'for(int n=0; n&lt;p_interp1.size()-1; n++)&#123; line(im1, p_interp1[n], p_interp1[n+1], color_white, lane_width);&#125;for(int n=0; n&lt;p_interp2.size()-1; n++)&#123; line(im2, p_interp2[n], p_interp2[n+1], color_white, lane_width);&#125;//计算IoUdouble sum_1 = cv::sum(im1).val[0];double sum_2 = cv::sum(im2).val[0];double inter_sum = cv::sum(im1.mul(im2)).val[0];double union_sum = sum_1 + sum_2 - inter_sum; double iou = inter_sum / union_sum;return iou;&#125; 逐图像累积TP、FP、FN数量，并计算整个数据指标long tp = 0, fp = 0, tn = 0, fn = 0;for (auto result: tuple_lists) &#123;tp += get&lt;1&gt;(result);fp += get&lt;2&gt;(result);// tn = get&lt;3&gt;(result);fn += get&lt;4&gt;(result);&#125;counter.setTP(tp);counter.setFP(fp);counter.setFN(fn);double precision = counter.get_precision();double recall = counter.get_recall();double F = 2 * precision * recall / (precision + recall); VIL-100数据 综合上述point-based和region-based的评估指标 一篇论文[2014] On Performance Evaluation Metrics for Lane Estimation 针对单车道线$L$而言，lane position deviation (LPD): 定义为在y轴方向的$y_{min}$到$y_{max}$区间范围内等间隔$N$采样，每一采样点处$x$坐标偏差$\delta$在所有采样点处的累计和。 \delta_{LPD}=\frac{1}{y_{max}-y_{min}} \sum_{y=y_{min}}^{y_{max}} \delta_{y}写在最后车道线的评估本质上是线段的比对和匹配问题。其实就五步： 比对单个GT线和单一预测线，这时候比对可以是基于offset或者基于IoU或者直接用阈值指示函数； 遍历所有预测线； 找该GT线的最佳匹配，并基于某个限定条件确定TP or FN； 按上述遍历所有GT线； 确定所有TP FN进而确定整个数据TP and FN and FP，进而确定最终指标（或者也可以不加限定条件，直接根据最近匹配值作为最终指标）。 但是，一个可能的边界情况是：GT线$L^{gt}_p$最佳匹配是预测车道线$L^{pred}_s$，下一个GT线$L^{gt}_q$最佳匹配也是预测车道线$L^{pred}_s$；假设都满足限定条件，$TP+2?$所以，最佳匹配也需要是唯一匹配。$L^{pred}_s$一旦成为$L^{gt}_p$的满足限定条件的最佳匹配，$TP+1$的同时也应该$L^{pred}.pop(s)$。]]></content>
      <tags>
        <tag>车道线评估</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[说废话的几点原因]]></title>
    <url>%2F2022%2F01%2F09%2F%E8%AF%B4%E5%BA%9F%E8%AF%9D%E7%9A%84%E5%87%A0%E7%82%B9%E5%8E%9F%E5%9B%A0%2F</url>
    <content type="text"><![CDATA[今天和老婆大吵了一架，原因是我说话她抢我话；而她抢我话的原因是她觉得我说的太多废话。我解释说我废话多是因为你老在打断我说话，而我又要重新开始说。就这样双方开始拉锯。 很有意思的矛盾体。 首先分析了为什么她老爱抢别人说话：她说她下意识地脱口而出。为什么会下意识？因为她理解了我的意思进而想给我做出补充或者反驳我的观点。那我问她，我话都没说完你为什么会理解我的意思？她答是因为我好多话说了一遍又一遍，反复说，废话多。 经过回想以前多个场景，我确定我的确在这方面有问题——废话太多。 为什么？ 我分析有以下几点原因—— 研究生时跟导师学做学问，他是工科，合作的大多是医学病理，知识上互有壁垒，导致他经常要反复讲某些概念对方才能听懂；我自然而然耳濡目染，也会对接一些病理医生，因此很多问题概念会反复讲； 和上一个很像，和导师沟通时知识上也会有壁垒存在，他不太能理解算法实现细节，但偏偏又很想搞懂，导致我渐渐会重复讲一些内容； 研究生组会过于频繁，往往因为摸鱼导致工作没有实际性进展就又要汇报，自己却又不能显得没有进展，所以组会汇报会故意冗长化，内容自然就重复起来了。 自己好像很喜欢“循循善诱”状，总会有意无意地拔高地位，进而出现讲完一句又尝试再补充或抽象概括一遍。 这应该就是我现在废话多的四点原因，怎么改正？少说话？不现实。 考虑有三： 说话前，脑子有个框架要说哪些内容，列个提纲。先把每一点说完，然后看对方反映再确定是否需要逐一解释。 说话尽量一句话说完。当感觉自己又不经意反复说某些内容的时候，立马停止，问对方能理解吧？ 避免升华！日常交流不要抽象不要文绉绉。必须避免高站位，要接地气。 以上，自勉。]]></content>
  </entry>
  <entry>
    <title><![CDATA[2022元旦感想]]></title>
    <url>%2F2022%2F01%2F01%2F2022%E5%85%83%E6%97%A6%E6%84%9F%E6%83%B3%2F</url>
    <content type="text"><![CDATA[真快啊，马上2022年了。以前觉得很多不会发生的，不可思议的事情，都一一成为现实。 比如—— 2021年新冠疫情还在流行。2020年初发生的事情，在家蹲了两个月，然后回校封闭。那时正值五六月份，疫情完全被控制住，但学校始终不放开校门自由出行，最亲的师兄又临近毕业，所以我们总是在打探学校哪面墙有洞哪块栏杆低矮，试图钻出去搓一顿两顿三顿。 这说话间，时间就走到了这里，已经记不清钻了多少个洞翻了多少个栏杆。而我自己，也都已经毕业来杭州半年了。尽管这样，疫情始终没有完全消散，一直在国内各地零星爆发，反反复复。我们疫苗两针打了，德尔塔变异来了，加强针又打了，眼下小孩子们两针疫苗都打完了，奥秘克戎又来了。而我们，也已经开始逐渐适应、习惯乃至麻木。也许过不多久，新闻报道会说世卫组织宣布新冠疫情彻底消失。那时我们会喜会乐还是无动于衷？不过倒也不必想这么远，因为谁知道呢，或许他们宣布的是新冠疫情将“流感化”，将长期存在呢？ 说到我，做饭从零到一成就达成。以前在学校的时候，特别期待着以后能有机会自己做饭，期待到什么程度呢？网上看怎么做爆炒花甲、酸辣土豆丝和西红柿蛋汤，然后自己一边思考整个流程一边写到便利贴上，持续看了一两周，基本做到滚瓜烂熟了，不熟不行，毕竟这两菜一汤正好是凑成一顿饭了嘛！后来去师兄家里玩，明明说好了晚上吃火锅，我非要整个土豆丝给大家露一手，结果买了土豆后发现家里没有刨子，而我又没有系统学过怎么切丝啊。。。折腾了半个土豆后发现，尼玛回去还真得系统性学习一下怎么切丝。最后只好厚薄不一地切了片，下了火锅，吃得也挺香。话说回来，虽然那样一份做菜宝典在当时的实践中没有对我起到积极作用，但它完整持续地指导了我后来的做菜体验，因为直到现在每每要做某个菜的时候我脑海中总是立马蹦出起锅烧油葱姜蒜爆香这一句祖传箴言。 言归正传，关于做菜这件事，我始终觉得我还有很大的空间，因为我只是做到了从零到一，却没有做到从一到二。毕竟从不会到会只是一个入门，从入门到精通却要用很久很久的时间去自我体会自我升级。 而这个自我体会的过程，绝不能光在网上看宝典、写便利贴，滚瓜烂熟也不行的；要下到厨房里，要实践，要折腾，还要可行，然后总结。不然，起锅烧油葱姜蒜爆香之后就不知道接下来该做什么。 总而言之，2021，我已经毕业离开校园正式走向工作岗位，开始实践做菜以及诸多种种，也开始实现我小时候的理想——“做一个对社会有用的人”。虽然有的入门有的还没有，但2022年继续折腾就是了！ 最后，附上习主席的2022新年贺词链接及部分内容 国家主席习近平发表二〇二二年新年贺词_新闻频道_央视网(cctv.com) 回首这一年，意义非凡。我们亲历了党和国家历史上具有里程碑意义的大事。“两个一百年”奋斗目标历史交汇，我们开启了全面建设社会主义现代化国家新征程，正昂首阔步行进在实现中华民族伟大复兴的道路上。 从年头到年尾，农田、企业、社区、学校、医院、军营、科研院所……大家忙了一整年，付出了，奉献了，也收获了。在飞逝的时光里，我们看到的、感悟到的中国，是一个坚韧不拔、欣欣向荣的中国。这里有可亲可敬的人民，有日新月异的发展，有赓续传承的事业。 七月一日，我们隆重庆祝中国共产党成立一百周年。站在天安门城楼上感慨系之，历史征程风云激荡，中国共产党人带领亿万人民经千难而百折不挠、历万险而矢志不渝，成就了百年大党的恢宏气象。不忘初心，方得始终。我们唯有踔厉奋发、笃行不怠，方能不负历史、不负时代、不负人民。 党的十九届六中全会通过了党的第三个历史决议。百年成就使人振奋，百年经验给人启迪。我曾谈到当年毛主席与黄炎培先生的“窑洞对”，我们只有勇于自我革命才能赢得历史主动。中华民族伟大复兴绝不是轻轻松松、敲锣打鼓就能实现的，也绝不是一马平川、朝夕之间就能到达的。我们要常怀远虑、居安思危，保持战略定力和耐心，“致广大而尽精微”。 大国之大，也有大国之重。千头万绪的事，说到底是千家万户的事。我调研了一些地方，看了听了不少情况，很有启发和收获。每到群众家中，常会问一问，还有什么困难，父老乡亲的话我都记在心里。 民之所忧，我必念之；民之所盼，我必行之。我也是从农村出来的，对贫困有着切身感受。经过一代代接续努力，以前贫困的人们，现在也能吃饱肚子、穿暖衣裳，有学上、有房住、有医保。全面小康、摆脱贫困是我们党给人民的交代，也是对世界的贡献。让大家过上更好生活，我们不能满足于眼前的成绩，还有很长的路要走。 黄河安澜是中华儿女的千年期盼。近年来，我走遍了黄河上中下游9省区。无论是黄河长江“母亲河”，还是碧波荡漾的青海湖、逶迤磅礴的雅鲁藏布江；无论是南水北调的世纪工程，还是塞罕坝林场的“绿色地图”；无论是云南大象北上南归，还是藏羚羊繁衍迁徙……这些都昭示着，人不负青山，青山定不负人。 这一年，还有很多难忘的中国声音、中国瞬间、中国故事。“请党放心、强国有我”的青春誓言，“清澈的爱、只为中国”的深情告白；“祝融”探火、“羲和”逐日、“天和”遨游星辰；运动健儿激情飞扬、奋勇争先；全国上下防控疫情坚决有力；受灾群众守望相助重建家园；人民解放军指战员、武警部队官兵矢志强军、保家卫国……无数平凡英雄拼搏奋斗，汇聚成新时代中国昂扬奋进的洪流。 祖国一直牵挂着香港、澳门的繁荣稳定。只有和衷共济、共同努力，“一国两制”才能行稳致远。实现祖国完全统一是两岸同胞的共同心愿。真诚期盼全体中华儿女携手向前，共创中华民族美好未来。]]></content>
  </entry>
  <entry>
    <title><![CDATA[不需要梯度就能搞CAM之Score-CAM与Ablation-CAM]]></title>
    <url>%2F2020%2F12%2F11%2F%E4%B8%8D%E9%9C%80%E8%A6%81%E6%A2%AF%E5%BA%A6%E5%B0%B1%E8%83%BD%E6%90%9ECAM%E4%B9%8BScore-CAM%E4%B8%8EAblation-CAM%2F</url>
    <content type="text"><![CDATA[这一博文讨论关于今年CVPR workshop的Score-CAM方法和WACV的Ablation-CAM方法。两者的一大共同点就是不再像gradient-CAM那样依赖梯度生成类激活图谱，也就是他们文中宣称的gradient-free CAM。 引在present新方法之前，我们先把之前的几种经典的CNN可视化解释方法过一下。包含有：梯度可视化（gradient visualization）、扰动型（perturbation）和类激活图谱（CAM-based）。 gradient-based：利用目标类对输入图像的导数作为可视化解释结果，即saliency map；这种方法最大的问题就是会有较大的noise，而且saliency map low quality不够obvious； perturbation-based：对输入图像添加不同的有意义的扰动，然后观察模型输出的变化以确定有意义的区域； CAM-based：将卷积层的feature map进行线性加权组合得到；常用的有CAM、Grad-CAM——CAM其实很简单，就是将GAP前的feature map拿出来，再将GAP后fc层与target class对应的权重拿出来，然后做linear combination就行。可以定义为：L_{C A M}^{c}=\operatorname{ReLU}\left(\sum_{k} \alpha_{k}^{c} A_{l-1}^{k}\right)其中$\alpha_{k}^{c}=w_{l, l+1}^{c}[k]$ 也就是GAP后接的fc上拿出来的归属类别$c$的权重，$A_{l-1}^{k}$就是feature map。为什么可以这么做？因为GAP将feature map变成了$1*1$大小，紧接着的fc进行了一个映射，刚好这个映射就是针对feature map的channel映射到类别层的类别数，所以我们可以把这个$w_{l,l+1}^{c}$拿出来作为feature map的channel的重要性进行线性加权组合得到attention map。但是它的缺陷就是必须要求网络一定要有个GAP并且GAP后跟且仅跟一层全连接（不算最后的类别层）。为了解脱这个限制，Grad-CAM出来了。核心还是feature map进行线性加权组合的理念。这时，feature map不用动，还是提取最后一个卷积层的特征图（越往后的特征越有visual explanation，noise越少），然后就是怎么获得这个$\alpha_{k}^{c}$权重，发现在CAM中其实这个权重就是特征图$A_{l-1}^{k}$每个channel归属于类别$c$的weight。因此可以对$Y^{c}$求关于$A_{l-1}^{k}$的偏导，然后进行一次特征图大小的平均得到每个channel的weight，即： \alpha_{k}^{c}=\frac{1}{Z} \sum_{i} \sum_{j}\underbrace{\frac{\partial y^{c}}{\partial A_{i j}^{k}}}还有Grad-CAM++ 方法是Grad-CAM的升级版，可参考Grad-CAM++: Generalized Gradient-based Visual Explanations for Deep Convolutional Networks Score-CAMCAM-based 方法很经典，也很好用。但是Score-CAM作者在这里宣称了CAM-based的一个问题$^*$：就是feature map进行线性加权组合时的权重$\alpha_{k}^{c}$并不总是有效。比如他说：high weight的feature map mask到input图像上后再进行输入得到的output并不一定比low weight的高。具体图示如下： The weights for activation maps (2)-(4) are 0.035, 0.027, 0.021 respectively. The values above are the increase on target score given (1)-(4) as input。 因此Score-CAM作者提出了一种他们的方法—— 具体的visualization流程图如上图所示。多看两眼很容易理解。首先将输入图像$X$送入trained CNN网络中得到feature maps $A_{l-1}$，将其上采样后mask到输入图像$X$上得到$k$个新输入图像$X’$，再次送入该trained CNN网络中得到属于类别$c$的预测概率作为$\alpha_{k}^{c}$然后与之前的feature maps $A_{l-1}$进行线性加权组合得到了最终的attention热图。其算法伪代码如图所示： 其中有几个细节—— feature maps $A_{l-1}$上采样（双线性插值）后mask到输入图像$X$上前首先进行了一个平滑操作，其实就是针对$A^{k}_{l-1}$的min max normalization； $\alpha_{k}^{c}$的表示，$\alpha_{k}^{c}=C\left(A_{l}^{k}\right)=f\left(X \circ H_{l}^{k}\right)-f\left(X_{b}\right)$，其实是输入$X’$的probability output与一个baseline输入的probability output的差。但是其实作者把baseline的输出置0了。。。 啥意思，就是$X’$的属于类别$c$的probability output作为$\alpha_{k}^{c}$； $k$个新输入图像$X’$的属于类别$c$的$\alpha_{k}^{c}$值并不在同一区间范围内，因此作者又针对$\alpha^{c}$做了$softmax$。最后来一手线性加权组合加ReLU得到attention热图。 实验评估实验部分主要采用了一些可解释可视化领域的常用指标和方法：包括有多目标结果可视化对比、Average Drop、Average Increase、Deletion和Insertion、用检测框定量分析等。 Ablation-CAMAblation-CAM就不再写了。。。大体扫了一眼，和Score-CAM思路是一致的，都是从feature maps入手，只不过不是重定义输入，而是对feature maps中的一个进行remove后得到新的预测值$y’^c_k$，搞了个式子$1-\frac{y’^c_k}{y^c}$(其中$y^c$是正常没有任何remove的预测值)作为该channel的feature map的权重$\alpha_{k}^{c}$，然后继续线性加权组合得到attention热图。 Score-CAM和Ablation-CAM个人理解 $^*$Score-CAM作者宣称的Grad-CAM方法的弊端我个人是不敢苟同的，因为gard-cam的目标是所有feature maps的线性加权组合，而单个channel feature map的weight意义并不那么重要；以及，单个channel feature map mask到input上并不是CAM的方式，而只是Score-CAM作者为了引出他们自己方法的一个过渡手段。 这里提出的Score-CAM和Ablation-CAM本身还是没有跳出特征线性加权组合的框架，甚至丢失了CAM以及grad-CAM方法的一个最重要特性：对attention map进行GAP后得到的值即为归属于类别$c$的预测值。 最后，私人评论：这两篇都是乍看很惊喜，但是通读下来并没有太多热情的文章。还是Grad-CAM的方法更优美。 参考文献 Grad-CAM paper Score-CAM paper Ablation-CAM paper Ablation-CAM appendix Score-CAM代码仓库以及被集成进torch-cam仓库和pytorch-cnn-visualizations仓库]]></content>
      <categories>
        <category>Paper</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[HE图像上为什么可以颜色反卷积进行通道分离以及染色标准化]]></title>
    <url>%2F2020%2F11%2F19%2FHE%E5%9B%BE%E5%83%8F%E4%B8%8A%E4%B8%BA%E4%BB%80%E4%B9%88%E5%8F%AF%E4%BB%A5%E9%A2%9C%E8%89%B2%E5%8F%8D%E5%8D%B7%E7%A7%AF%E8%BF%9B%E8%A1%8C%E9%80%9A%E9%81%93%E5%88%86%E7%A6%BB%E4%BB%A5%E5%8F%8A%E6%9F%93%E8%89%B2%E6%A0%87%E5%87%86%E5%8C%96%2F</url>
    <content type="text"><![CDATA[这篇博客原本是应该在4月份就更新出来的，但是迟迟拖到了11月份。而且很恰好地组会讲到了病理H&amp;E图像的染色分离，否则2020年都不会再填这个坑。 待续。 参考文献 学分析法（有关朗伯比尔定律） 度百科_朗伯比尔定律 光密度与吸光度的正确使用说明 博客_颜色反卷积 知乎Adia的几篇color deconvoluation相关博文 Paper: Structure-Preserving Color Normalization and Sparse Stain Separation for Histological Images及source code Paper: Fast GPU-Enabled Color Normalization of Whole Slide Images in Digital Pathology及source code]]></content>
  </entry>
  <entry>
    <title><![CDATA[秋招感悟帖]]></title>
    <url>%2F2020%2F10%2F15%2F%E7%A7%8B%E6%8B%9B%E6%84%9F%E6%82%9F%E5%B8%96%2F</url>
    <content type="text"><![CDATA[我的秋招结束了。 曾经我觉得非常遥不可及的事情，今天已然摆在我面前；甚至，曾经我觉得我不会走上去企业这条路，如今也不得不直面现实。这里不要觉得我在说去工业界不好，而是，人被一个组织洗脑久了，心理预设自然而然被改变、被说服。但是很庆幸人是灵长动物，他总能认识到原来梦想不过是一场梦魇。噩梦醒来，他会发现当下即将面对的现实绝没有像梦里所言恐怖，梦里的美景也没有比现实美好半分。它不仅不美好，甚至都没有真实存在，不过是造梦者绘制的一张张大饼。 从7月初事情发生，到7月14决定工作，投了一波牛客提前批。随后7月底又在犹豫不决开始心理波动，到8月初正式决定，开始海投。同时开始疯狂刷乐扣，看机器学习深度学习基础复习编程语言等内容，到基本结束应该算是9月18号了。再往后虽然也陆陆续续有些面试，但是几乎就没再看新内容了，一直持续到10月9号，以宁波银行终面结束。并且很有意思的是，终面的演讲关键词是反思—— 信息爆炸的时代，我们所处的社会环境总要求我们要奔跑着去追逐新的事物，要多学一些东西，我们都在讲学习能力很重要，但是很多时候却忘记了反思能力同等重要。反思，不仅是使我们的心态平和谦卑的能力；反思，更是让我们自身变得更强的能力。周恩来撰写的《我的修养要则》，从学习到工作到生活都给自己制定了严格的标准，时刻提醒自己，时刻反思自己。去年也有“不忘初心，牢记使命”的党员主题教育活动，让我们不忘入党初心，为新时代不懈奋斗。这些，都是在教我们学会反思，强化自身，以完成更大的事业。信息爆炸的时代，需要我们学会反思，反思我们的心态是否积极，反思我们的态度是否端正，反思是遇见更优秀自己的第一步。人们总说爱笑的人运气不会太差，而我说，爱反思的人，让自己更强大。 以上是我面试的演讲稿，以这个词作为秋招面试的告终，很耐人寻味。虽然内容掺杂了太多网络上的成分，但核心观点输出是自发的。 关于反思，其他的也不太想表述了。另计，莫要混淆它和后悔即可。 再说回来秋招，虽说秋招持续了蛮久，但是实际上作为一个已经算是仓促上马的“新兵”，着实面试并无太多。满打满算应该也才20场。所以要说经验，可能也并无几多。而且在今年较为严峻的就业大形势和本身双非院校的“加持”下，最终拿到两家都比较满意的offer，确实该归功于运气。 实事求是地讲，还在秋招泥潭里时，这家给我offer那家给我offer，我是没有太大感觉的，除了开心。但当学校三方下来准备让我选择签约时，我才发现，确实有太多值得我考虑也需要我去考虑。工作城市、个人未来规划、亲人女朋友、企业未来、职业行业、福利待遇、工作强度…… 这个时候，我才发现，我原本对于就业的理解过于肤浅，对于未来的解读过于简单。真的不是钱多就行…… 但是现在最终决定过后，当我再看我的选择，得，还是选了个钱多的…… 但它确实不是之前的选择。就像那句话说的—— 我还是我，但我已不再是我。]]></content>
  </entry>
  <entry>
    <title><![CDATA[PyTorch里构建Transforms进行医学病理图像数据增强]]></title>
    <url>%2F2020%2F04%2F25%2FPyTorch%E9%87%8C%E6%9E%84%E5%BB%BATransforms%E8%BF%9B%E8%A1%8C%E5%8C%BB%E5%AD%A6%E7%97%85%E7%90%86%E5%9B%BE%E5%83%8F%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA%2F</url>
    <content type="text"><![CDATA[近期分析了一下PyTorch里torchvision里的transforms包，即，我们用PyTorch训练时常设置用于Normalize的preprocessing函数——from torchvision import transformspreprocess = transforms.Compose([transforms.Resize((50, 50)), transforms.ToTensor(), transforms.Normalize(normMean, normStd)]) 其实说是个transforms，我更把它理解为on-the-fly的数据增强。 整个transforms包中，共有27个类：__all__ = ["Compose", "ToTensor", "ToPILImage", "Normalize", "Resize", "CenterCrop", "Pad", "Lambda", "RandomApply", "RandomChoice", "RandomOrder", "RandomCrop", "RandomHorizontalFlip", "RandomVerticalFlip", "RandomResizedCrop", "RandomSizedCrop", "FiveCrop", "TenCrop", "LinearTransformation", "ColorJitter", "RandomRotation", "RandomAffine", "Grayscale", "RandomGrayscale", "RandomPerspective", "RandomErasing"] 大致上可以分为四类—— 应用类；不是对图像本身进行变换，而是针对变换操作本身的处理； [&quot;Compose&quot;, &quot;RandomApply&quot;, &quot;RandomChoice&quot;, &quot;RandomOrder&quot;, &quot;Lambda&quot;] Lambda类存在意义很大，可以方便地将用户自定义的图像变换函数作为transform使用。如：将transforms.Lambda(lambda img: my_trans(img, 0.01))放进Compose中。 基本变换类；进行基本的必备的变换； [&quot;ToTensor&quot;, &quot;ToPILImage&quot;, &quot;Normalize&quot;, &quot;Resize&quot;, &quot;Pad&quot;] 其中，pad操作中的&quot;reflect&quot;, &quot;symmetric&quot;极好地弥补了PIL.Image中有些变换后边界只能置固定值的缺陷。 选择类；图像本身没有变换，只是做了原图像内容的不同选择； [&quot;CenterCrop&quot;, &quot;RandomCrop&quot;, &quot;RandomHorizontalFlip&quot;, &quot;RandomVerticalFlip&quot;, &quot;RandomResizedCrop&quot;, &quot;RandomSizedCrop&quot;, &quot;FiveCrop&quot;, &quot;TenCrop&quot;, &quot;RandomRotation&quot;, &quot;Grayscale&quot;, &quot;RandomGrayscale&quot;, &quot;RandomErasing&quot;] 当然，这里有点冲突，形如RandomResizedCrop，同时包含图像内容选择和resize，我私认为主体还是图像内容选择。 复杂变换类； [&quot;LinearTransformation&quot;, &quot;ColorJitter&quot;, &quot;RandomAffine&quot;, &quot;RandomPerspective&quot;] 包含了白化变换、HSV颜色空间的H和S通道扰动、亮度对比度扰动、仿射变换、透视变换。 除此以外，我也实现了几个特别适用于医学病理图像增强的几个增强类，包含有： HEDJitter：HE染色病理图像的HED空间随机扰动增强采用颜色反卷积方法，将HE图像从RGB空间变换到HED空间，然后针对HED空间每一通道添加随机扰动，最后再变换到RGB空间。 其中，HED空间通道扰动策略：$s’ = \alpha * s + \beta$ RandomElastic：随机弹性变换对每个像素点分别进行x和y的扰动偏差，具体可参考原论文section2 RandomAffineCV2：随机仿射变换（基于opencv方法）该仿射变换和torchvision中的RandomAffine本质一样，只不过实现上是PIL.Image和cv2的区别。采用opencv方法，可以对变换后的无内容边界区域进行镜像填充，而非PIL.Image那般死板只能填充固定像素值； RandomGaussBlur：随机高斯模糊基于高斯滤波方法，对图像进行不同程度的模糊，适用于病理全扫描切片图像中的部分区域聚焦不够的情况； AutoRandomRotation：自动随机旋转这个自动有点牵强，其实就是将torchvision的RandomRotation稍微改动了一下，从{0，90，180，270}这个集合中随机选择一个角度进行旋转，没有其他角度。这种做法主要是为了避免PIL.Image中边界区域只能填充固定像素值的问题（当然也可以用pad操作解决）。 代码已经开源托管在Github: Augmentation-PyTorch-Transforms ，并且自认为在README中清晰地介绍了增加的图像增强类的使用方式，或者可以查看Example_Transforms 参考，欢迎star，fork。 一个基本的myTransforms示例preprocess = myTransforms.Compose([ myTransforms.RandomChoice([myTransforms.RandomHorizontalFlip(p=1), myTransforms.RandomVerticalFlip(p=1), myTransforms.AutoRandomRotation()]), # above is for: randomly selecting one for process # myTransforms.RandomAffineCV2(alpha=0.1), # alpha \in [0,0.15], # myTransforms.RandomAffine(degrees=0, translate=[0, 0.2], scale=[0.8, 1.2], shear=[-10, 10, -10, 10], fillcolor=(228, 218, 218)), myTransforms.RandomElastic(alpha=2, sigma=0.06, mask=None), myTransforms.ColorJitter(brightness=(0.65, 1.35), contrast=(0.5, 1.5)), myTransforms.RandomChoice([myTransforms.ColorJitter(saturation=(0, 2), hue=0.3), myTransforms.HEDJitter(theta=0.05)]), # myTransforms.RandomGaussBlur(radius=[0.5, 1.5]), myTransforms.ToTensor(), #operated on original image, rewrite on previous transform. myTransforms.Normalize([0.6270, 0.5013, 0.7519], [0.1627, 0.1682, 0.0977])])print(preprocess)]]></content>
      <categories>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>数据增强</tag>
        <tag>pytorch</tag>
        <tag>transforms</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何上百倍加速病理图像的全扫描层面预测]]></title>
    <url>%2F2020%2F04%2F22%2F%E5%A6%82%E4%BD%95%E4%B8%8A%E7%99%BE%E5%80%8D%E5%8A%A0%E9%80%9F%E7%97%85%E7%90%86%E5%9B%BE%E5%83%8F%E7%9A%84%E5%85%A8%E6%89%AB%E6%8F%8F%E5%B1%82%E9%9D%A2%E9%A2%84%E6%B5%8B%2F</url>
    <content type="text"><![CDATA[引言在病理图像分析领域，针对肿瘤组织区域检测，浸润性组织检测等问题，绝大部分都是采用图像块分类的策略达到特定类别检测，因为直接分割的思路并不适用于ROI面积过大的病理全扫描图像。 通常来说，我们通过在病理全扫描切片图像上的ROI区域进行取patch并给定正样本标签，同时在非ROI区域中取patch并给定负标签，以此当作一个分类任务，训练得到模型。然后在推断阶段，我们采用滑动窗遍历的策略对一张病理全扫描切片进行逐patch的样本预测，同时将预测结果按照patch的位置顺序进行拼接，最终得到全扫描切片上的ROI预测结果。 然而， 我们都知道，经典的分类网络中最后都会接一个全局池化，然后接全连接层得到类别输出概率值。这在自然图像中没有问题，但是在我们这个从slide到patch的问题中，就带来一个弊端—— 滑动窗取patch的过程就是要保证patch连续、逐次，但是经过全局池化和全连接层后，特征反而被压缩为向量，这时候，原本相邻patch的重叠区域的特征也一并被压缩为向量了。这导致了重叠区域特征的浪费。也就是说，对于某一区域，滑动窗步长越小，重叠区域越多，重复的特征就会越多，重复的计算极大地浪费了算力！ 因此，这里学习一个方法，能否在病理全扫描切片图像的滑动窗预测过程中，对于重复的区域，我们只计算一次特征，这样就不会浪费算力，自然把推断效率提上来了！ 这也就引出来本博客介绍的方法：ScanNet，一种快速全扫描病理切片预测方法。 将原本的全局池化和全连接层替换为固定kernel size大小的池化和1x1的卷积层。在patch-level训练测试时，本质和原本VGG一样，最终输出类别概率值。但是在slide-level推断阶段，因为网络结构中没有了全局池化和全连接，全卷积这种方式允许了更大的图像输入从而输出更大尺寸的概率输出，因此在推断阶段通过输入更大的block，没有了重复区域，也就避免了特征的重复计算，如此也就提高了推断效率。 实现思路 用全卷积层的VGG_fullyConv替换原来的经典VGG进行训练； 对全卷积VGG_fullyConv模型进行分类训练，与原始训练过程一致； 基于验证集查看模型性能，并根据评估指标选择最优模型； 在Patch-level测试数据集上测试，评估训练好的VGG-fullyConv模型； 针对病理全扫描切片图像预测，输入尺寸不再局限于patch大小，在GPU允许的前提下，输入更大的block，进行快速推断ROI区域概率热图！ 实现细则self.avgpool = nn.AvgPool2d(kernel_size=2, stride=2)self.classifier = nn.Sequential( nn.Conv2d(out_channel, out_channel, kernel_size=1, stride=1), nn.BatchNorm2d(out_channel), nn.ReLU(inplace=True), nn.Conv2d(out_channel, num_classes, kernel_size=1, stride=1)) 实现代码博客代码已经实现，开源托管在Github: Fast-WSI-Prediction，欢迎star，fork.]]></content>
      <categories>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
        <tag>病理图像</tag>
        <tag>快速全扫描切片推断</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[BeautyGlow论文的一点笔记]]></title>
    <url>%2F2020%2F02%2F26%2FBeautyGlow%E8%AE%BA%E6%96%87%E7%9A%84%E4%B8%80%E7%82%B9%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[Life isn’t perfect, but your makeup can be. 这几天帮朋友看个BeautyGlow的代码。感觉挺有意思，就把论文看了下，里面定义的损失函数蛮丰富的，也许有机会可以参考一下，所以这里记录一下。 该工作提出的模型框架如图所示。想要的目标就是把参考图像中脸上的妆容迁移到目标图像中的脸上。因此输入是一个有妆容的参考图像和一个没有妆容的目标图像，输出是有妆容的目标图像（生成的）。中间的思路就是用glow模型（生成式流模型）得到各自的隐空间latent vectors；然后分别将隐空间分解为脸部facial特征和妆容makeup特征；再将参考图像的妆容makeup特征和目标图像的脸部facial特征相加得到有妆容的目标图像的隐空间；最后再用glow模型反变换为有妆容的RGB目标图像。 思路很合理，但是仔细一想，其实最重要的就是如何将脸部特征和妆容特征分解开来，以及如何将脸部特征和妆容特征进行组合？作者这里最基本的假设就是隐空间只由facial和makeup特征加法组成。因此这里对隐空间$L$用了一个线性映射的权重$W$来变换得到facial特征$F$，那么剩下的部分就是makeup特征$M$，也就是: M=L-F=L-WL=(I-W)L后面组合的思路也和分解一样，直接相加即可。 那么问题来了，权重$W$如何得到？很简单的思路就是，送到网络里去学习啊。那么这就牵扯到优化目标，应该关注、优化什么才能让权重$W$将隐空间分为facial和makeup特征，继而让$L_{s}^{Y}$更好，最终得到有妆容的RGB目标图像。很显然，我们可以直接优化有妆容的RGB目标图像。但是事实上，一旦前面的glow模型确定，我们的glow反变换也就确定了。因此，我们只需要优化$L_{s}^{Y}$。但是仔细一想，这东西没有监督，所以我们只能通过对自身特征的约束来优化$W$去验证（或者说满足）假设。所以下面来介绍这些约束，也就是文中的目标函数： 针对$L_{s}^{Y}$的循环一致性损失。得到的目标是有妆容的，那么我们可以再对$L_{s}^{Y}$用权重$W$提取facial特征和makeup特征。理想情况，这两个特征应该是分别与$F_{s}^{X}$和$M_{r}^{Y}$一致的。 \mathcal{L}_{c y c}=\left\|L_{s}^{Y} W-F_{s}^{X}\right\|_{2}+\left\|L_{s}^{Y}(I-W)-M_{r}^{Y}\right\|_{2} 针对$F_{s}^{X}$的感知损失。因为目标图像一开始是没有妆容的，所以理想情况，$F_{s}^{X}$不就是$L_{s}^{X}$本身么？ \mathcal{L}_{p}=\left\|F_{s}^{X}-L_{s}^{X}\right\|_{2} 针对$M_{r}^{Y}$的妆容损失。我们把有妆容图像和无妆容图像的集合看成是两个域。那么整体上，两个域之间其实是一个包含被包含的关系，只差妆容这一部分。因此，我们可以用有妆容图像的隐空间集合的均值和无妆容图像的隐空间集合的均值量化表示这两个域。两个域的差来近似表示每一个有妆容图像的makeup特征。 \mathcal{L}_{m}=\left\|M_{r}^{Y}-\left(\bar{L}^{Y}-\bar{L}^{X}\right)\right\|_{2} 针对$F_{r}^{Y}$和$L_{s}^{Y}$的域内损失。我们优化的目标无非就是$L_{s}^{Y}$，但是它是$F_{s}^{X}$以及$M_{r}^{Y}两个部分的和，而这两个部分又有各自的补。因此，还是可以从域这个角度下手，毕竟域内关系和域间关系是老生常谈的问题。而这两个域又是包含被包含的关系，所以很好定义。对于有妆容图像，去掉妆容其实就处于没有妆容的域；对于没有妆容图像，加上妆容其实就处于有妆容的域。 \mathcal{L}_{intra}=\left\|F_{r}^{Y}-\bar{L}^{X}\right\|_{2}+\left\|L_{s}^{Y}-\bar{L}^{Y}\right\|_{2} 针对$F_{r}^{Y}$和$L_{s}^{Y}$的域间损失。既然希望域内损失更小，就希望域间损失更大。这里用了个相似度度量 \operatorname{sim}(A, B)=\frac{\operatorname{sum}(A \otimes B)}{|A||B|}$\otimes$表示逐元素乘；$|\cdot|$表示向量的模。（这其实就是余弦相似度度量；两向量从0到180度间变化，余弦值从1到-1越来越小；这样两个域就越远。正好对应下面定义的损失中的常数$1$） \mathcal{L}_{\text {inter}}=\left(1+\operatorname{Sim}\left(F_{r}^{Y}, \bar{L}^{Y}\right)\right)+\left(1+\operatorname{Sim}\left(L_{s}^{Y}, \bar{L}^{X}\right)\right) 值得注意的是，论文中这里的几个损失的加权和的权重值如下，$\lambda_{inter}$值相比十分巨大。也许不是因为只有$inter$损失有用，而且度量标准不一样。$\lambda_{p} = 0.01,\lambda_{cyc} = 0.001,\lambda_{m} = 0.1,\lambda_{intra} = 0.1,\lambda_{inter} = 1000$ 写在最后论文中定义的几个损失的确蛮有意思的，无监督方法里对多个特征进行约束，尤其含有域关系的约束。另外，文中introduction中还提到MSPM，单model多style怎么做。即：一个模型可以transfer多个styles，通过调整instance Normalization layers。有机会再看。 参考BeautyGlow原文地址Glow文章地址RealNVP文章地址OpenAI对Glow模型的讲解细水长flow之RealNVP与Glow：流模型的传承与升华细水长flow之NICE：流模型的基本概念与实现CSDN论文笔记（五）DENSITY ESTIMATION USING REAL NVPCSDN 论文阅读笔记 Glow:Generative Flow with Invertible 1*1 Convolutions]]></content>
      <categories>
        <category>Paper</category>
      </categories>
      <tags>
        <tag>BeautyGlow 损失优化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pytorch的反向传播及求导机制相关]]></title>
    <url>%2F2019%2F12%2F25%2Fpytorch%E7%9A%84%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E5%8F%8A%E6%B1%82%E5%AF%BC%E6%9C%BA%E5%88%B6%E7%9B%B8%E5%85%B3%2F</url>
    <content type="text"><![CDATA[本人在GITHUB上发布了CAM和GradCAM注意力可视化方法以及 FeatureExtractor特征提取特征可视化方法。欢迎star和fork。 autograd机制参考:pytorch docs 在pytorch中，autograd是由计算图实现的。如果一个操作有一个输入需要梯度，那么它的输出也需要梯度。相反，只有当所有输入都不需要梯度时，输出才会不需要梯度。在所有张量都不需要梯度的子图中，不会执行反向梯度计算。如此，对每个tensor的requires_gradflag置false可以使其从计算图中排除，减少非必要tensor的梯度计算，以提高效率。因此，当你想冻结你的模型的一部分，或者你事先知道你不打算使用一些参数的梯度的时候，这是特别有用的。例如，如果你想微调一个预先训练好的CNN，在冻结的基础上将requires_grad置为false就足够了，并且不会保存中间缓冲区，直到计算到达最后一层你需要优化的参数权重，才将需要梯度。model = torchvision.models.resnet18(pretrained=True)for param in model.parameters(): param.requires_grad = False# Replace the last fully-connected layer# Parameters of newly constructed modules have requires_grad=True by defaultmodel.fc = nn.Linear(512, 100)# Optimize only the classifieroptimizer = optim.SGD(model.fc.parameters(), lr=1e-2, momentum=0.9) 注意：神经网络的全连接层卷积层等结构的参数都是默认需要梯度的，而用torch.tensor([1.,2.,3.])得到的tensor默认是不需要梯度的，但可以通过torch.tensor([1.,2.,3.], requires_grad = True)显式指定。注意的是，此时的[1., 2., 3.]只能是浮点数，不能是整数。 关于计算图参考:手把手教你使用PyTorch 从PyTorch的设计原理上来说，在每次进行前向计算得到pred时，会产生一个用于梯度回传的计算图，这张图储存了进行反向传播需要的中间结果，这张计算图保存了计算的相关历史和提取计算所需的所有信息。只有调用了xx.backward()后，它才会从内存中被释放。为了理解，给出multi-task任务一个标准的流程：# http://www.imooc.com/article/282785for idx, data in enumerate(train_loader): xs, ys = data optmizer.zero_grad() # 计算d(l1)/d(x) pred1 = model1(xs) #生成graph1 loss1 = loss_fn1(pred1, ys) loss1.backward() #释放graph1 # 计算d(l2)/d(x) pred2 = model2(xs)#生成graph2 loss2 = loss_fn2(pred2, ys) loss2.backward() #释放graph2 # 使用d(l1)/d(x)+d(l2)/d(x)进行优化 optmizer.step() 其中，optmizer.zero_grad()用于在计算梯度之前将需要梯度的tensor的梯度置零，否则会出现梯度随iteration不断累加的情况。loss.backward()用于得到关于loss的所有梯度保存在对应的tensor中，同时释放计算图，最后optmizer.step()进行一次梯度更新。这里，因为是多任务，所以每一iteration只进行一次梯度置零，两次梯度计算，一次梯度更新。如此被累加的梯度的更新实现了多任务学习，而且不会导致多个分支同时保留计算图，占用过多显存。关于梯度累加的思想可以参考:PyTorch中的梯度累加。用optimizer.zero_grad()玩出花样! 临时关闭求导with torch.no_grad()参考:浅谈 PyTorch 中的 tensor 及使用 我们在训练时前向传播得到pred和保留的计算图，然后反向传播得到梯度。但测试的时候，对于输入数据，我们只需要前向传播得到pred，因此不需要backward()进行反向传播，更不需要保留计算图来进行梯度回传。因此可以将前向传播代码放在with torch.no_grad()下，不保留计算图，从而大大地减少显存使用率。一个简单的示例：&gt;&gt;&gt; x = torch.randn(3, requires_grad = True)&gt;&gt;&gt; print(x.requires_grad)True&gt;&gt;&gt; print((x ** 2).requires_grad)True&gt;&gt;&gt; with torch.no_grad():&gt;&gt;&gt; print((x ** 2).requires_grad)False&gt;&gt;&gt; print((x ** 2).requires_grad)True in-place 原地操作参考:PyTorch中in-place in-place运算指改变一个tensor的值的时候，直接在原始内存空间上进行值的改变，不经过复制操作。一般以后缀表示原地操作，如add_()或者 +=。由于pytorch中参数的求导在计算图中进行，因此，一旦在原始内存中修改了数据，则需要重写计算图，所以绝大多数情况不推荐使用in-place操作。 detach() 与 data()detach()和data()都是从计算图中得到一个新的相同的tensor，和原始的数据共享内存空间，但requires_grad为False。而一旦对新的tensor进行赋值等操作，会同样改变原始tensor的值。detach()和data()方式得到的新的tensor值被改变后，都会相应改变原始的tensor的值，造成原始tensor反向传播不能求导的情况。但是！不同的是，detach()会以RuntimeError报错，而data()却不会报错，但得到的$grad$也是错的！具体理解可以看下面的例子：# 使用detach()&gt;&gt;&gt; a = torch.tensor([1,2,3.], requires_grad = True)&gt;&gt;&gt; out = a.sigmoid()&gt;&gt;&gt; c = out.detach()&gt;&gt;&gt; c.zero_() tensor([ 0., 0., 0.])&gt;&gt;&gt; out # modified by c.zero_() !!tensor([ 0., 0., 0.])&gt;&gt;&gt; out.sum().backward() # Requires the original value of out, but that was overwritten by c.zero_()RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation # 使用data()&gt;&gt;&gt; a = torch.tensor([1,2,3.], requires_grad = True)&gt;&gt;&gt; out = a.sigmoid()&gt;&gt;&gt; c = out.data&gt;&gt;&gt; c.zero_()tensor([ 0., 0., 0.])&gt;&gt;&gt; out # out was modified by c.zero_()tensor([ 0., 0., 0.])&gt;&gt;&gt; out.sum().backward()&gt;&gt;&gt; a.grad # The result is very, very wrong because `out` changed!tensor([ 0., 0., 0.]) 因此，日常取用计算图中的tensor推荐使用detach()，不建议data()，这样出现问题的时候能够及时报错。当然，还有一种特殊的情况，detach()后进行原地操作再梯度计算不会导致RuntimeError报错，可以参考:pytorch中的detach和data 注意：之前的版本中，对新tensor进行resize_ / resize_as_ / set_ / transpose_原地操作也同样会改变原始tensor。而新的版本中，对新tensor进行resize_ / resize_as_ / set_ / transpose_原地操作不会改变原始tensor，反而会报错。 item()与tolist()tensor.item()以标准python数字的形式返回tensor的值，只适用于只有一个元素的tensor。其他情况，使用tensor.tolist()返回python list。&gt;&gt;&gt; x = torch.tensor([1.0])&gt;&gt;&gt; x.item()1.0&gt;&gt;&gt; a = torch.rand(2,2)&gt;&gt;&gt; a.tolist()[[0.33648067712783813, 0.29370278120040894], [0.28659379482269287, 0.07816547155380249]]&gt;&gt;&gt; a.cuda()tensor([[0.3365, 0.2937], [0.2866, 0.0782]], device='cuda:0')&gt;&gt;&gt; a.tolist()[[0.33648067712783813, 0.29370278120040894], [0.28659379482269287, 0.07816547155380249]] 附录1 一点神经网络训练tricks 一种方法是constant warmup，18年Facebook又针对constant warmup进行了改进，因为从一个很小的学习率一下变为比较大的学习率可能会导致训练误差突然增大。提出了gradual warmup来解决这个问题，即从最开始的小学习率开始，每个iteration增大一点，直到最初设置的比较大的学习率。 在凸优化问题中，随着批量的增加，收敛速度会降低，神经网络也有类似的实证结果。随着batch size的增大，处理相同数据量的速度会越来越快，但是达到相同精度所需要的epoch数量越来越多。也就是说，使用相同的epoch时，大batch size训练的模型与小batch size训练的模型相比，验证准确率会减小。具体做法很简单，比如ResNet原论文中，batch size为256时选择的学习率是0.1，当我们把batch size变为一个较大的数b时，学习率应该变为 0.1 * b/256。 标签平滑(Label-smoothing regularization,LSR)是一种通过在标签y中加入噪声，实现对模型约束，降低模型过拟合程度的一种正则化方法。它的具体思想是降低我们对于标签的信任，例如我们可以将损失的目标值从1稍微降到0.9，或者将从0稍微升到0.1。标签平滑最早在inception-v2中被提出，它将真实的概率改造为 q_{i}=\left\{\begin{array}{ll} {1-\varepsilon} & {\text { if } i=y} \\ {\varepsilon /(K-1)} & {\text { otherwise }} \end{array}\right. 参考:深度神经网络模型训练中的最新tricks总结 附录2 tensor和module的hook这个有时间针对$Grad-CAM$做一次解读。参考:PyTorch的hook及其在Grad-CAM中的应用]]></content>
      <categories>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
        <tag>反向传播autograd</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[matlab下的SSAE特征提取与图像分类]]></title>
    <url>%2F2019%2F11%2F25%2Fmatlab%E4%B8%8B%E7%9A%84SSAE%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%E4%B8%8E%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%2F</url>
    <content type="text"><![CDATA[写在前面之前在投稿isbi时做对比实验的SSAE方法。近期有时间总结一下，防止后续需要的时候又找不到。 Stacked Sparse Autoencoder (SSAE) for Nuclei Detection on Breast Cancer Histopathology images这篇isi热引论文采用堆叠稀疏自编码器(SSAE)对病理WSI做细胞核检测，虽然是检测，但也是通过滑动窗+图像块+细胞核/非细胞核二分类的思路实现。放在现在来看，方法比较简单，但是2013年投稿isbi2014的时候算是神经网络在计算病理领域的开创性工作。因此，决定好好理解一下AE，同时也matlab实现一下代码demo。 介绍自编码器是神经网络的一种，经过无监督训练后可以将输入近似复制到输出。具体地，一个自编码器有两个部分：用于特征表示的编码器和用于生成重构的解码器，分别由函数$h = f(x)$和$r = g(h)$表示。其中，隐藏层$h$用来产生编码表示输入数据$^{[1]}$。具体的公式如下： z=h^{(1)}\left(W^{(1)} x+b^{(1)}\right) ~~ \widehat{x}=h^{(2)}\left(W^{(2)} z+b^{(2)}\right)事实上，自编码器中，由输入到输出的复制并不太重要，重要的是我们通过这种方式，能够得到隐藏层$h$对于输入的特征编码。 极端来说，如果我们直接让$f()$和$g()$是线性函数，自编码器的输出值就会处处等于输入值，那这个特征编码就没有意义。因此，我们希望通过训练自编码器的输出值近似等于输入值，让隐藏层$h$能够学习到数据的有用特性来表征输入。而这通常需要向自编码器强加一些约束，使模型考虑输入的哪个部分需要被优先复制。 获取有用特征的一种方法是，限制隐藏层$h$的维度小于输入$x$的维度，即让数据有一定的损失后再进行重构恢复，使得自编码器学习数据中重要的特征。这种情况称为欠完备自编码器（undercomplete）。一般的，学习过程可以简单描述为最小化一个损失函数$L(x, g(f(x)))$。 通常地，我们可以直接通过对损失函数添加正则项来鼓励模型学习其他特性（如稀疏表示、对噪声的鲁棒性等），而不必一定需要隐藏层$h$的维度小于输入$x$的维度。实际中比较常见的有去噪自编码器和稀疏自编码器。 为了学习有用特征，去噪自编码器的做法是改变输入数据，引入噪声。它通过对原始数据$x$引入损坏过程$C(|)$得到$\tilde{x}$，再进行$g(f(\tilde{x}))$的变换，最后最小化损失函数$L(x, g(f(\tilde{x})))$的方式实现特征学习。 而稀疏自编码器的做法则是让隐藏层得到的特征更加稀疏 （也就是更多特征值接近0）。通过在损失函数中结合了重构误差$L(x,g(f(x)))$和稀疏正则项$\Omega(f(x))$的方式实现。 稀疏自编码器中的稀疏正则项稀疏正则项是关于一个神经元的平均输出激活值的函数。对于第$i$个神经元，其平均输出激活定义为： \hat{\rho}_{i}=\frac{1}{n} \sum_{j=1}^{n} z_{i}^{(1)}\left(x_{j}\right)=\frac{1}{n} \sum_{j=1}^{n} h^{(1)}\left(w_{i}^{(1) T} x_{j}+b_{i}^{(1)}\right)其中$n$是训练样本的数量，$x_j$是第$j$个训练样本，$w_{i}^{(1) T}$是权重矩阵$W^{(1)} $的第$i$行，$b_{i}^{(1)}$是$b^{(1)}$的第$i$个元素。如果输出激活值很高，意味着神经元被“引爆”。而低的输出激活值意味着隐藏层神经元只对应小部分样本“引爆”。通过添加约束，使得$\hat{\rho}_{i}$较低，可以使自编码器学习一种每个神经元只“引爆”一小部分样本的特征表示。也就是说，每一个神经元通过响应一些只存在于一小部分训练样本中的特征来进行专门化。 而稀疏正则的目的，就是让输出激活值低，也就是约束隐藏层输出使其稀疏。具体做法为，在损失函数中加入正则项，当第$i$个神经元的平均激活值$\hat{\rho}_{i}$和希望的激活值${\rho}$差异很大时，给一个很大的惩罚值，反之，则较小的惩罚。一种惩罚方式是用KL散度度量。 \Omega_{\text {Sparsity}}=\sum_{i=1}^{D^{(1)}} K L\left(\rho \| \hat{\rho}_{i}\right)=\sum_{i=1}^{D^{(1)}} \left(\rho \log \left(\frac{\rho}{\hat{\rho}_{i}}\right)+(1-\rho) \log \left(\frac{1-\rho}{1-\hat{\rho}_{i}}\right)\right)用KL散度度量平均激活输出$\hat{\rho}_{i}$和期望输出${\rho}$的差异，当两者相同时，KLD为0。这个时候，最小化损失函数不仅意味着最小化重构误差，也在最小化该正则项惩罚，使得$\hat{\rho}_{i}$和${\rho}$差异越来越小。其中，${\rho}$是训练稀疏自编码器时设置的超参数。 稀疏自编码器中的L2正则项然而实际应用中，优化过程很可能出现$w^{(1)}$很大或者$z^{(1)}$很小，导致稀疏正则项很小。因此，通常地，我们会在损失函数中加一个L2权重正则项防止这种情况出现，定义为： \Omega_{\text {weights}}=\frac{1}{2} \sum_{l}^{L} \sum_{j}^{n} \sum_{i}^{k}\left(w_{j i}^{(l)}\right)^{2}这里，$L$时隐藏层层数，$n$是样本数量，$k$是训练样本的维度。 稀疏自编码器的损失函数此时，训练一个稀疏自编码器的损失函数是这样的： E=\underbrace{\frac{1}{N} \sum_{n=1}^{N} \sum_{k=1}^{K}\left(x_{k n}-\hat{x}_{k n}\right)^{2}}_{MSE} + \underbrace{\lambda *\Omega_{weights}}_{L2~Reg} + \underbrace{\beta *\Omega_{sparsity}}_{sparsity~Reg}其中，$\lambda$是L2正则项的系数，$\beta$是稀疏正则项的系数。在训练稀疏自编码器时，可以设置。 以下，我会讲解一个堆叠稀疏自编码器的实现demo。 代码详解1：载入数据及实例展示载入matlab自带手写数字训练及测试数据集，其中image尺寸为28*28大小，以cell形式存储，label以one-hot格式存储。另外，这里随机选取训练数据中的9个实例进行可视化。clear;clc;% loading the digital data with cell-format raw and double label.load('digittrain_dataset.mat');load('digittest_dataset.mat');%%% show some examples in training imagesshownums = 9;ind = randperm(size(xTrainImages,2), shownums); for i = 1:shownums subplot(3,3,i); imshow(xTrainImages&#123;ind(i)&#125;);end 代码详解2：第一个AE训练及超参数设置对于28*28大小的图像数据（直接拉成784维特征），先采用一个稀疏自编码器进行训练用来特征提取。其中，参数设置：隐藏层特征200维，迭代训练500次，L2权重正则化系数0.004，稀疏正则系数4，稀疏值0.15。%%% define the hype-parametersfeat1nums = 200;maxepoch1 = 500;reg1 = 0.004;sparsityReg1 = 0.4;sparsityVal1 = 0.15;%%% training the Autoencoderautoenc1 = trainAutoencoder(xTrainImages, feat1nums, 'MaxEpochs', maxepoch1,... 'L2WeightRegularization', reg1, 'SparsityRegularization', sparsityReg1, ... 'SparsityProportion', sparsityVal1, 'ScaleData', false);save('SSAEe1', 'autoenc1')disp('Finished autoenc1!') matlab的toolbox做得很好，训练过程可视化很清晰。包括自编码器网络可视化、进程以及实时performance。训练完成后，得到自编码器模型$autoenc1$。 代码详解3：模型参数可视化和重构结果展示%%% show the weightsclose all;plotWeights(autoenc1);% reconstruct the imagesreconstruct = predict(autoenc1, xTestImages);ind = randperm(size(reconstruct,2), 1); imshowpair(reconstruct&#123;ind&#125;, xTestImages&#123;ind&#125;,'montage') 对于训练好的模型，我们可以采用plotWeights对模型中的编码器进行权重可视化。同样，为了直观查看自编码模型的好坏，我们可以将该模型应用于测试集predict得到所有图像对应的重构输出，然后show出来进行定性评估。 代码详解4：特征提取尽管直观来看，我们经过一次的重构结果已经不错，但是对于SSAE来说，我们希望有多个堆叠的自编码器进行特征提取。也就是说，第一个自编码器模型的解码器部分只是用来训练重构误差，而编码器部分才是用来对输入进行特征提取的关键。我们将第一个自编码器的编码部分encode拿出来对训练集数据进行特征提取，然后将提取的特征用于后续第二个自编码器的输入。%%% extract the features of training data, based on one autoencoderfeat1 = encode(autoenc1, xTrainImages); 代码详解5：第二个AE训练及超参数设置对于$autoenc1$提取到的200维的特征$feat1$，采用第二个稀疏自编码器进行训练用来进一步地特征提取。其中，参数设置：隐藏层特征100维，迭代训练500次，L2权重正则化系数0.002，稀疏正则系数0.4，稀疏值0.1。%%% define the hype-parameters of autoencoder 2 feat2nums = 100;maxepoch2 = 500;reg2 = 0.002;sparsityReg2 = 0.4;sparsityVal2 = 0.1;autoenc2 = trainAutoencoder(feat1, feat2nums, 'MaxEpochs', maxepoch2,... 'L2WeightRegularization', reg2, 'SparsityRegularization', sparsityReg2,... 'SparsityProportion', sparsityVal2, 'ScaleData', false);save('SSAEe2', 'autoenc2')disp('Finished autoenc2!') AE训练过程可视化如图所示: 代码详解6：特征提取及分类器训练训练完成后，得到第二个自编码器模型$autoenc2$。此时我们将第二个自编码器的编码部分encode拿出来再次对$feat1$进行特征提取得到$feat2$。此时的$feat2$即代表了我们用堆叠自编码器提取到的图像的特征。也就是说，每一张28*28大小的图像被100维特征表示。此时我们可以用trainsoftmaxLayer分类器进行有监督地训练。得到分类器模型$softmax$。softmax训练过程可视化如图所示： %%% extract the features of training data, based on one autoencoderfeat2 = encode(autoenc2, feat1);%%% training the softmax classifier based on the feat1 of autoencodersoftmax = trainSoftmaxLayer(feat2, tTrain, 'MaxEpochs', 1000);save('SSAEsoftmax', 'softmax')disp('Finshed softmax!') 代码详解7：样本测试及定量评估得到分类器模型，我们可以进行测试样本的评估。先对测试图像应用$autoenc1$提取200维特征，然后$autoenc2$对这200维特征提取100维特征$testfeats2$并送入$softmax$分类器得到样本的预测结果$pred$（以softmax概率形式）。可以用matlab内置的plotconfusion函数直接得到混淆矩阵，如图。 %%% predict the testing datatestfeats = encode(autoenc1, xTestImages);testfeats2 = encode(autoenc2, testfeats);pred = softmax(testfeats2);plotconfusion(tTest, pred); 代码详解8：可视化整个网络结构整个SSAE的过程大致如此。为了直观理解我们整个过程用了哪些网络结构，我们还可以将所有的模型结构stack起来view：% show the autoencoder structurenet = stack(autoenc1, autoenc2, softmax);view(net); 代码总结SSAE的代码就是这样，比较简单，而且matlab的toolbox集成相当好，整个过程很直观。当然，该demo也还有很多可以自行开发的地方： 数据：这里只是展示SSAE应用到matlab自带手写数字集上的demo，完全可以迁移到自己的数据集上进行分类应用。具体的数据制作方法，可以见下面的附录function：数据生成; 特征提取： 稀疏自编码器（SAE）的本质是无监督提取特征，因此可以更换其他特征提取方法或者其他自编码器（）； 分类器：softmax分类器其实是一种神经网络方法。这里的分类器和SSAE部分是相互独立的。可以换成其他分类器，如SVM，LDA等等。乃至换成无监督聚类也未尝不可。 附录function： 数据生成%% generate the data for SSAE input on image folder% input: PATH of multi class samples. eg. '../data/test/'% input: format is 'png' or 'jpg' or 'bmp' format of images you want to index% input: shuffle or not , false or true% input: saveFLAG or not, false or true% output: genData and genLabel (one-hot format)function [genImg, genLabel] = genData(PATH, format, shuffle, saveFLAG)% PATH = 'E:\Projects\ISBIcomparison\data\test\';% format = 'png'; D = dir(PATH); directories = &#123;D.name&#125;; disp(['There are **', num2str(length(directories)-2), ... '** dirs under this folder: ', PATH]); allImg = &#123;&#125;; allLabel = []; for d = 3:length(directories) % 1 and 2 is '.' and '..' subdir = fullfile(PATH, directories&#123;d&#125;); lists = dir(fullfile(subdir, ['*', format])); subdirnames = &#123;lists.name&#125;; nums = length(subdirnames); disp(['Indexing files of ', subdir, ' ... ']); disp([format, ' files: ', num2str(nums)]); % loop for all images in each subdir, saved in allData for ind = 1:nums imgName = fullfile(subdir, subdirnames&#123;ind&#125;);% disp(['Now is processing **', imgName, '**']); img = imread(imgName);% img = imresize(img, 0.5); allImg = cat(2, allImg, double(img)/255); % [1,255] convert to [0,1] allLabel = cat(2, allLabel, d-2); end end matrix = eye(max(allLabel)); % get the identity matrix label_OneHot = matrix(:,allLabel); % label numbers to one-hot disp('Finish generating!'); genImg = allImg; genLabel = label_OneHot; if shuffle randomlist = randperm(numel(genImg)); genImg = genImg(:,randomlist); genLabel = genLabel(:, randomlist); end if saveFLAG save('save_from_genData.mat', 'genImg', 'genLabel'); endend 参考[1] 深度学习 自编码器章节Ian Goodfellow[2] 自编码器是什么？[3] 为什么稀疏自编码器很少见到多层的？[4] Matlab中的trainAutoencoder]]></content>
  </entry>
  <entry>
    <title><![CDATA[VPS的ubuntu上搭建v2ray服务端及客户端配置教程]]></title>
    <url>%2F2019%2F11%2F21%2FVPS%E7%9A%84ubuntu%E4%B8%8A%E6%90%AD%E5%BB%BAv2ray%E6%9C%8D%E5%8A%A1%E7%AB%AF%E5%8F%8A%E5%AE%A2%E6%88%B7%E7%AB%AF%E9%85%8D%E7%BD%AE%E6%95%99%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[难得有时间，又难得对谷歌有需求，苦于ss经常被堵，开始寻求一种更可行的方案，v2ray。以下简单记录一下配置v2ray服务端和客户端的基本内容。 和SS以及SSR一样，v2ray本质上也是一个代理软件。但是相比而言，其具有更强大的功能，不仅网速快，而且协议完善，安全度高，而且可以开启网站伪装，隐蔽性更好。尽管配置复杂了一些，好在有大神的一键脚本安装方便很多。献上大牛的博客https://www.v2ray.com/https://233v2.com/ 准备材料 vps服务器一台 (我这里是从vultr官网购买) MobaXterm软件用于ssh远程登录 下载地址 服务端配置，v2ray脚本一键安装 在终端执行下面脚本：bash &lt;(curl -s -L https://git.io/v2ray.sh)回车 选择安装 1 选择WebSocket + TLS 数字4 更好掩护自己，不至于被pingbi 输入代理端口，按个人喜好 输入自己的域名， 然后到对应域名所在的控制台进行解析，这一步主要用于掩护 解析完成后，Y并回车 并进行自动配置TLS Y 是否开启网站伪装和路径分流 Y， 然后依次填入分流路径和伪装的网址， 比如www.baidu.com或者自己的博客网址cyyan.cn 是否开启广告拦截 N 是否配置shadowsocks Y，然后依次输入端口，密码，以及加密协议。 回车，安装。 安装完成后自动打印出vmess协议配置信息以及shadowsocks配置信息。 可以v2ray url 查看url信息用于v2ray客户端配置。 Windows客户端配置 到github上下载v2rayN-Core Github链接 解压后，双击打开v2rayN.exe文件，该文件即为v2ray客户端，然后双击任务栏的对应小图标； 复制服务端的vmess的url配置信息，然后点击从剪切板导入批量URL Http代理选项选择开启PAC并自动配置PAC IOS客户端配置IOS端需要首先下载小火箭客户端，但是app store上不能在国服下载，美服下载也收费。3刀。但是网上也有很多热心肠贡献出自己的账号，可以供大家免费下载。当然，也不一定什么时候就没了。IOS shadowrocketShadowrocket小火箭在线安装地址 安装好软件后，可以通过手动输入配置信息的方式，基本上和服务端的对应即可。可以参考这篇 安卓客户端配置V2Ray安卓(Android)客户端有V2RayNG和BifrostV 可供下载。Google Play上V2RayNG下载安装Github上的V2RayNG下载安装具体的配置，都比较简单啦，摸索摸索就可以。可以参考这篇 几点问题FAQ 我之前用的都好好的，但是ip被封了，我重新搞了个vps再重新来一遍之后，设置都保持一样，最终出现只有UUID改变了，如果我想用之前的UUID，我该怎么修改？UUID的确是默认的，但是可以通过修改配置文件的方式修改UUID，将/etc/v2ray/config.json该配置文件中的UUID修改为原来的即可。 我都是按照步骤来的，但是配置好之后（客户端也确认配置没问题），为什么还是翻不出去呢？一个可能的问题是，你不是在ubuntu系统里操作的。对于centos系统，有些端口不会开放，因此正常步骤完成后无法连接。但是ubuntu系统没有这个问题，因此建议重新install系统为ubuntu后，再次来过即可使用。比在centos系统里修改端口开放省事得多。 写在配置之后几台vps，每月10刀，但是每月流量相当多，因此决定出售流量，回笼资金。定价10元/100G/月 收费主要考虑几个原因：1.自己穷学生，毕竟也花了金钱花了功夫折腾，收点钱多少可以回笼点资金，后续才会源远流长，不至于断流；2.免费的东西反而会让购买方觉得廉价，对待更随意；花钱买到的尤其虚拟产品，更多是售后保障和售后服务；3.自己能赚到一点钱，也会觉得自己这些折腾受到鼓励和支持，也许你们的十元就是一种投资! 欢迎咨询购买 QQ: 771022845 Mail: gatsby2015@163.com]]></content>
      <categories>
        <category>SS</category>
      </categories>
      <tags>
        <tag>v2ray</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机视觉中的Non-local-Block以及其他注意力机制]]></title>
    <url>%2F2019%2F05%2F26%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E4%B8%AD%E7%9A%84Non-local-Block%E4%BB%A5%E5%8F%8A%E5%85%B6%E4%BB%96%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[总结一下最近看的一篇论文，Non-local Neural Networks，一种注意力机制模块，文章相对来说比较老，CVPR2018， 部分内容参考了这篇知乎博客 我将从以下几个方面总结一下论文，也有一点自己的整理，欢迎拍砖。 为什么提出Non-local？计算机视觉领域，尤其对于动态视频序列中，帧内帧间的依赖关系十分重要。尤其像下图中视频的行为分类任务，全局内容的理解以及不同帧间的联系对于分类结果导向作用很强。现在比较通用普遍的做法是，通过循环卷积网络联系$t$和$t-1$，或者通过更深的网络增大感受野提高对全局内容的理解。尽管如此，这种方式仍旧是比较local的，不论时间方向或者空间位置。而且，最大的问题是：没法进行远距离信息的来回传递；而且deeper网络计算量大但效率低，梯度优化起来也比较困难。因此，针对远距离信息传递问题，提高长距离依赖，本文从传统的非局部均值滤波方法中受到启发，提出了卷积网络中的non-local，即：某一像素点处的响应是其他所有点处的特征权重和，将每一个点与其他所有点相关联，实现non-local 思想。【可见，文章解决的痛点targetable，有意义。而且解决方法也有图像处理基础，禁得起推敲】 Non-local 的思想和简单数学原理Non-local的核心思想就是上面说的，某一像素点处的响应是其他所有点处的特征权重和。因此，假设对于一个2D的7*7特征图，总共是49*1（拉平操作）个位置，每个位置都与其他位置（包括本身位置）有关联性度量，就会得到49*49的关联性矩阵，而这个关联性矩阵经过归一化后其实可以理解为彼此之间的关联权重，因为不同像素点间的关联性都是不同的，因此这个权重本质上其实已经实现了注意力。当将这个49*49的权重再与原特征图49*1做矩阵乘时，得到的49*1矩阵就是该2D特征图所有像素点的响应。因此在这里的2D特征图的空间位置的注意力操作就是这个non-local操作。以上是举了一个例子，根据核心思想去理解这个东西。下面可以看一下具体的数学公式代入，抽象化。以下是我整理的公式图片，所有公式都在这上面。 将non-local的核心思想某一像素点处的响应是其他所有点处的特征权重和进行数学化，就是图片中的左公式（1），很好理解，其中$y_i$代表第$i$像素点处的响应，$f(x_i, x_j)$表示两个像素点的关联性度量函数，$g(x_j)$表示对$x_j$特征的embedding线性映射，这里用$W_g x_j$表示，$C(x)$表示一个归一化操作。如果左式理解不清，可以看右式，非常直观。其他所有点$x_j$，权重$f(x_i, x_j)$，特征$W_g x_j$， 和$\sum$， 以及归一化$C(x)$，一目了然。而在核心思想的数学化公式中，$f(x_i, x_j)$和$C(x)$没有具体的函数定义，下面就对其进行实例化。 公式（2）是其中一种常用的高斯函数进行相似度度量，两向量直接进行矩阵乘然后通过指数放大差异；这里归一化函数选用该点处所有相似度值的和$\sum_j f(x_i, x_j)$。这里值得注意的是，归一化函数的选用使得公式（1）变成了$softmax= \frac{\exp(x_i) }{ \sum_j \exp(x_j)}$函数形式; 公式（3）是公式（2）的改进形式，先将两向量分别映射到不同的嵌入空间（也就是进行不同的特征线性映射），然后进行公式（2）的应用； 公式（4）又是公式（3）的改进形式，不再用指数变换，归一化函数$C(x)$也直接采用$N$；这里主要是为了进行验证$softmax$激活函数的作用； 公式（5）是根据Relation~Networks论文提出来的特征concatenate形式；然后进行卷积变换；在卷积中实现不同位置的关联，也就是相似度的度量。【总而言之，几种相似度度量函数都有基础依据，又有自己实验的想法在里面。数学原理很清晰。】 Non-local在神经网络中的实现数学原理讲完了，下面就是具体的卷积网络中的应用了。论文以视频流的行为分类做了一个例子，不仅包含了同一帧的空间位置的相似度度量，还有不同帧之间同一位置、不同位置的相似度度量。为了简化，下面主要以图像领域空间位置的相似度度量做一个通俗的解释。 从图片右边公式（3）出发，为了在卷积网络中实现该相似度度量函数下的像素点响应，设计的non-local block具体如图左边网络结构。从上往下看，输入特征图为$C*H*W$，首先矩阵拉成向量得到$C*HW$，然后采用1*1卷积操作分别进行$g(x)$、$\theta(x)$和$\phi(x_j)$的线性映射，也就是图中的$W_v$、$W_k$和$W_q$；都得到嵌入空间下的$C*HW$；对于$\theta(x)$的$C*HW$进行转置得到$HW*C$与$\phi(x_j)$下的$C*HW$矩阵乘得到$HW*HW$相似度矩阵；然后$g(x)$的$C*HW$与经过$softmax$操作的$HW*HW$相似度矩阵再矩阵乘得到$C*HW$的响应，此时再转换为$C*H*W$即是经过了non-local加强距离依赖的特征图。论文同时又借鉴了resnet的恒等映射思想，再次对$y_i$下的的$C*H*W$经过1*1卷积然后与原输入特征图进行像素点的加和。即公式（6）。理解了公式（3）的这个block结构，剩下的也很好理解。公式（2）的block结构相较于结构图不进行$W_k$和$W_q$的1*1卷积操作；公式（4）相较于结构图不进行$softmax$激活，直接$1/N$归一化即可；公式（5）变换相较于结构图复杂一点，用特征concatenate，然后1*1卷积再进行$ReLU$激活。其实可以看到，$HW*HW$的相似度矩阵可以是$C*C$；那这时候其实就是通道的关联性度量了。另外，可以想象的是，像素点关联矩阵的计算量是很大的，因此，为了减少计算量，论文有几个小技巧，一个是通道减半bottleneck操作，另一个是进行尺寸pooling降维。后面还有针对这个弊端提出的几篇论文，CCNet，GCNet等。【看得出来，在卷积网络中应用也不是很复杂】 论文中的实验部分结果 实验部分比较足，主要是通过对比实验，如果想要用non-local block的话可以有所帮助，避免踩坑。 第一个，四个相似度度量函数，哪个好？解释是，四个结果都差不多，说明主要是我们这个non-local的思想好，什么度量函数，什么激活函数，不是根本因素。 第二个，放在哪里合适？ 结果显示，建议放在前面层，后面的层特征图中的空间信息被弱化，所以效果不如前面的好。 第三个，几个non-local block合适？ 多了好一点，但是多了计算量太大了，trade-off下就5个吧。 第四个，针对视频分类中的时间还是空间还是时空的注意力，三种对比实验。不解释 第五个，针对baseline的改进，用了non-local，参数量差不多情况下，性能好。 后面的不解释了，主要是针对baseline的改进 个人扩展 Non-local其实可以被认为是channel level和spatial level的泛化。这种方式通过注意力加强距离依赖，更是直接实现了全局的联系性。既有attention的思想也有context联系的思想。基于这个non-local，后面相继又有几个网络结构提出来，本质还是做注意力机制，只不过操作不一样，或者是改进版的。像刚刚说的，CCNet， GCNet。可以看图。另外，想说一点注意力的见解形如resnet alexnet等通用网络结构中，我们可以理解为空间或者通道间的所有位置，其重要性均等，即权重都为1。而注意力机制的根本目的，就是对原本平均分配的权重通过手动或者自学习的方式进行非等份分配。所以，从这个角度看，挂在嘴边的先验知识或是上下文关系（local， global， context）都可以理解为对原本等价权重的非等分配。在诸如SENet，CBAM中，通过网络训练的方式得到权重；而人为先验，是不是就是手动的权重分配，针对我们觉得重要的部分进行高权重赋值然后操作？ 附录 channel level， SENet SKNet 博客 spatial &amp; channel level：BAM和CBAM github代码 双注意网络 博客 CCNet； 理解为对non-local的效率上的改善；由dense message passing变成了iterative sparse message passing。博客 网易cs231n课程关于attention的部分]]></content>
      <categories>
        <category>Paper</category>
      </categories>
      <tags>
        <tag>注意力</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pytorch中的几个loss]]></title>
    <url>%2F2019%2F05%2F26%2Fpytorch%E4%B8%AD%E7%9A%84%E5%87%A0%E4%B8%AAloss%2F</url>
    <content type="text"><![CDATA[最近跟着Tensor_Yu学习pytorch，理一理里面很多东西。当我们想训练一个网络时，最重要的几个步骤是：如何载入数据，怎么定义并调用网络结构，最后，优化什么目标函数以及用什么方式优化？因此，针对于最容易被忽略的部分——损失函数loss，做一些学习的整理，后面会慢慢把其他几个部分补上。 L1_loss$L1~Loss$，顾名思义，源于L1范数。计算target与output的差的绝对值。 \ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad l_n = \left| x_n - y_n \right| \ell(x, y) = \begin{cases} \operatorname{mean}(L), & \text{if reduction} = \text{'mean';}\\ \operatorname{sum}(L), & \text{if reduction} = \text{'sum';}\\ \operatorname(L), & \text{if reduction} = \text{'none'.} \end{cases}L2_loss, MSE_Loss$L2~Loss$, 又称MSE均方误差损失，常用于回归任务的目标优化中。 \ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad l_n = \left( x_n - y_n \right)^2, \ell(x, y) = \begin{cases} \operatorname{mean}(L), & \text{if reduction} = \text{'mean';}\\ \operatorname{sum}(L), & \text{if reduction} = \text{'sum'.}\\ \operatorname(L), & \text{if reduction} = \text{'none'.} \end{cases}NLL_Loss, Negative Log Likehood lossPyTroch中，在用$NLL~Loss$之前，输入需要先经过$logsoftmax = \log\left(\frac{\exp(x_i) }{ \sum_j \exp(x_j)} \right)$，得到每一类$softmax$归一化后的对数似然；然后进行下列公式计算即可。 \ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad l_n = - w_{y_n} x_{n,y_n}, \quad w_{c} = \text{weight}[c] \cdot \mathbb{1}\{c \not= \text{ignore_index}\}, \ell(x, y) = \begin{cases} \sum_{n=1}^N \frac{1}{\sum_{n=1}^N w_{y_n}} l_n, & \text{if reduction} = \text{'mean';}\\ \sum_{n=1}^N l_n, & \text{if reduction} = \text{'sum'.}\\ \operatorname L, & \text{if reduction} = \text{'none'.} \end{cases}当然也可以直接采用$CrossEntroy~Loss$，因为内部集成了$logsoftmax$函数，而无需再手动将输入进行$logsoftmax$变换。$NLL~Loss$和$CrossEntroy~Loss$适用二分类任务以及多类分类。&gt;&gt;&gt; # classification task &gt;&gt;&gt; m = nn.LogSoftmax(dim=1) &gt;&gt;&gt; loss = nn.NLLLoss() &gt;&gt;&gt; # input is of size N x C = 3 x 5 &gt;&gt;&gt; input = torch.randn(3, 5, requires_grad=True) &gt;&gt;&gt; # each element in target has to have 0 &lt;= value &lt; C &gt;&gt;&gt; target = torch.tensor([1, 0, 4]) &gt;&gt;&gt; output = loss(m(input), target) &gt;&gt;&gt; output.backward() &gt;&gt;&gt; &gt;&gt;&gt; # 2D loss example (used, for example, with image inputs, segmentation task) &gt;&gt;&gt; N, C = 5, 4 &gt;&gt;&gt; loss = nn.NLLLoss() &gt;&gt;&gt; # input is of size N x C x height x width &gt;&gt;&gt; data = torch.randn(N, 16, 10, 10) &gt;&gt;&gt; conv = nn.Conv2d(16, C, (3, 3)) &gt;&gt;&gt; m = nn.LogSoftmax(dim=1) &gt;&gt;&gt; # each element in target has to have 0 &lt;= value &lt; C &gt;&gt;&gt; target = torch.empty(N, 8, 8, dtype=torch.long).random_(0, C) &gt;&gt;&gt; output = loss(m(conv(data)), target) &gt;&gt;&gt; output.backward() CrossEntroy_Loss, 交叉熵loss其实就是$logsoftmax$函数和$NLL~Loss$的集成。用$NLL~Loss$时需要先对网络forward后的原始输出进行$logsoftmax$再传入；而用$CrossEntroy~Loss$直接传入原始输出即可，内部进行归一化。 BCE_Loss$BCE$，binary_cross_entroy， 是交叉熵损失的二分类下的特例。应用之前，需要先对输入经过$sigmoid$函数，得到归一化值。现在来想一想，把$x_n = sigmoid(x_n)$算入在内，将下面公式中的$\log x_n$看作是一个整体，那么$\log x_n$与$ \log(1 - x_n)$其实可以理解为二类下原始数据先求$softmax$再求$log$，和$cross~entory~loss$一样。至于这里为什么用$sigmoid$函数而不用$softmax$函数?$softmax= \frac{\exp(x_i) }{ \sum_j \exp(x_j)} = \frac{1}{1+ \exp(x_0 - x_1)}$要比$sigmoid(x_1) =\frac{1}{1+ \exp(x_1)}$需要每个batch的$x_0$缓存空间，但是$x_0$作为常数不参与反向求导，所以两者的本质是一样的。所以更多的采用sigmoid函数。还不懂为啥本质一样？有详解 \ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad l_n = - w_n \left[ y_n \cdot \log x_n + (1 - y_n) \cdot \log (1 - x_n) \right],BCEWithLogits_Loss其实就是$Sigmoid$函数和$BCE~Loss$的集成。 \ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad l_n = - w_n \left[ y_n \cdot \log \sigma(x_n) + (1 - y_n) \cdot \log (1 - \sigma(x_n)) \right],Conv 输出尺寸与输入关系这个部分与本文无关，卷积操作后的尺寸问题，记录一下防止忘记： H_{out} = \left\lfloor\frac{H_{in} + 2 \times \text{padding}[0] - \text{dilation}[0]\times (\text{kernel\_size}[0] - 1) - 1}{\text{stride}[0]} + 1\right\rfloor总结这里就介绍6个Loss，但是实质上就是4个Loss，再实质上就是两个Loss，一个范数Loss一个交叉熵Loss。一个适用回归，一个适用分类。这两个任务下基于Loss的改进其实整体上还是基于他们改进。像FocalLoss等等。另外还有三元组损失KLD损失什么的，用不上就不介绍了。]]></content>
      <categories>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
        <tag>loss function</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[安装ubuntu16.04 系统及1080ti配置cuda的记录]]></title>
    <url>%2F2019%2F05%2F11%2F%E5%AE%89%E8%A3%85ubuntu16.04%20%E7%B3%BB%E7%BB%9F%E5%8F%8A1080ti%E9%85%8D%E7%BD%AEcuda%E7%9A%84%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[制作系统盘准备材料： U盘8G，UltralSO 9.7软件（软碟通），ubuntu16.04系统ISO文件系统盘制作具体细节不赘述，按照下图三部走，即可。 系统安装 系统盘插上，开机！点按F11， 直到出现选择引导（SATA硬盘或者USB或者UEFI），选择USB引导； 进入ubuntu系统安装界面；语言：下拉到最后选择中文（简体），然后点击安装ubuntu 进入界面准备安装ubuntu，都不需要选择，点击继续; （更新后续可以在系统里进行） 点击最后的其他选项， 然后现在安装； 进行系统安装位置选择，进行分区； 具体分区根据自己硬盘空间而定，以下表格分区方式以我2T硬盘为例；分区完成后，安装启动引导器的设备选择你装ubuntu系统的硬盘即可； 如/dev/sda; 然后现在安装！ 后面就是一些小的操作了，选择地区 填入计算机名和用户名密码等。正常操作就可以，不再赘述。 直到最后安装完成，重启！ 至此，ubuntu16.04系统安装成功！ 进入ubuntu新系统后，建议开启一波更新操作。而在这之前，最好更换系统源：系统设置 &gt; 软件和更新&gt; 下载自 &gt; 其他站点 &gt; ftp.sjtu.edu.cn 然后密码授权、关闭及重新载入。此时，$ sudo apt update + $ sudo apt upgrade等待完成更新。 挂载点 分区大小 格式 描述 /boot 1G 逻辑分区 空间起始位置 ext4 Linux的内核及引导系统程序所需要的文件，比如 vmlinuz initrd.img文件。在一般情况下，GRUB或LILO系统引导管理器也位于这个目录；如kernels，initrd，grub swap 8G 逻辑分区 空间起始位置 交换空间 相当于Windows中的“虚拟内存”，一般等于物理内存大小 / 300G 主分区 空间起始位置 ext4 相当于Windows中的C盘，根目录，系统盘 /home 1.7TB 逻辑分区 空间起始位置 ext4 除了以上的所有剩余空间都给这个了。为工作目录；后续分配用户账号都在home工作目录下，如/home/cyyan/ 参考1.安装Ubuntu Linux系统时硬盘分区最合理的方法 显卡驱动及配置cuda10.0nvidia官网文件 cuda_10.0.130_410.48_linux.run参考2. cuda toolkit 安装指导手册大全参考3. cuda快速安装一本通 下载cuda_10.0.130_410.48_linux.run文件至/home/cyyan，然后打开参考3链接，仔细阅读一遍，做好操作心理准备； 创建conf文件$ sudo vim /etc/modprobe.d/blacklist-nouveau.conf添加以下内容禁用Nouveau驱动，保存退出后$ sudo update-initramfs -u 重新生成kernel内核和initramfs。 blacklist nouveauoptions nouveau modeset=0 重启进入系统， 在grub选择页面按e键进入编辑启动项，在行首是linux字样的所在行，添加 3 nomodeset至行尾，然后ctrl + x进入系统； 进入黑乎乎的tty1页面后，输入用户名&gt; cyyan和对应密码进入用户目录，然后直接执行$ sudo sh cuda_10.0.130_410.48_linux.run开始安装显卡驱动及cuda10.0 阅读user license agreement并accept同意， （不想阅读按q键结束） Install NVIDIA Accelerated Graphics Driver for Linux-x86_64 410.48 ?，yes啦； Do you want to install the OpenGL libraries? 如果是NVIDiA显卡，yes啦； Do you want to run nvidia-xconfig? 这个不是很清楚哎，那就默认吧no； Install the CUDA 10.0 Toolkit?就是要装这个东西的，yes; Enter ToolKit Location，默认/usr/local/cuda-10.0就可以啦; Do you want to install a symbolic link at /usr/local/cuda? 软连接到/usr/local/cuda； yes; Install the CUDA 10.0 Samples? 这个安装一下啦，后面用来验证cuda是否安装好；yes; Enter CUDA Samples Location，默认位置啦； 然后就等待安装啦！ 一切安装配置完成后，$ sudo nvidia-xconfig创建xorg.conf 文件用于显卡显示；然后$ reboot重启系统； 进入系统后，打开终端进入目录$ cd /usr/local/cuda-10.0/samples/1_Utilities/deviceQuery/编译samples$ sudo make及$ ./deviceQuery，验证cuda；当最后一行Result = PASS时，表明cuda安装好。 打开bashrc文件$ vim ~/.bashrc将以下两行环境变量添加至文档最后，保存退出后执行bashrc文件$ source ~/.bashrc。大功告成！ export LD_LIBRARY_PATH=/usr/local/cuda-10.0/lib64export PATH=$PATH:/usr/local/cuda-10.0/bin 可以尝试检查驱动版本$ cat /proc/driver/nvidia/version和cuda 版本$ nvcc -V。 $ nvidia-smi 查看显卡信息，如下图所示： 基本工具: terminal快捷键 + annconda + pytorch + synergyterminal快捷键打开系统设置 &gt; 键盘 &gt; 快捷键 &gt; 自定义快捷键 &gt; +名称：fastterminal 命令：gnome-terminal --geometry=128x48+300+100 --working-directory=N应用后新建加速键 Ctrl+N ok！效果就是Ctrl+N后 出来terminal；128x48表示大小，横向128，纵向48，+300+100表示位置，以屏幕的左下角为原点；所在目录为home/cyyan anaconda及pytorch安装常规操作，anaconda安装具体可以参考我女朋友的爆款博文然后在anaconda里create 新env 然后安装pytorch 参考官网咯$ conda install pytorch torchvision cudatoolkit=10.0 -c pytorch synergy 和dukto 和teamviewer安装]]></content>
      <categories>
        <category>Ubuntu</category>
      </categories>
      <tags>
        <tag>ubuntu</tag>
        <tag>cuda</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[途经农田有感]]></title>
    <url>%2F2019%2F02%2F16%2F%E9%80%94%E7%BB%8F%E5%86%9C%E7%94%B0%E6%9C%89%E6%84%9F%2F</url>
    <content type="text"><![CDATA[坐在大巴车上，卡农钢琴曲的音乐缓缓流淌，看着车外一闪而过的农田和车内昏昏入睡的大爷大妈们，想到广袤的苏北农村大地，蓦地感慨良多。 也许是最近印度电影看的比较多的缘故，《厕所英雄》和《印度合伙人》中对陈俗文化的“革命”直接给了我这样一个契机— 以前听我老爸讲，他年轻的时候，没有电话，若是家里有什么急事，和远方的人通报只能去邮局打电报，即便如此，电报也十分昂贵，按字收费。直到零几年家里才装上一台电话，全靠了它，爸妈平时才能互通音讯互报平安。后来，老爸老妈又用上了小灵通和翻盖手机。那时候会这样开玩笑：你不移动，电话当然就会没信号啦(中国移动)。那时候的诺基亚真的无人能敌。零九年我上初二的时候，我姐去读职业学校，老妈给她买了一台绿色的滑盖手机，正是那台手机，让我知道了原来音乐不仅是电视和mp3上才有，手机不仅能打电话还能听音乐看视频看电子书简直太神奇了！那时候最火的是乡镇上的手机电脑店，5块钱就能帮你下载很多音乐和电影放到手机上；那时候2G流量5块钱只有30M却也用不完。一二年我读高一的时候，一个同学买的手机震惊了我：没有物理键盘，屏幕超大，还可以用笔用手触摸！简直高端货！别提当时那羡慕劲了！但是直到一四年高考完，我才终于有了人生第一台新手机！虽然是我姑父充了两千多块钱话费赠送的。那时候我姑父向我推荐用支付宝和美团，因为支付宝存钱年化4点7而美团可以在线团购还便宜。现在想想，姑父真是开启我手机新世界的启蒙老师啊！唯一遗憾的是，当时6月份拿到的3G手机，结果到了7月份4G手机就开始普及了，真是个利空！可对社会来说这却是个实实在在的大利好！应该说，从那以后，4G，结结实实地改变了我们的生活—电商、外卖、快递、共享、支付、地图、打车……无所不用手机！连以前的信息不对称赚钱也被流量经营代替：直播创造内容，短视频分享内容。放在几年前，这真的很难想象！ 手机，作为4G网络的载体，几乎彻底颠覆了我们农村以前的生活状态。同时，作为农村快速变化的记录者见证者更是施惠者，它解决了老一辈人跨越地界的一触即“见”的思念；带给了这一辈人外面日新月异的快速发展的中国；尤其是为下一辈人提供了新的更多更全面的机会，去成长去展示。我想，这对于中国尤其中国农村而言，真的算是一场自发性的“手机革命”了。相信当下一辈人成为这一辈人的时候，这种巨变将更加凸显，更加令人深刻！ 2019年1月于高速]]></content>
      <categories>
        <category>life</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[libsvm与perfcurve]]></title>
    <url>%2F2018%2F11%2F03%2Flibsvm%E4%B8%8Eperfcurve%2F</url>
    <content type="text"><![CDATA[直入主题在matlab中，用SVM分类器做二分类问题时，除了准确率，不可避免地需要得到在测试数据上分类结果的auc值，此时我们可以采用matlab自带的函数perfcurve进行，所以我们现在先来看perfcurve的输入输出参数：perfcurve函数功能： 用于实现分类器输出的ROC曲线绘制和对应AUC值计算输入参数： labels, 测试数据的真实label 如 [0;0;0;0;1;1;1;1;1;1] 10*1列向量 scores, 由分类器得到的以正样本列的预测概率 如分类器将样本预测为1的概率 [0.1; 0.2; 0.21; 0.12;...] 10*1列向量 posclass, 正样本类label 如 这里的 1输出参数： X, 用于绘制ROC曲线的x轴坐标 Y, 用于...的y轴坐标 T, 计算X Y时采用的各个阈值 AUC, ROC曲线下的AUC值 ... 好，现在应用perfcurve，我们发现，我们通过matlab自带的svm分类器预测得到的直接是assignment label，而非概率值。为了得到预测概率值，此时，我们需要换用libsvm下的libsvmtrain和libsvmpredict。其中，用于预测的libsvmpredict的输入输出参数有：输入参数： testlabel, 测试数据的真实label，如 [0;0;0;0;1;1;1;1;1;1] 10*1列向量 如果测试数据label未知，可以随意指定值 test_instance, 测试数据的数据， 如m*n的矩阵， 其中m为样本数，n为特征数 model, 训练得到的模型，也就是svmtrain的输出模型 &apos;options&apos;, 一些可选的的参数, &apos;-q&apos;表示不在窗口打印预测信息, &apos;-b 1&apos; 表示需要返回概率估计值。 对于使用&apos;-b 1&apos;需要注意，只有同时在svmtrain函数里也指定才可以，如svmtrain(trainlabel, trainfeats, &apos;-s 0 -t 0 -c 1 -b 1 -q&apos;)输出参数： predict_label, 模型预测的label，如 [0;0;0;1;1;1;1;1;1;1] accuracy, 模型预测准确率， 一个包含了准确率（针对分类），均方误差和平方相关系数（针对回归） decision_values/prob, 模型预测决策值 当&apos;-b 1&apos;指定时， 这里输出为概率估计值，维度m*2 （针对二分类问题）。 好，现在采用libsvmpredict，为了得到预测概率值，我们需要对libsvmtrain和libsvmpredict指定-b 1。接下来，可以进行libsvm训练，测试并得到其准确率和ROC曲线及AUC了。 举例clear;clc;tic;addpath('/usr/local/MATLAB/R2015a/libsvm/matlab/'); load('./train.mat'); % 载入训练数据。1200*5的trainfeats 1200*1的trainlabel 两类样本为0 1 load('./test.mat'); % 载入测试数据。200*5的testfeats 200*1的testlabel %% data preprocessing scale to -1 1[trainfeats, ~] = mapminmax(trainfeats', -1, 1); % the min and max value aim to each row trainfeats = trainfeats';[testfeats, ~] = mapminmax(testfeats', -1, 1); % the min and max value aim to each row testfeats = testfeats';%% model train and predict cmd = '-s 0 -t 0 -c 1 -b 1 -q';model = libsvmtrain(trainlabel, trainfeats, cmd);[predict_label, accuracy, prob] = libsvmpredict(testlabel, testfeats, model, '-b 1'); [X,Y, ~, AUC] = perfcurve(testlabel, prob(:,2), 1); % calculate the aucdisp(accuracy(1))disp(AUC)plot(X, Y, 'r') toc;]]></content>
      <categories>
        <category>Matlab</category>
      </categories>
      <tags>
        <tag>matlab</tag>
        <tag>libsvm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[神经网络与深度学习代码解读]]></title>
    <url>%2F2018%2F08%2F19%2F%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB%2F</url>
    <content type="text"><![CDATA[Below is my NOTES for the first code file Network.py in 《Neural Networks and Deep Learning》wrote by Michael Nielsen Maybe you can find all codes in this book here Also, there are still some mistakes in this note, you can mail to me if you have any problems. Besides, I will try to note more for other codes in the document. """First import the standard library "random" and third-library "numpy" if we have the library which is made by ourself, we can import them then.the order is: &gt;&gt;standard library &gt;&gt;third-party library &gt;&gt;library by ourself"""################以下为class Network定义###################### 应注意块代码的缩进问题 # 而且关于类名的命名 首字母最好大写 括号内为继承的父类 若不写则默认继承自object object属于父类最顶端class Network(object): """__init__用于对bias和weight进行正态分布的初始化""" def __init__(self,sizes): #__xxx__表示这是一个特殊方法 方法中定义第一个为self用于对类进行实例化时自动传递实例 self.num_layers = len(sizes) # len() 用于计算sizes的维度 传递给 实例属性self.num_layers self.sizes = sizes self.biases = [np.random.randn(y,1) for y in sizes[1:]] # 以上np.random.randn(y,1)为进行随机初始化y*1的正态分布的array存入biases # for y in sizes[1:] 为列表推导式 # sizes[1:]为切片操作 # 如：sizes[1:5:2] 表示截取索引为1（包含）至索引为5（不包含）步长为2的序列 # 如果第一个不写默认为0 第二个不写默认为-1 第三个不写默认步长为1 此时后一个：也可以省略 # 因为for y in sizes[1:] 会被执行不止一次 所以产生的bias为多维 # 因此整个np.random.randn外面要加[] self.weights = [np.random.randn(y,x) for x,y in zip(sizes[:-1],sizes[1:])] # 以上zip()函数用于接受多个序列参数 返回tuple list # 如： &gt;&gt;&gt; x = [1,2,3] &gt;&gt;&gt; y = [4,5,6] &gt;&gt;&gt; z = [7,8,9] # &gt;&gt;&gt;xyz = zip(x,y,z) # &gt;&gt;&gt; print xyz &gt;&gt;&gt; [(1,4,7),(2,5,8),(3,6,9)] """feedforward前向传播 输入激活值 输出最后一层激活值""" def feedforward(self,a): # define feedforward method and the parameter is 'a' 注意冒号 for b,w in zip(self.biases,self.weights): a = sigmoid(np.dot(w,a) + b) return a # 经过zip函数后 此时b w为每一层的biases 和 weights # 假设输入为784个神经元 第一个隐藏层为30个神经元 # 则biases为30*1的array weights为30*784的array 第一次a为784*1的array # 因此np.dot(w,a)是array的矩阵乘法 经过for循环后 return 最后一层的激活值 # sigmoid函数为下方定义的函数 用来计算S型函数的值 详细函数定义在下 """SGD 输入为training_data epochs mini_batch_size eta (test_data) 整个method用于执行epochs次迭代期 每次迭代期打印数据 同时每一迭代期内 随机选取小批量样本数据 每一mini_batchs 计算梯度并更新bias and weights""" def SGD(self,training_data,epochs,mini_batch_size,eta,test_data=None): # define SGD method and default parameter 'test_data' is None， # 迭代期epochs 小批量数据mini_batch_size 学习速率eta if test_data: n_test = len(test_data) n = len(training_data) # 计算training_data的长度 同时如果test_data is not none 计算test_data 的长度 for j in xrange(epochs): # 循环epochs迭代期的次数 以下为一个epoch做的工作 random.shuffle(training_data) # random.shuffle(x) 随机将list中数据进行乱序打乱 mini_batches = [training_data[k:k+mini_batch_size] for k in xrange(0,n,mini_batch_size)] # 对于已经打乱的training_data # 从第0个数据开始 # 每隔mini_batch_size切出一块training_data 全部放入mini_batches list中 # 此时 mini_batches 可看作(n/mini_batch_size) 份 mini_batch_size training_data的array # （实际上 training_data仍旧有维度） for mini_batch in mini_batches: self.update_mini_batch(mini_batch,eta) # 对于mini_batches中每份mini_batch_size training_data # 执行update_mini_batch() 更新实例属性self.biases和weights # 下文method中 if test_data: print "epoch &#123;0&#125;:&#123;1&#125;/&#123;2&#125;".format(j,self.evaluate(test_data),n_test) else: print "epoch &#123;0&#125; complete".format(j) # string.format()函数 格式化字符串 通过&#123;&#125;代替% # 如：&gt;&gt;&gt;'&#123;0&#125;,&#123;1&#125;'.format('name',2017) 则print 为 'name,2017' # 如果有test_data print "epoch %d:%d/%d " %(j,evaluate(test_data),n_test) # 否则 print 形如 "epoch 3 complete" # range(1,6,2)为产生从1（包含）到6（不包含）步长为2的list # 第一个数默认为0 第三个数默认为1 第二个数不可省略 # xrange()和range()类似 但是xrange不产生整个list 适用于大范围list 不会在内存创建list 只在循环中使用 # 如：xrange(5) 则实际产生[1，2，3，4] 但是不在内存创建 """update_mini_batch method 输入mimi_batch eta 该方法执行一次后即完成一次mini_batch 然后直接更新实例属性""" def update_mini_batch(self,mini_batch,eta): nabla_b = [np.zeros(b.shape) for b in self.biases] # 初始化一个和biases相同维度的全零array 用于存储梯度和 nabla_w = [np.zeros(w.shape) for w in self.weights] # 同上 for x,y in mini_batch: # 该for循环用于累加计算整个mini_batch上的梯度的和 delta_nabla_b, delta_nabla_w = self.backprop(x,y) # 计算一个mini_batch的梯度 nabla_b = [nb + dnb for nb, dnb in zip(nabla_b,delta_nabla_b)] # 迭代计算将此次mini_batch上的b的梯度加上nabla_b 再传递给nabla_b # 在以上这一for循环中 执行一次是执行一层 最后形成形如biases的array 传递给nabla_b nabla_w = [nw + dnw for nw, dnw in zip(nabla_w,delta_nabla_w)] # 同上 self.weights = [w-(eta/len(mini_batch)*nw for w,nw in zip(self.weights,nabla_w)] # 更新weights （每个mini_batch更新一次） # w(this mini_batch) = w(last mini_batch) 减去 （eta*（梯度的和）/mini_batch_size) self.biases = [b-(eta/len(mini_batch)*nb for b,nb in zip(self.biases,nabla_b)] # 同上 # x.shape 返回array或者matrix x 的维度 以tuple的形式 """backpropagation 应用feedforward计算每一层的带权输入和激活值 然后计算输出层的误差 再应用backpropagation计算every layers every neurons 的误差 求得关于b的梯度 关于w的梯度为误差*前一层对应激活值（转置）""" def backprop(self,x,y): nabla_b = [np.zeros(b.shape) for b in self.biases] # 初始化一个和biases相同维度的全零array 用于存储b w的梯度 nabla_w = [np.zeros(w.shape) for w in self.weights] # 同上 activation = x # 将输入赋给activation 作为激活值 activations = [x] # 作为array的元素传给activations list zs = [] # 初始化一个用来放置带权输入的zs list for b,w in zip(self.biases,self.weights): z = np.dot(w,activation) + b # 计算带权输入z for表示每一层每一层的计算 zs.append(z) # 将元素z（代表一层的z）增加到zs list中 zs.append(元素) activation = sigmoid(z) # 计算S型函数 即为该层的激活值 # 以上7行代码为前向传播 计算每一层的带权输入z和激活值 delta = self.cost_derivative(activations[-1],y) * sigmoid_prime(zs[-1]) # BP1 activations[-1]为输出层激活值 （二次代价函数的导数即为a-y） # 所以以上公式为求输出层误差delta nabla_b[-1] = delta # BP3 神经元的误差即为其关于b的偏导数 所以将输出层上C关于b的梯度存入nabla_b[-1] nabla_w[-1] = np.dot(delta, activations[-2].transpose()) # BP4 当前层C关于w的梯度是当前层的误差*前一层的激活值 # x.transpose()为对array x 转置 # 若神经元为【4，3，2】 则delta为2*1 activations[-2]为3*1 # activations[-2].transpose()为1*3 此时矩阵相乘则得2*3 array 为输出层w梯度 for l in xrange(2,self.num_layers): # 从倒数第二层到第二层 应用backpropagation z = zs[-l] sp = sigmoid_prime(z) delta = np.dot(self.weights[-l+1].transpose(),delta)*sp # BP2 计算(-l)层神经元误差 l层w的转置*l层的误差*l层S型函数的导数 nabla_b[-l] = delta nabla_w[-l] = np.dot(delta,activations[-l-1].transpose()) return (nabla_b,nabla_w) # 返回整个神经网络每一层每一神经元的b和w的梯度 """测试函数 输入test_data 输出为the target numbers in all test_data numbers""" def evaluate(self,test_data): # 计算test_data number中达到target的总数 test_results = [(np.argmax(self.feedforward(x)),y) for (x,y) in test_data] # np.argmax()用于选取list中最大值，同时将该最大值和对应test_data的y作为一个tuple，然后放入list test_results return sum(int(x == y) for (x,y) in test_results) # 比对list中每个tuple的x与y 如果相等为1 否则为0 然后对整个求和 返回值即为target numbers in all numbers """function cost 的导数derivative 返回值为输入参数out_activations 和y的差 纯粹为了直观""" def cost_derivative(self,output_activations,y): return (output_activations - y)################以下为函数定义#####################def sigmoid(z): # The sigmoid function. return 1.0/(1.0+np.exp(-z)) #np.exp(x) numpy内置函数 返回e的x次方 实际输入为array""" 如果是手动输入一个3*1的list需要先显式将list转化为array 再进行np.exp(-x)运算 否则exp()无法进行list中元素取负号的运算 运行np.exp(x)则不会 如果随机化一个array 则不会出现任何问题且能计算该array中所有元素的exp()的值"""def sigmoid_prime(z): # Derivative of the sigmoid function return sigmoid(z)*(1-sigmoid(z)) # S型函数的导数]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[matlab下操作SVM种种]]></title>
    <url>%2F2018%2F05%2F14%2Fmatlab%E4%B8%8B%E6%93%8D%E4%BD%9CSVM%E7%A7%8D%E7%A7%8D%2F</url>
    <content type="text"><![CDATA[引言今天用SVM支持向量机对特征数据做分类。虽然matlab自带有svmtrain和svmclassify两个函数，但是只能做二分类，而且师兄说不好做交叉验证，用libsvm更方便一点。所以记录一点在ubuntu上安装使用libsvm的过程。 libsvm下载其实libsvm就是一个matlab工具箱，下载安装这几步和openslide-matlab很相像。libsvm官网地址https://www.csie.ntu.edu.tw/~cjlin/libsvm/工具箱可以自己在官网下载，这里也有下载下来后解压，然后放到matlab安装目录下。 libsvm编译因为我是用的matlab版本的libsvm，所以需要先打开matlab软件进入libsvm目录下的matlab/目录，运行make.m文件。这里有点问题：我make后warning提示什么当前g++版本是4.8.4，不支持mex这个要求的g++4.7这种话，所以我尝试对自己的Ubuntu的gcc和g++进行降级——### 安装4.7版本gcc和g++$ sudo apt-get install -y gcc-4.7$ sudo apt-get install -y g++-4.7 ### 重新建立软连接$ cd /usr/bin$ sudo rm -r gcc$ sudo ln -sf gcc-4.7 gcc$ sudo rm -r g++$ sudo ln -sf g++-4.7 g++ 然后再运行make.m文件就没有warning了。gcc降级参考链接后续需要使用libsvm只需要addpath(&#39;/usr/local/MATLAB/R2015a/libsvm/matlab/&#39;),然后直接调用svmtrain和svmpredict即可。 libsvm使用libsvm具体使用我现在也还有点问题，所以附上libsvm指导手册一份一起学习 libsvm指导手册入门版另外，网上有篇博客，用于训练时的参数选择的代码，个人觉得不错！贴一下链接[关于SVM参数c&amp;g选取的总结帖[matlab-libsvm](https://blog.csdn.net/alextowarson/article/details/4764801) 其他参考链接LIBSVM在Matlab下的使用Matlab中对svmtrain迭代次数MaxIter的设置Matlab-SVM分类器 附录记录一个僵尸币，LOOM，志向远大，干劲十足！它开发的其中一个在线学习solidity语言开发以太坊智能合约的网站，很有意思，推荐一下https://cryptozombies.io/我是自来粉！]]></content>
      <categories>
        <category>Matlab</category>
      </categories>
      <tags>
        <tag>SVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ubuntu上跑Caffe-SegNet]]></title>
    <url>%2F2018%2F04%2F14%2Fubuntu%E4%B8%8A%E8%B7%91Caffe-SegNet%2F</url>
    <content type="text"><![CDATA[引言本文记录一下用caffe-segnet训练的过程。虽然现在结果还没出来，但是我已经预感得差不多了，是的，结果不是很好… (写在测试时，结果还ok啊！)注意:在segnet分割网络中， 注意你的mask一定是uint8型，(0,255)值中的0代表第一类，1代表第二类，2代表第三类…等等。 而不能用logical二值，是会出问题的！ 前期准备 根据自己的cudnn版本确定安装好caffe-segnet版本 到github上下载SegNet-Tutorial文件 自己数据集，以train和test以及train_mask,test_mask形式存放于对应文件夹(我这里是自己做的数据集，搞了一周才把前列腺穿刺数据中的腺体画出来取块并做好了mask) 参考链接 SegNet官方教程 我师兄的两篇博客 第一步，制作数据集这一部分基本没有什么干货，主要就是谈一下心路历程。其实搞大数据啊深度学习计算这些，有两种路子，一个是算法开发模型设计，这时候对基础要求高，需要自己设计网络设计算法等等，但是对数据集要求不高，只要拿通用的数据集测试算法性能即可；另一个路子就是拿自己的数据集在别人先进的算法上做，也就是走的应用路线；这时候，自己能根据自己项目的需求、问题，选择合适的算法并制作筛选合适的数据集就相当重要。其实，这个时候，数据集怎么做，做得怎么样，就至关重要了，因为基本上先进的算法都是经过在通用数据集上评估的。anyway, 不管走什么路线，都是要付出心血去做的。不然搞什么都不行。好吧好吧，跑偏了。说一下，训练和测试数据做好之后，需要做一个train.txt和test.txt甚至val.txt，这个就不多说了，主要就是要保证在训练时网络能寻找到对应每张图像的mask。 因为原图和mask是在不同文件夹的。另外我有个个人建议，每张图mask的名称和原图保持一致，只仅仅多个_mask后缀。如：15019_1_1.png和15019_1_1_mask.png。有一点需要注意是，在matlab中保存二值logical图像数据只能以png格式保存 第二步，更改prototxt文件首先更改segnet_train.prototxt和segnet_inference.prototxt中的source:后面的train.txt及test.txt文件位置，改为自己的存放位置；然后更改这两个文件中最后位置的num_output=11为自己的类别，如num_output=2，其中11和2表示了自己的分割类别；同时更改segnet_train.prototxt中的ignore_label=11为2,ignore_label=2下方的class_weighting在第三步有讲解；再然后，更改两个文件中的upsample_w和upsample_h为自己训练图片的尺寸对应的当时大小；如我的训练图像为200x200，而例程是480x360， 所以需要将原来的30,23改为13,13,原来的60,45改为25,25。两个文件中都有需要改的地方，总共是2x2x2个位置需要修改,注意这里一定要改的；最后，修改segnet_solver.prototxt的net位置和snapshot_prefix位置，可自行参考注释，不再赘言；包括是采用gpu还是cpu跑网络，都可以在这里更改。 第三步，更改class_weighting值在segnet_train.prototxt文件中，拉到最后，会有形如class_weighting的11个值，表示了所有类别各自占比；可以按如下代码计算自己数据集中不同类别的比重，并更改其中的值，其他的可以注释掉。%% below is used for calculating the class weighting when i run the segnet. CHECK the variable 'total'clear;clc;tic;Path='/home/yann/SegNet/Data/train_mask/';files=dir(Path);for k=3:length(files) subpath=[Path,files(k).name]; name=files(k).name; image=imread(subpath); I=image; img = I; label_num=double(unique(image)); element(:,1)=[0;1]; % 2class 3class is [0;1;2] for j=1:length(label_num) a=label_num(j); e=length(find(img==a)); element(j,k-1)=e; endendnum=element(:,2:end);sum_num=sum(num,2);median=sum(sum_num)/length(sum_num);class_weighting_=median./sum_num;total=[element(:,1), class_weighting_];toc; 第四步，开始训练在开始训练前，如果希望很快收敛，可以采用ImageNet训练的VGG模型参数来初始化segnet模型参数，下载放到SegNet/Models/下：VGG_ILSVRC_16_layers模型参数#/home/yann/SegNet/caffe-segnet/build/tools/caffe train -gpu 0 -solver ./Models/segnet_solver.prototxt # This will begin training SegNet on GPU 0#/home/yann/SegNet/caffe-segnet/build/tools/caffe train -gpu 0 -solver ./Models/segnet_basic_solver.prototxt # This will begin training SegNet-Basic on GPU 0/home/yann/SegNet/caffe-segnet/build/tools/caffe train -gpu 0 -solver ./Models/segnet_solver.prototxt -weights ./Models/VGG_ILSVRC_16_layers.caffemodel 2&gt;&amp;1|tee ./Log/result.log # This will begin training SegNet on GPU 0 with a pretrained encoder 以上最后一段加上了重定向输出log文件到Log文件夹下 最后，测试结果打开/Scripts/compute_bn_statistics.py和/Scripts/test_segmentation_camvid.py，更改caffe安装目录；运行如下，记得更改为自己的caffemodel，/Inference/用于保存计算到的均值和方差数据的文件夹：python compute_bn_statistics.py ../Models/segnet_train.prototxt ../Training/segnet_iter_29000.caffemodel ../Inference/ # compute BN statistics for SegNet 打开/Scripts/test_segmentation_camvid.py文件，更改11为自己的类别数量，然后可以选择将label_colours的颜色值更改为自己喜欢的颜色。即可。运行如下，将iter后面的数字改成自己测试集的数据数量即可：python test_segmentation_camvid.py --model ../Models/segnet_inference.prototxt --weights ../Inference/test_weights.caffemodel --iter 1000 # Test SegNet 可以看到预测结果了~~后面可以再根据需求，保存预测分割结果或者怎么样，都可以！ 可选，画出loss曲线将caffe安装目录下的tools/extra/下的parse_log.sh和extra_seconds.py以及plot_training_log.py.example复制到Log/文件夹下，然后./plot_training_log.py.example 6 loss.png result.log可以看到loss和迭代期的曲线图：如图 总结希望以后自己能独立一点，稳重一点。不要被&#39;恶势力&#39;牵住头脑。]]></content>
      <categories>
        <category>Caffe</category>
      </categories>
      <tags>
        <tag>segnet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python使用matplotlib的pyplot包]]></title>
    <url>%2F2018%2F04%2F01%2FPython%E4%BD%BF%E7%94%A8matplotlib%E7%9A%84pyplot%E5%8C%85%2F</url>
    <content type="text"><![CDATA[引言python中读图片显示图片等之前都用的scipy.misc,但是python中scipy.misc.imread和imshow现在好像在1.2.0已经不支持了。开始寻求用新的方式：matplotlib.pyplot—— pyplot读取,显示及关闭一张图像# -*- coding: utf-8 -*-import numpy as npfrom matplotlib import pyplot as pt#data = np.random.randint(0,256,(100,100,3))img = pt.imread('validation_code.jpg') # 从本地读取图像print img.shape # 打印img尺寸Title = 'test'pt.title(Title) # 显示图像时展示的标题pt.axis('off') # 坐标轴的开关 'on' 'off'pt.imshow(img) # 显示的图像数据pt.show() # 只有最后pt.show() 才能展示出来 但是pt.show()使用后是无法自动关闭只能通过鼠标点击关闭# pt.pause(1) # 延时一秒 # pt.close() #和上一行共同组成表示显示一秒后关闭图像 注意close()和pt.show()不能同时使用 pyplot一次显示多张图像# -*- coding: utf-8 -*-import numpy as npfrom matplotlib import pyplot as ptdata = np.random.randint(0,256,(100,100,3)) # 新建一个100*10*3随机矩阵, 代表图像矩阵数据img = pt.imread('validation_code.jpg') # 读取本地图像# 以下展示两张图像， 以1行2列的形式pt.subplot(1,2,1)pt.axis('off')Title1 = 'validataion_code'pt.title(Title1)pt.imshow(img)pt.subplot(1,2,2)pt.axis('off')Title2 = 'Random_image'pt.title(Title2)pt.imshow(data)pt.show() pyplot循环显示图像pyplot循环显示图像,即显示一张图像一定时间，然后自动关闭图像，进入下一循环再次显示# -*- coding: utf-8 -*-import numpy as npimport openslideimport timeimport mathimport matplotlib.pyplot as pltstart=time.time()img_WSI = '/media/yann/FILE/MI___Data/Prostate_data/UPenn_Prostate_Biopsy/50533.svs'slide = openslide.open_slide(img_WSI)[w, h] = slide.dimensionsx_max = int(math.floor(w/5000))y_max = int(math.floor(h/5000))for i in range(x_max): y_start = 0 x_start = i*5000 for j in range(y_max): y_start = j*5000 img = np.array(slide.read_region((y_start, x_start), 0, (5000, 5000))) # 这里将24000改成56000 Title = 'Pic_%d_%d_'%(x_start,y_start) plt.imshow(img) plt.title(Title)# plt.show()# plt.imsave('/home/yann/svs_read_patch/'+Title+'.jpg',img) plt.pause(1) plt.close()end = time.time()print end-start 参考链接matplotlib（一）——pyplot使用简介python 图像的显示关闭以及保存python matplotlib 显示图像matplotlib中ion()和ioff()的使用]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>matplotlib</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ubuntu上matlab安装openslide包]]></title>
    <url>%2F2018%2F03%2F26%2Fubuntu%E4%B8%8Amatlab%E5%AE%89%E8%A3%85openslide%E5%8C%85%2F</url>
    <content type="text"><![CDATA[前提条件我之前装过openslide的python版，所以在按照下述安装openslide-matlab版时，最好先装一下openslide-python版,如下：sudo apt-get updatesudo apt-get install openslide-toolssudo apt-get install python-openslide openslide-matlab安装配置 首先到github上下载openslide-matlab包； 然后下载openslide原生包； 将下载好的文件解压，然后将解压文件统一放到matlab安装目录下，我这里目录是/usr/local/MATLAB/R2015a/: cd /usr/local/MATLAB/R2015a/sudo mkdir openslide-matlabsudo cp -r /home/yann/Downloads/openslide-matlab-master/* /usr/local/MATLAB/R2015a/openslide-matlab/sudo mkdir openslidesudo cp -r /home/yann/Downloads/openslide-3.4.1/* /usr/local/MATLAB/R2015a/openslide/ 放过去之后，到matlab里运行/usr/local/MATLAB/R2015a/openslide-matlab/openslide_load_library.m文件；如果报错 libopenslide.so.0 not available on the MATLAB path 是因为没有将文件libopenslide.so.0和运行文件openslide_load_library.m放到同一目录，所以执行。然后将该文件复制到/usr/local/MATLAB/R2015a/openslide-matlab/目录，如下：sudo find / -name libopenslide.so.0 -type f #查看文件libopenslide.so.0位置# locate libopenslide.so.0 #或者用这个检索sudo cp -r /usr/lib/x86_64-linux-gnu/libopenslide.so.0 /usr/local/MATLAB/R2015a/openslide-matlab/ 再次运行m文件，如果报错openslide.h not available on the MATLAB path 则将/usr/local/MATLAB/R2015a/openslide/src/下所有文件复制到同等目录下，命令如下：sudo cp -r /usr/local/MATLAB/R2015a/openslide/src/* /usr/local/MATLAB/R2015a/openslide-matlab/ 再次运行m文件，应该没有问题了，此时可以使用openslide-matlab版了。 参考链接我师兄的’ubuntu安装openslide-matlab’]]></content>
      <categories>
        <category>Matlab</category>
      </categories>
      <tags>
        <tag>openslide</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[当我重装ubuntu时我该怎么做]]></title>
    <url>%2F2018%2F03%2F25%2F%E5%BD%93%E6%88%91%E9%87%8D%E8%A3%85ubuntu%E6%97%B6%E6%88%91%E8%AF%A5%E6%80%8E%E4%B9%88%E5%81%9A%2F</url>
    <content type="text"><![CDATA[引言这篇post不做过多介绍，主要方便我个人记录参考使用。 具体实施步骤# Make Sure the NVIDIA-Linux-x86_64-390.25.run, cuda-8.0.run, and cudnn-8.0-linux-x64-v5.1.tgz has been in the /home/yann 1 sudo gedit /etc/modprobe.d/blacklist.conf # blacklist nouveau # options nouveau modeset=0# above two lines, add to files' ending# then restart the system 2 lsmod | grep nouveau 3 sudo service lightdm stop 4 sudo chmod 777 NVIDIA-Linux-x86_64-390.25.run 5 sudo ./NVIDIA-Linux-x86_64-390.25.run 6 sudo service lightdm start## then set your system setting and make sure you have been on Internet 7 sudo apt-get update 8 sudo apt-get install git 9 sudo apt-get install python-pip 10 sudo apt-get install python-pip python-dev build-essential 11 sudo apt-get install python-numpy python-scipy python-matplotlib 12 sudo apt-get install vim 13 sudo apt-get install ipython 14 sudo apt-get install firefox 15 sudo pip install -U pip 17 pip -V 19 ibus-daemon -drx # fix the input pinyin 20 cat /proc/driver/nvidia/version # check the nvidia driver 21 ls 22 sudo sh cuda-8.0.run 23 cd /usr/local/cuda-8.0/samples/1_Utilities/deviceQuery/ 24 ls 25 sudo make 26 ./deviceQuery # if you have see the sentence ' result = PASS ' at last line, then OK! 27 sudo gedit /etc/profile# enter this file and add——# 'export LD_LIBRARY_PATH=/usr/local/cuda-8.0/lib64'# 'export PATH=$PATH:/usr/local/cuda-8.0/bin' 28 cd 29 source /etc/profile 30 echo $PATH 31 nvcc -V 32 ls 33 tar -zxvf cudnn-7.5-linux-x64-v5.0-ga.tgz 34 sudo cp cuda/include/cudnn.h /usr/local/cuda/include/ 35 sudo cp cuda/lib64/* //usr/local/cuda/lib64/ 36 cd /usr/local/cuda/lib64/ 37 ls 38 sudo chmod a+r libcudnn* 40 sudo rm -r libcudnn.so libcudnn.so.5 41 sudo ln -s libcudnn.so.5.1.10 libcudnn.so.5 42 sudo ln -s libcudnn.so.5 libcudnn.so 43 sudo ldconfig # check compile 44 cd 45 nvidia-smi 46 sudo pip install tensorflow-gpu==1.2.1 47 python 48 history&gt;when-i-just-install-the-system.sh # import your history command for next configuation 驱动及库链接以下链接可以直接通过wget的方式得到。另外，即便不是cuda8.0，cuda9.0也不错的，但是要注意cudnn和cuda要对应！ http://cn.download.nvidia.com/XFree86/Linux-x86_64/390.25/NVIDIA-Linux-x86_64-390.25.run https://developer.nvidia.com/compute/machine-learning/cudnn/secure/v5.1/prod_20161129/8.0/cudnn-8.0-linux-x64-v5.1-tgz cuda8.0.run官网找不到了 附录送一点福利：wallet：&#123;&quot;secret&quot;:false,&quot;public&quot;:&quot;04fdd2eeb9cc15d255e6f4bbe2270d38b6a810eb10aba4ab6149d3dc004e4b8a9107a1267f5025d0ccd43ac66ae470bdc26371d75ef2436eb1248d5967f0fc55c4&quot;,&quot;private&quot;:&quot;d41c9485296b952b0ac48b6e5aeae65a5c6370984100218c9845ad33580a4bfc&quot;,&quot;address&quot;:&quot;Qr5KXpYJUJUCevnxLZG4GMVuVrjTnCwrW2pkUGAvNc7y&quot;&#125;]]></content>
      <categories>
        <category>Ubuntu</category>
      </categories>
      <tags>
        <tag>tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ubuntu14.04在tensorflow-gpu基础上安装caffe-segnet]]></title>
    <url>%2F2018%2F03%2F25%2F%C2%96ubuntu14-04%E5%9C%A8tensorflow-gpu%E7%89%88%E5%9F%BA%E7%A1%80%E4%B8%8A%E5%AE%89%E8%A3%85caffe-segnet%2F</url>
    <content type="text"><![CDATA[前提条件其实已经装好了TensorFlow-GPU版的意思就是，已经装好了cudnn和cuda。 NVIDIA 驱动装好 已装cudnn5.1 已装cuda 8.0 具体步骤 到github上下载caffe-segnet包，(记住对应你的cudnn版本，我这是5.0以上版本的。之前就在这遇过坑。) 然后解压； 安装依赖包等等，如下： sudo apt-get updatesudo apt-get install libprotobuf-dev libleveldb-dev libsnappy-dev libopencv-dev libhdf5-serial-dev protobuf-compilersudo apt-get install --no-install-recommends libboost-all-devsudo apt-get install libatlas-base-devsudo apt-get install python-dev#sudo apt-get install python-skimage ipython python-pil python-h5py ipython python-gflags python-yaml sudo apt-get install libgflags-dev libgoogle-glog-dev liblmdb-dev 进入caffe-segnet目录，cp Makefile.config.example Makefile.config 这一步可选，sudo pip install -r python/requirements.txt make all -j4 &amp;&amp; make test -j4 &amp;&amp; make runtest -j4,出现PASSED字样表示make成功 make pycaffe -j4 NOTE重点 最后添加caffe环境变量 sudo gedit ~/.bashrc打开文件后，在最后一行加上export PYTHONPATH=/home/yann/caffe-segnet-cudnn5/python:$PYTHONPATH然后保存退出， 终端下执行source ～/.bashrc OK!!! 接下来可以到python里import caffe了！！！ 总结我是发现了，配置这种环境真的很麻烦很麻烦，尤其网上博客特别多而大家机器环境又多不一样， 我都不知道该看哪一篇了。所以，最好的办法是: 有即是无。把所有博客都忽略掉，然后到官网上找资料，一点点耐着性子看，即便是英文，没关系，一步步来，毕竟官网的教程是针对所有人的。这样即便你出问题了，你也可以到github上到issue里找，或者提问。 而不是照着别人的博客安装出现问题时在那里抓耳挠腮大骂fxxk。记住：别人写出来的东西，更多是对某件事情的记录，而且那永远依托在他们个人机器的背景环境和自身技术积累上。我们折腾很久才做成的事情，也许对别人来说易如反掌。 参考链接caffe官网Segnet官网到最后添加环境变量时参考我师兄记录装caffe及一些其他的链接Ubuntu14.04通过make或cmake编译安装caffe]]></content>
      <categories>
        <category>Caffe</category>
      </categories>
      <tags>
        <tag>segnet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用Expect脚本自动从远程下载文件]]></title>
    <url>%2F2018%2F03%2F23%2F%E4%BD%BF%E7%94%A8Expect%E8%84%9A%E6%9C%AC%E8%87%AA%E5%8A%A8%E4%BB%8E%E8%BF%9C%E7%A8%8B%E4%B8%8B%E8%BD%BD%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[引言最近有点想法，很多大文件用本地网络下载很慢，毕竟只有100M带宽；所以想投机取个巧，到vultr上的vps下载文件，毕竟是1000M带宽，然后下载好之后再用scp传到本地。但是有一点不一样的是，要登录然后再写scp root@...很麻烦，所以我想，用sh脚本可以自动执行啊。于是，昨晚看了一点bash脚本语言，诸如echo啊$甚至&lt;&lt; EOF这些，但是像ssh这样的命令在ubuntu中是无法直接交互的，只能通过expect语言(我理解为bash的可交互版)。所以，今天装了一下，尝试一下。 不过，其实还有很多想法想去实现，比如，tomcat的前端后端啊这些的，不过能力有限；而且想捣鼓的太多，反而会散而不精了。 哦，还有，matlab下调用python，这个我还没有看。 一点bash介绍bash其实就是打开ubuntu终端后输入的一些命令，如sudo apt-get update啊cd /home/yann/downloads啊这些的。但是这些都是需要一句句输入的，所以尤其对于那些重复使用的命令，为了减少工作量，可以采用将所有命令写入脚本，然后执行脚本文件。这样，一劳永逸吧！举例,连续执行两条python运行命令,结束后打印Command completed!:#! /usr/bin/bashpython /home/yann/Codes/SegNet/Scripts/compute_bn_statistics.py /home/yann/Codes/SegNet/Models/segnet_train.prototxt /home/yann/Codes/SegNet/Models/Training/1_3/_iter_50000.caffemodel /home/yann/Codes/SegNet/Models/Training/1_3/Inference/# above is for BNpython /home/yann/Codes/SegNet/Scripts/test_segmentation_camvid.py --model /home/yann/Codes/SegNet/Models/segnet_inference.prototxt --weights /home/yann/Codes/SegNet/Models/Training/1_3/Inference/test_weights.caffemodel --iter 20echo "Command completed!" 再举例，${a}调用变量然后打印出来。#! /usr/bin/bash# NOTE# 必须用双引号 不能用单引号 # 赋值等于号左右都不能有空格# 取用变量前面必须加$ 最好用$&#123;a&#125;这样的形式 a="hello test"echo "----------------------"echo "hello world $&#123;a&#125;" 再再举例，执行sudo时，是不能用echo的方式输入的，那么如何输入密码？#! /usr/bin/bash# 第一种方法，管道方式，用`&lt;&lt;EOF xxxx EOF `的方式读取sudo密码， 注意此时sudo后面需要 -Ssudo -S apt-get update &lt;&lt; EOF1234EOF# 第二种方法，文本重定向方式，用` echo xxxx | `的方式读取sudo密码， 同样注意此时sudo后面需要 -Secho 1234 | sudo -S apt-get update 参考链接: ubuntu运行sh脚本sudo自动输入密码 尽管以上几条命令能满足大多数的脚本需求，但是，我的目的是ssh和scp啊。所以，只能通过expect。 ubuntu安装expectsudo apt-get install tcl tk expect是的，需要装个包，但是就一行。 使用expect搞了一中午，终于弄好了，通过ssh进入远程vps终端，然后进入/home/yan目录通过wget ××××方式下载文件；然后退出远程，scp执行远程文件传输，将文件拉到本地来。美滋滋。以后下文件，只要改改变量参数，运行一下就可以了。 首先明确的是，expect和bash地位相同。 新建文件yanng 编写程序首行#! /usr/bin/expect 掌握set variable &quot;value&quot;用法，用于设置变量 并为变量传值 掌握spawn ssh root@ss.yanng.cc用法， spawn用于新建进程 掌握expect &quot;*#&quot;用法，expect命令用于查看当前交互结果是否满足 掌握send cd /home/yan用法，send用于在交互时将字符串发送给expect控制的进程 掌握expect eof用法，结束一个进程 掌握exit用法，退出脚本文件 源代码共享以下脚本实现执行ssh和scp复制操作。#! /usr/bin/expect# NOTE: 通过`expect ×××× `方式执行该脚本文件，或者加权限，然后`./xxxx`执行。 都行# 以下为设置链接 密码和文件名变量 并设置超时等待为一直等待set link "wget yanng.cc/download/pic.py"set password "thisispassword"set filename "pic.py"set timeout -1# terminal下执行第一个command，sshspawn ssh root@ss.yanng.ccexpect &#123; "*password:" &#123;send "$password\r";&#125; "yes/no" &#123;send "yes\r";exp_continue&#125;&#125;expect "*#"send "cd /home/yan\r"send "$link\r"send "exit\r"expect eof# terminal下执行第二个command，scpspawn scp root@ss.yanng.cc:/home/yan/$&#123;filename&#125; /home/yann/Downloadsexpect "*password:"send "$password\r"expect eofexit 结语以后可以用脚本了，再也不用打那么多命令啦。努力，努力！ 参考链接 expect简介和使用例子 Ubuntu使用Spawn和expect实现ssh自动登陆 shell里面输入密码 Expect使用小记]]></content>
      <categories>
        <category>SS</category>
      </categories>
      <tags>
        <tag>vultr</tag>
        <tag>expect</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Matlab学习笔记-2]]></title>
    <url>%2F2018%2F03%2F21%2FMatlab%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-2%2F</url>
    <content type="text"><![CDATA[引言对这几天的用到的东西做一个总结。虽然现在网络还没跑起来。 matlab保存mat文件save()函数，详细可以到matlab命令行输入doc save matlab读取某文件夹下所有文件matlab读取文件夹下所有文件， dir(&#39;/folder/&#39;)得到struct，包含fields name date bytes isdir datenum等属性，需要取name属性并存储所有cell(或称单元数组)型然后对数据加以利用；举例如下，读取文件夹下所有数据，然后挨个imread()allfile = dir('/media/yann/FILE/GitLab/Graduation/Data_Kidney/1_nucleus/*.jpg');allname = &#123;allfile.name&#125;;for j = 1:1:length(allname) filename = ['/media/yann/FILE/GitLab/Graduation/Data_Kidney/1_nucleus/', char(allname(j))]; A = imread(filename);end NOTE:cell型数组里的数据仍旧是cell，无法与str连接，必要时需要用char()函数将cell数据转化为str；或者可以用strcat()将两个字符串链接起来 python读取mat文件from scipy import iodata = io.loadmat('/home/yann/1.mat') # 读取mat文件type(data) # 得到data为字典格式data.keys() #查看所有键imgdata = data['img'] # 保存键img对应值为imgdata,然后可以操作数据 numpy的矩阵叠加import numpy as nptrainY1 = np.zeros((1000,2)).astype(np.int32)trainY2 = np.ones((1000,2)).astype(np.int32) trainY = np.concatenate((trainY1,trainY2),axis = 0) # np.concatenate()trainY.shape # 得到(2000,2) python随机打乱数据顺序import numpy as nppi = np.random.permutation(len(trainX)) #随机生成len(trainX)个元素的array 随机排列trainX, trainY = trainX[pi], trainY[pi] #以上两行的目的是为了随机打乱数据的次序 同时又保持data与label的对应性 参考链接 Matlab 读取文件夹里所有的文件 python读取文件——python读取和保存mat文件 matlab中(),[],与{}的区别认识]]></content>
      <categories>
        <category>Matlab</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>matlab</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo添加下载功能]]></title>
    <url>%2F2018%2F03%2F18%2Fhexo%E6%B7%BB%E5%8A%A0%E4%B8%8B%E8%BD%BD%E5%8A%9F%E8%83%BD%2F</url>
    <content type="text"><![CDATA[引言经过n多次的test，终于实现了将需要下载的文件存储在本地并上传到github，如果某用户某时需要某文件，可以采用点击下载的方式直接下载到用户本地，或者wget方式直接下载。 虽然之前用七牛云，但是现在这样脱离了第三方，简直太棒了！ 具体操作在source目录下，新建download目录，和_posts``About``tags``categories等目录并列。然后，将你需要分享的文件或者需要展示的图片之类，统一放到该download下；在写文章时，通过诸如[点击下载](/download/xx.exe)这样的链接，直接写入。引用图片，采用![pic](/download/test.png)这样。其他，照旧。 示例点击下载pic.py文件 参考链接Hexo中提供文件下载功能Hexo搭建的博客中如何提供下载文件功能?]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>文件存储下载</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Matlab学习笔记-1]]></title>
    <url>%2F2018%2F03%2F13%2FMatlab%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-1%2F</url>
    <content type="text"><![CDATA[引言今天做病理图像分割，matlab代码，本来打算改成python代码，但是仔细一想，后面还有很多要取中心，取块，算边界的问题，用matlab更方便一点。所以硬着头皮看matlab代码，早晚的事，不如现在就干。python和matlab有一些语法很容易就混淆…… 所以几乎遇到问题就百度。 查看矩阵大小假设A为矩阵，则size(A)为查看矩阵大小，如4行5列中的4,5length(A)给出行数和列数中较大者，如4行5列中的5。[H,W] = size(A) 将4传给H,5传给W。 matlab取模在C或者python中，取模，我们采用5%2，但是matlab中%是注释，所以取模直接是一个函数，mod(5,2) matlab读取存储图像A = imread(&#39;name.jpg&#39;)读取图像， imwrite(A,&#39;name.jpg&#39;)存储图像。存储图像时注意如果出现图像命名需要动态按顺序，可以采取如下方式：mkdir('/home/yann/Split_Image') % 创建文件夹imwrite(I,['/home/yann/Split_Image/',num2str(i),'.jpg']) % I为矩阵数据， i为动态变化数字 具体可见2 matlab图像读取和存储。 matlab循环判断语句记住for if结束后加上end%%% 以下从1到100打印语句for i = 1:1:100 disp(['this is the ', num2str(i), 'number!'])end 具体可见3 matlab的for结构等 matlab 修改图片尺寸在resize之前，一定要确保已经读入文件imread()过。举例：A = imread('/home/yann/a.jpg');B = imresize(A,0.5); % 0.5 为尺寸倍数，可以是大于0的任何数C = imresize(A,[100,100]) % 直接resize为固定尺寸大小 源码分享用程君论文里引用的细胞核分割方法分割细胞核，然后基于核中心取40*40大小的块，并保存每一patch图像。clearclose alltic A = imread('/home/yann/Graduation/histology_segmentation/sample.jpg'); seg = hmt(A,[],false); % get the segmentation result stats = regionprops(seg, 'centroid'); % get seg result matrix 'centroid' datac = struct2cell(stats); % turn the struct data to cell position = int32(cell2mat(c(:))); % turn the cell data to mat format and int32 the double data, position variable is 172*2% below is the loop for getting the patch 40*40 image based on the centroid coordinate as the center of the patch % take the centroid as the base. for i = 1:1:length(position) center = position(i,:); patch = A(max(center(1)-20,1):min(center(1)+20,600), max(center(2)-20,1):min(center(2)+20,600), 1:3); imwrite(patch,['/home/yann/split_image/',num2str(i),'.jpg']); % save the patch image, function `imwrite` if mod(i,20) ==0 disp ([num2str(i), '.jpg', ' has saved !']) % disp function endendtoc 文中hmt()方法出处方法引用出处 其他关于bwlabel() bwboundary() regionprops()等函数有时间再统一分析一下。 参考文献1 matlab查看矩阵大小2 matlab图像读取和存储3 matlab的for结构等Nucleus Segmentation in Histology Images with Hierarchical Multilevel ThresholdingIntegrative Analysis of Histopathological Images and Genomic Data Predicts Clear Cell Renal Cell Carcinoma Prognosis]]></content>
      <categories>
        <category>Matlab</category>
      </categories>
      <tags>
        <tag>matlab</tag>
        <tag>颜色反卷积</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[零碎记录]]></title>
    <url>%2F2018%2F03%2F13%2F%E9%9B%B6%E7%A2%8E%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[关于ubuntu因为upgrade后系统启动无限登陆问题解决方案：重新安装显卡驱动。如果存在显卡驱动文件不在的情况，通过以下方式将文件下载下来再安装驱动wget http://cn.download.nvidia.com/XFree86/Linux-x86_64/390.25/NVIDIA-Linux-x86_64-390.25.run 关于在linux使用终端命令查找文件查找目录： find /（查找范围） -name &#39;查找关键字&#39; -type d &gt;&gt;参考文献&lt;&lt; ubuntu 更改文件及文件夹权限 对Document文件夹下所有子文件相同权限修改sudo chmod -R 777 Document/ 对doudou.jpg文件修改为可读可写可执行权限，sudo chmod 777 doudou.jpg参考链接另外关于777代表的权限和666,700代表的权限有什么不同，可以参考这篇一般，我们修改都是直接777最高权限。]]></content>
  </entry>
  <entry>
    <title><![CDATA[python模拟登陆并爬取数据]]></title>
    <url>%2F2018%2F03%2F08%2Fpython%E6%A8%A1%E6%8B%9F%E7%99%BB%E9%99%86%E5%B9%B6%E7%88%AC%E5%8F%96%E6%95%B0%E6%8D%AE%2F</url>
    <content type="text"><![CDATA[引言这几天玩这个以太妖怪玩得上瘾，先是爬神兽数据，看看神兽图片，看看神兽归属，心里还是挺骄傲的。然后发现查看神兽不需要登陆就能查看，静态网页；但是妖怪信息就必须登陆，动态。所以这几天，工作时间赶工赶点，试着用先前的思路尝试爬取动态网页，抱着试试看的态度结果没想到一上午成功解决问题。特此公告。喜大普奔。 如何查看请求发送地址打开登陆页面，然后按F12查看控制台，选择Network选项，这时在网页点击登陆，查看控制台变化，这时候应该name下会新增几条记录，通过查看这几个name记录查看登陆和验证码的请求地址。具体可查看下图： 源码共享以下是源码信息，和之前那篇爬取神兽的post不一样的部分主要在动态网页模拟登陆。要用代码将登陆请求模拟发送出去。更详细可以通过代码理解。。这次网站内容没有屏蔽，如感兴趣可以尝试。技术是无罪的，本人单纯性地分享源码，如果对网站造成任何损失，本人概不负责。# encoding: utf-8import urllib # 导入urllib模块 import urllib2import re # 导入re模块 import cookielib # cookielib 模块#### add opener and headerscj=cookielib.CookieJar() headler=urllib2.HTTPCookieProcessor(cj) opener=urllib2.build_opener(headler) '''--------------------------------------------------模拟登录--------------------------------------------------'''def login(): login_url = 'https://etmon.cc/loginApi' validation_code_url = 'https://etmon.cc/kaptcha.jpg' # 通过f12的network 调试查看 headers = &#123;'User-Agent':'Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US; rv:1.9.1.6) Gecko/20091201 Firefox/3.5.6'&#125; print "---------登录中-------------------" print '输入登陆邮箱：' email = raw_input() print "输入密码：" password = raw_input() # 请求验证码网页内容，保存登陆页面的验证码图片到根目录，查看并输入，然后回车登陆进入。 t=urllib2.Request(validation_code_url, headers=headers) yzm=opener.open(t) yzmgif=open("validation_code.jpg","w") yzmgif.write(yzm.read()) yzmgif.close() print "到当前目录下查看验证码validation_code.jpg并输入:" validation_code = raw_input() values=&#123;"email":email, "password":password, "verificationCode":validation_code&#125; request=urllib2.Request(login_url,urllib.urlencode(values),headers = headers) f=opener.open(request) print "---------登录成功！----------------" '''---------------------------------------------------传入i参数 用于查看第i只妖怪的需要的数据---------------------------------------------------'''def check_monster_id(i): headers = &#123;'User-Agent':'Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US; rv:1.9.1.6) Gecko/20091201 Firefox/3.5.6'&#125; link = 'https://etmon.cc/monster/' + str(i) request = urllib2.Request(link,headers=headers) page = opener.open(request) content = page.read() re_span = re.compile('&lt;span&gt;(.*)&lt;/span&gt;') # 正则&lt;span&gt;&lt;/span&gt;中间内容 如owner，monster name，state， time etc. re_div = re.compile('&lt;div class="level"&gt;(.*)&lt;/div&gt;') # 正则div中的妖怪等级信息 # 以下为正则到要求数据后并对数据列表做一定的清洗 选择 ele_span = re.findall(re_span,content) ele_div = re.findall(re_div, content) ele_span[0] = ele_div[0] ## 将选择后的数据写入文本，'a'为文本追加模式 filename = open('/home/yann/PC/data.txt','a') if ele_span[5][-3:] != 'eth': for i in ele_span[:6]: filename.write(i+' ') filename.write('\n') filename.close()###################login()for i in range(1,2000): check_monster_id(i) print 'monster_id: ', i, '---complete!' 结语 模拟登陆思路和之前南通大学一键测评脚本代码思路一致，反而更简单了，毕竟只是爬数据，没有模拟更多请求，如购买 出售等信息。。毕竟随随便便就是0.02个eth的，调试不起啊，万一炸天了 。。。 好好学习，用技术创造自己的精神世界，做更好的自己！ 参考链接无。]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python爬取网页图片信息]]></title>
    <url>%2F2018%2F03%2F06%2Fpython%E7%88%AC%E5%8F%96%E7%BD%91%E9%A1%B5%E5%9B%BE%E7%89%87%E4%BF%A1%E6%81%AF%2F</url>
    <content type="text"><![CDATA[引言记录一点昨天现学现卖爬取以太妖怪的代码，以前觉得爬虫很复杂，现在再想，还是挺简单的。思路很清新，主要就是读取网页信息将网页代码全部拿出来，然后用正则表达式去匹配自己想要的内容。 然后把需要的保存到本地即可。稍微复杂一点的可能是中文编码问题，比如爬下来的名字乱码，需要写个函数重新转换一下。 (这个编码转换网上也能找到很多源码)还有点问题的是，有的网站稍微限制爬虫，所以我们爬取时可能需要伪装成chrome或者firefox浏览器， 这个也很容易实现。 不过这个网页没有这个问题。最后一个问题是，动态网站和静态网站的问题，需要模拟登录， 这个上学期帮一个同学做一键评测脚本时遇到过。 也可以解决。 这里网站一部分数据是静态，另一部分数据是动态，需要登录才能爬取。。 有时间再尝试爬去动态部分。。 源码共享以下是源码信息，主要是爬取网站所有神兽图片保存本地，同时以图片名的方式保存神兽归属人信息和神兽编号。当然也把信息print出来了。这里作者将网站xxxx掉了。防止不必要问题。# encoding: utf-8# author: Yanng# Link: http://yanng.cc/import urllib # 导入urllib模块 import re # 导入re模块 import sysreload(sys)sys.setdefaultencoding("utf8")# 转中文格式def translate(str): line = str.strip().decode('utf-8', 'ignore') # 处理前进行相关的处理，包括转换成Unicode等 p2 = re.compile(ur'[^\u4e00-\u9fa5]') # 中文的编码范围是：\u4e00到\u9fa5 zh = " ".join(p2.split(line)).strip() zh = ",".join(zh.split()) outStr = zh # 经过相关处理后得到中文的文本 return outStrfor i in range(1,101): link = 'https://xxxxxxxx/super/' + str(i) page = urllib.urlopen(link) # 读取link页面内容保存页面内容并读取 html = page.read() reg = 'src="(.+?\.png)" type=' # 正则表达式 imgre = re.compile(reg) # 编译正则表达式为对象 answer = re.findall(imgre,html)[0] # 正则匹配 reg 相似的 编译为imgre 并在html文件中寻找 url = 'http://xxxxxxxx/' + answer # 添加图片保存位置信息 形成完整图片路径 name = answer.split('/static/svgs/super/')[-1].split('.png')[0] # 分离图片路径中闲杂信息只保留神兽名称 utf_name = translate(name) # 编码转换 # 查找归属用户 regg = '&lt;span&gt;(.*)&lt;/span&gt;' # 另一正则表达式 imgree = re.compile(regg) user = re.findall(imgree,html)[1] # 查找神兽归属用户信息 if user[-3:] == 'eth': # 判断归属用户信息是否正确 user = re.findall(imgree,html)[3] if user[-3:] == 'eth': user = '000' # 下载图片 if user != '000': urllib.urlretrieve(url,'/root/etmon/'+'%s__%d__%s.png'%(user,i,utf_name)) print link, ' ', user, i, utf_name, '.....complete!' 结语学以致用的感觉，真好；如果还能赚钱，那就更好了。 参考文献Python简单爬取图片实例python提取标签中间的内容python提取文本中的中文]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Vultr搭建ss服务]]></title>
    <url>%2F2018%2F02%2F25%2FVultr%E6%90%AD%E5%BB%BAss%E6%9C%8D%E5%8A%A1%2F</url>
    <content type="text"><![CDATA[这里主要对如何在vultr上购买的cloud instance(vps)进行搭建SS服务进行一个简要的操作说明。 Vultr官网 前言首先贴一下用于pingvultr上各个地区的代码，方便选择心仪延时时间的地区服务器。echo 东京ping hnd-jp-ping.vultr.comecho 新加坡ping sgp-ping.vultr.comecho (AU) Sydney, Australia[悉尼]ping syd-au-ping.vultr.comecho 德国 法兰克福ping fra-de-ping.vultr.comecho 荷兰 阿姆斯特丹ping ams-nl-ping.vultr.comecho 英国 伦敦ping lon-gb-ping.vultr.comecho 法国 巴黎ping par-fr-ping.vultr.comecho 美东 华盛顿州 西雅图ping wa-us-ping.vultr.comecho 美西 加州 硅谷ping sjo-ca-us-ping.vultr.comecho 美西 加州 洛杉矶ping lax-ca-us-ping.vultr.comecho 美东 芝加哥Chicago, Illinois[美东 芝加哥]ping il-us-ping.vultr.comecho 美中 德克萨斯州 达拉斯ping tx-us-ping.vultr.comecho 美东 新泽西ping nj-us-ping.vultr.comecho 美东 乔治亚州 亚特兰大ping ga-us-ping.vultr.comecho 美东 佛罗里达州 迈阿密ping fl-us-ping.vultr.com 另外，值得一提的是，Server Destroy是销毁服务器，只有这样，服务器才不会继续扣费！Server reinstall是重新装系统！ 如何以ssh方式连接服务器这里不做过多介绍，可以到参考网站查询,下面描述具体操作 安装依赖包及shadowsockscurl "https://bootstrap.pypa.io/get-pip.py" -o "get-pip.py"python get-pip.pypip install --upgrade pippip install shadowsocks 添加配置文件先vi /etc/shadowsocks.json新建配置文件，然后填写配置信息，如下：&#123;"server": "0.0.0.0","port_password": &#123;"8381": "password1","8382": "password2","8383": "password3","8384": "password4"&#125;,"timeout": 600,"method": "aes-256-cfb"&#125; 以上为多用户配置，以下为单用户配置。两者选择一种设置即可！&#123;"server": "0.0.0.0","server_port": 2018,"password": "12345678","method": "aes-256-cfb"&#125; 防火墙配置systemctl stop firewalld.service 启动Shadowsocksssserver -c /etc/shadowsocks.json -d start 附录(开启bbr魔改版加速)wget -N --no-check-certificate "https://raw.githubusercontent.com/chiakge/Linux-NetSpeed/master/tcp.sh" &amp;&amp; chmod +x tcp.sh &amp;&amp; ./tcp.sh 选择1安装内核安装完成后提示重启，输入y。然后过一会重新ssh连接。接着输入./tcp.sh重新运行脚本，输入4安装bbr魔改版，安装完成，出现魔改版bbr启动成功！最后，需要再次配置防火墙和启动Shadowsocks。同上。 到此，可以科学上网了。 参考网站hellojava]]></content>
      <categories>
        <category>SS</category>
      </categories>
      <tags>
        <tag>vultr</tag>
        <tag>shadowsocks</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ubuntu上如何配置使用ss]]></title>
    <url>%2F2018%2F02%2F24%2Fubuntu%E4%B8%8A%E5%A6%82%E4%BD%95%E9%85%8D%E7%BD%AE%E4%BD%BF%E7%94%A8ss%2F</url>
    <content type="text"><![CDATA[安装shadowsockssudo pip install shadowsocks 来安装shadowsocks 建立配置文件需要建立配置文件以载入信息，操作如下可以在主文件夹下新建json文件, #我这里目录是/home/yann/shadowsocks.json根据个人服务器地址账号 对照下面内容修改填写&#123; &quot;server&quot;:&quot;45.77.153.166&quot;, // 服务器地址 &quot;server_port&quot;:2018, // 端口 &quot;local_port&quot;:1080, // 本地端口 默认1080 &quot;password&quot;:123456, // 密码 &quot;timeout&quot;:800, // 超时时间 需要和服务端一致 &quot;method&quot;:&quot;aes-256-cfb&quot; // 加密方式,需要和服务端一致&#125; 然后保存退出即可接下来可以通过sslocal -c /home/yann/shadowsocks.json运行，此时终端不能关闭 设置代理但是这时候只是连接到服务器是不行的，需要系统设置代理，，全局代理或者pac代理 可以通过安装genpac来生成本地pac文件 也可以借助我上传的文件直接使用如何安装genpac并生成pac文件参考链接我上传的pac文件 将下载的pac文件或者是生成的pac文件位置记住然后到系统设置 &gt;&gt; 网络 &gt;&gt; 网络代理 &gt;&gt; 方法 &gt;&gt; 自动配置URL处填写file:// 后面跟你的pac文件路径，如图，然后点击应用到整个系统即可 到此 可以科学上网 附录当然 如果觉得每次重启电脑都需要重新运行sslocal -c shadowsocks.json嫌麻烦可以用开机自启动的方式设置 ubuntu如何设置开机自启动终端输入gnome-session-properties添加 名称和注释随便填个啥 然后命令填写你需要在终端执行的命令即可 如sslocal -c shadowsocks.json]]></content>
      <categories>
        <category>SS</category>
      </categories>
      <tags>
        <tag>shadowsocks</tag>
        <tag>ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一点通信网络知识实践]]></title>
    <url>%2F2018%2F01%2F24%2F%E4%B8%80%E7%82%B9%E9%80%9A%E4%BF%A1%E7%BD%91%E7%BB%9C%E7%9F%A5%E8%AF%86%E5%AE%9E%E8%B7%B5%2F</url>
    <content type="text"><![CDATA[寒假，以努力学习早起早睡为荣，以不学习赖床熬夜为耻。 Introduction前几天我爸让姐夫和我把家里的老电视机用网络电视盒子接起来，放在饭桌旁，这样他吃饭时能看个新闻看个综艺听个唱歌的啥的（是的，我老爸看电视只有这两个爱好）。所以我们那天下午就把网线扯过来，接上创维的网络机顶盒，当时我不在家里，所以我姐夫和我表哥一起弄得，我回来时他们还没弄好，能收到信号，但是播放视频的时候就是不行，显示码率为0。然后当时我看过之后，以为是AV线的问题，所以我自以为机智地跑去网上买了一根HDMI2AV的转换器。就把那事包下来了。 Related Work很快，今天转换器就到了，我迫不及待地打开包装，把线接上，但是很遗憾的是，出现了同样的问题，能收到信号，但是无法播放视频。然后我百度啊咨询淘宝卖家啊等等等等。一直搞到了中午，，也没弄好。。午饭点了，我老妈让我吃饭说弄不好就不弄了。。这让我十分地无地自容，毕竟前几天我信誓旦旦和我爸妈说，就差一个转换器的问题。。。所以吃完午饭我又动手了。其实用过网络机顶盒的都知道，上面主要就是接一个网线，然后输出是HDMI或者是直接的AV线。当时我想，既然是直接接AV线不行，那一定是需要用到HDMI咯。所以才买HDMI2AV的转换器。。现在不能用，可咋办啊，幸亏我家还有个老电视和中国电信的机顶盒，所以我就拿着那个做对照组，调试（美其名曰）。。那边的机顶盒放到这边不能用，没有信号，这边的创维机顶盒放到那边也不能用，信号灯也是红色。我就想，是啥问题呢，反正后来不知道是什么原因，我就去负责网络线路的路由器那边看了一下，这一看才看出个明白了。原来，因为我爸妈房间的机顶盒是中国电信的，所以这个机顶盒的网络直接用的网络运营商特意开辟的端口，叫，ITV端口（我家网络是电信的）。然后，创维机顶盒那边的网络是普通的网口，我感觉好像发现了什么。于是我换着接来接去，才明白原来是这么回事。的确，中国电信的机顶盒用的网络必须是运营商特意开辟的ITV端口，这样机顶盒他们就是独家独用。但是我们自己买的网络机顶盒，是通用的，所以是需要直接接到网口上的。 接着，第二个问题出现了，我们用的猫上有网口，但是无线路由器上也有端口啊，无线路由器接到了猫上，但是我的网络电视和电脑是直接接到无线路由器的端口上的啊。。怎么上面还有LAN口，WAN口啊。。所以，我机智地又百度了。列数如下： LAN，就是local area net，local嘛，局部的，就是局域网，就是这一个局部的范围可以用的网络，一般就是一个实验室啊这样几个人接到同一路由器内即同一局域网，一般接电脑的网线啊，带网线插口的网络电视啊，都是需要插到路由器的LAN口的，，，还有就是可以接下级路由器等等。。这几个都是从路由器的LAN口接线出去。 WAN，就是wide area net，wide嘛，广泛的，就是广域网，就是你买网络服务时运营商给你的一个光纤接头，就是这一个大的地区，或是市，或是省等等。一般普通的路由器的入口是WAN，即接到猫的输出，这个猫就是对你这个光纤接头的处理调制吧。 所以，WAN一般是一个新局域网的头。或者可以说是负责由外部广域到内部局域的节点吧。 WLAN，就是Wireless area net，顾名思义，就是无线局域网络，就是我们说的无线网了。因为以非有线介质传输，靠无线电波所以受空间距离影响。 举个综合的例子，我们用的无线路由器，其实就是一个囊括了WAN（入口），LAN（输出有线分支），WLAN（无线信号输出）的设备。 通俗点说，WAN口是对外的接口，和运营商、上级网络打交道。LAN和WLAN是对内的接口，内部的电脑、手机、PAD，都是接入到LAN或者WLAN。 Experiment反正搞懂了上面那些之后，把网线重新理了下插了下，再设置电视机顶盒那些，信号也有了，视频也播放了，就莫名的好了。中国电信的机顶盒用itv网口，创维的机顶盒用的普通的网口，然后还有个网口接的路由器的WAN，路由器上LAN口一个接的网络电视，一个接我电脑。。完美 Conclusion不管做什么，一定要坚持一点，不懂不会的东西多考虑考虑为什么，然后寻因溯源，一点点把东西搞懂，接着也许问题也解决了，不是两全其美的事嘛。。（当然，如果还能份外赚钱，那就，更好了…） ReferencesLAN、WAN、WLAN的区别LAN口和WAN口的区别是什么？]]></content>
      <categories>
        <category>life</category>
      </categories>
      <tags>
        <tag>WAN LAN WLAN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于环境变量问题]]></title>
    <url>%2F2018%2F01%2F06%2F%E5%85%B3%E4%BA%8E%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[引言其实今晚我是没打算写环境变量这个问题的，但是因为之前用电脑添加adb模块刷跳一跳的成绩，改变了环境变量，所以导致刚才没法hexo new newpage了，所以我百度了这个问题，恰巧遇到了同样出现问题的同行。。。而且，环境变量这个问题一直我都不怎么懂，所以，这里record一下。 环境变量环境变量是在操作系统中一个具有特定名字的对象，它包含了一个或者多个应用程序所将使用到的信息。Windows操作系统中的path环境变量，用于系统运行一个程序而没有告诉它程序所在的完整路径时，系统除了在当前目录下面寻找此程序外，还到path中指定的路径去找。用户通过设置环境变量，来方便operation。而PATH环境变量中存放的值，就是一连串的路径。不同的路径之间用分号;连接(英文模式下)。若系统始终未找到可用文件，则会报错。以上就是对path环境变量的一个直观通俗的解释。 环境变量位置windows系统，计算机右键属性，点开之后，在左边栏有高级系统设置，点开，然后右下角有个环境变量，点开，就是了。我知道我解释的不清楚，看图总成了吧。。。从左往右看，然后从右往下看。 设置环境变量其实设置path环境变量还是挺简单的， 主要就是将你需要用到的执行文件的绝对路径加入进去，实例：F:\Blog\Blog\node_modules\hexo\bin。然后起个path环境变量名。注意：环境变量名不区分大小写。关于如何在cmd下用命令添加环境变量，可参考文献2。 参考文献hexo 博客出现command not found 解决方案Windows下PATH等环境变量详解]]></content>
      <tags>
        <tag>环境变量</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CapsuleNet论文翻译]]></title>
    <url>%2F2017%2F11%2F30%2FCapsuleNet%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91%2F</url>
    <content type="text"><![CDATA[摘要capsule是一组神经元，其活动向量代表了对象或对象一部分这样特定类型的实体的实例化参数。我们用活动向量的长度代表实体存在的概率，活动向量的方向代表实例化参数。在某一级别的活跃的capsule通过变换矩阵对更高级别的capsule的实例化参数做预测。当多个预测达到一致，更高级别的capsule变得活跃。我们展示了一个训练的多层capsule系统在MNIST上达到很好的效果，尤其在高度重叠数字的辨别上比卷积网络方法更好。为了达到这个结果我们采用了迭代的路由协议机制：低级别capsule倾向于将其输出发送到和活动向量具有更大标量积的高级别capsule。 引言人类视觉通过使用仔细确定的一系列注视点来忽略不相关的细节，以确保只有极小部分的光学阵列以最高分辨率进行处理。我们无法通过’回想’这种方式来理解一个场景的多少知识来自于一系列注视点，以及我们从一个注视点中收集到多少信息。但是在本文中，我们假设从每个注视点上我们不仅获得一个确定的对象及其属性，我们的多层视觉系统还会创建了一个类似于树状结构的解析结构，这里我们忽略这些单注视点的解析树如何在多注视点上协调的问题。通过动态分配内存方式，解析树通常随手构建出来。然而继Hinton之后，我们假设，对于单一的注视点，解析树是从固定的多层神经网络中雕刻出来的，就像雕塑是从岩石上雕刻出来。每层将被分成许多小的神经元组，这称为“capsule”，而且解析树中每个节点对应一个活跃capsule。 使用迭代的路由过程，每个活跃capsule将在上面的层中选择一个capsule作为父项。 对于视觉系统的更高级别来说，这个迭代的过程将解决分配部分给整体的问题。活跃capsule内神经元的活动代表图像中存在的特定实体的各种属性。这些属性可以包括许多不同类型的实例化参数，例如姿势（位置，大小，方向），变形，速度，反射率，色调，纹理等。一个非常特殊的属性是图像中实例化实体的存在性。 表示存在性的一个明显的方法是使用一个单独的逻辑单元，其输出是实体存在的概率。在本文中，我们提出了一个有趣的替代方法，即使用实例化参数向量的全部长度来表示实体的存在，并强制向量的方向表示实体的属性。 我们通过应用一个向量方向不变但幅度缩小的非线性保证capsule的向量输出的长度不超过1。capsule的输出是一个向量的事实使得可以使用强大的动态路由机制来确保capsule的输出被发送到上面层中的适当的父项。最初，输出被路由到所有可能的父项，但却被和为1的耦合系数缩小。对于每个可能的父项，capsule通过将其输出乘权重矩阵来计算“预测向量”。如果这个预测向量和可能父项的输出有一个大的标量积，则存在自顶向下的反馈，这会增加了该父项的耦合系数，并降低其他父项的耦合系数。这增加了capsule对该父项的贡献，从而进一步增加了capsule的预测与父项输出的标量积。这种“路由协议”应该比由max-pooling实现的允许一层中的神经元忽略下面层中本地池中最活跃的特征检测器的原始路由形式有效得多。我们证明我们的动态路由机制是实现分割高度重叠对象所需的“解释”的有效方法。卷积神经网络(CNN)使用的是学习过的特征检测器的转换副本。这使得CNN能够将关于图像中某一位置获得的好的权重值的知识转化为其他位置。已经证明，这对图像解释非常有帮助。尽管我们用向量输出的capsule和动态路由分别替换了CNN中标量输出的特征检测器和最大池化，但是我们仍然希望跨空间复制学习到的知识。为了达到这个目的，除了最后一层capsule以外其他我们都是卷积的。与CNN一样，我们使高级别capsule覆盖更大的图像区域。然而不像最大池化，我们不会丢弃区域内实体确切位置的相关信息。对于低级别capsule，位置信息由处于活动状态的capsule“地址编码”。随着我们升级层次结构，越来越多的位置信息在（译者注：或译作’以’）capsule的输出向量的实值部分被“速率编码”。将地点编码到速率编码的转变和高级别capsule代表有更多自由度的更复杂实体的事实相结合表明，capsule的维度应当随着我们升级层次结构而增加。 如何计算capsule的向量输入和输出有很多可能的方法来实现capsule的总体思路。 本文的目的不是要探索整个空间，而仅仅是为了表明一个相当直接的实现能够很好地工作，而且动态路由可以帮助。我们想要一个capsule的输出向量的长度来表示由capsule代表的实体出现在当前输入中的概率。因此，我们使用非线性的“squashing”函数来确保短向量长度缩小到几乎为零，长向量长度缩小到略低于1。为了充分利用这种非线性，我们将其留给区别学习。 \mathbf{v}_j=\frac{||\mathbf{s}_j||^2}{1+||\mathbf{s}_j||^2}\frac{\mathbf{s}_j}{||\mathbf{s}_j||}这里$\mathbf{v}_j$是capsule$j$的向量输出，$\mathbf{s}_j$是其全部输入。对于除了第一层capsule的所有层，capsule的全部输入$\mathbf{s}_j$是所有来自下层的capsule的预测向量$\mathbf{\hat{u}}_{j|i}$的加权和，其中预测向量由下层capsule的输出$\mathbf{u}_i$乘权重矩阵$\mathbf{W}_{ij}$ \mathbf{s}_j = \sum_i{c_{ij}\mathbf{\hat{u}}_{j|i}}\mathbf{\hat{u}}_{j|i} = \mathbf{W}_{ij} \mathbf{u}_i这里$c_{ij}$是由迭代动态路由进程决定的耦合系数capsule$i$与上述层中的所有capsule之间的耦合系数和为1，并由“路由softmax”确定，路由softmax的初始对数$b_{ij}$是capsule$i$应耦合到capsule$j$的对数先验概率。 c_{ij} = \frac{exp(b_{ij})}{\sum_k{exp(b_{ik})}}对数先验可以与其他所有权重同时被有差别地学习。这取决于两个capsule的位置和类型，而不是当前的输入图像。然后通过测量上层中的每个capsule$j$的当前输出$\mathbf{v}_j$与capsule$i$做出的预测$\mathbf{\hat{u}}_{j|i}$之间的一致性来迭代地改进初始耦合系数。一致性就是标量积$a_{ij}=\mathbf{v}_j.\mathbf{\hat{u}}_{j|i}$，这个一致性被看作是对数似然，并被添加到初始逻辑$b_{ij}$中，然后计算所有将capsule$i$连接到更高级别的capsule的耦合系数的新值。在卷积capsule层，每个capsule使用网格的每个成员以及每种capsule的不同变换矩阵将向量的局部网格输出到上述层中的每种capsule类型。 数字存在性的边缘损失我们使用实例化向量的长度代表一个capsule实体存在的概率。当且仅当数字出现在图像中时，我们希望对于数字类$k$的顶层capsule有一个长的实例化向量。为了表示更多数字，这里对每一数字capsule$k$使用独立边缘损失$L_k$ L_k = T_k \ max(0, m^+ - ||\mathbf{V}_k||)^2 + \lambda\ (1 - T_k)\ max(0,||\mathbf{V}_k||- m^-)^2当且仅当$k$类数字存在，$T_k = 1$，$m^+ = 0.9\ \ m^- = 0.1$ 对于缺少数字类别的损失的 $\lambda$ 降权将停止从缩小所有数字capsule的活动向量的长度的最初的学习。 我们使用$\lambda = 0.5$，总损失仅仅是所有数字capsule的损失的总和。 CapsNet 结构一个简单的CapsNet结构如图1，网络结构很浅，只有两层卷积和一层全连接。 Conv1卷积核为256x9x9，步长为1，ReLU激活函数。这一层将像素强度转换为局部特征检测器的活动，然后将其用作主capsule的输入。主capsule是多维实体的最低级别，并且从相反的图形角度看，激活主capsule对应于反转渲染过程。 比起将实例化的部分拼凑在一起以形成熟悉的整体，这是一个非常不同的计算类型，而这正是capsule设计的优点。第二层（主capsule）是有32通道，8Dcapsule的卷积capsule层（如：每一主capsule包含8个有9*9卷积核和2步长的卷积单元）。每一主capsule输出可以看到所有256×81的Conv1单元的输出，其感受野与capsule中心的位置重叠。总共，主capsule有[32×6×6]的capsule输出（每个输出是一个8D向量）并且[6×6]网格中的每个capsule互相分享他们的权重。人们可以将主capsule看作是一个具有等式1的卷积层作为块的非线性。 最后一层（DigitCaps）每个数字类有一个16Dcapsule，每个capsule都接收来自下层所有capsule的输入。 我们只在两个连续capsule之间进行路由发送（如：主capsule和DigitCaps间）。因为Conv1输出是1D，所以在它的空间中没有方向可以达成一致。因此，在Conv1和主capsule之间无法使用路由机制。所有的路由逻辑（$b_{ij}$）初始化为0。因此，最初时一个capsule输出($\mathbf{u}_i$)概率相同地($c_{ij}$)被发送给所有父项capsule($\mathbf{v}_0…\mathbf{v}_9$)。我们用tensorflow实现了它，并使用tensorflow的默认参数进行Adam优化，包括指数改变下降的学习率，到最小化等式4中的边缘损失和。 正则化方法重建我们使用额外的重建损失来促进数字capsule对输入数字的实例化参数进行编码。 在训练期间，我们掩盖了除了正确数字capsule的活动向量以外的所有数据。 然后我们使用这个活动向量重建输入图像。 数字capsule的输出被送到3个全连接层构成的解码器中，像图2描述那样对像素强度进行建模。我们最小化逻辑单元的输出与像素强度之间的差的平方和。 我们把这个重建损失降低了0.0005，这样它就不会在训练期间对边缘损失占主导地位。 如图3所示，当只保留重要的细节时，来自CapsNet的16D输出的重建是稳健的。 在MNIST数据集上应用我们在28×28的MNIST图像上进行训练，这些图像只通过零填充在每个方向扩展2个像素，而没有使用其他数据增强/变形方法。 数据集分别有60K训练数据和10K的测试数据。我们使用单个模型进行测试，没有任何模型平均。 Wan等人通过旋转和缩放来增加数据，达到0.21％的测试误差。没有旋转缩放的话他们只达到了0.39％。我们在3层网络上得到了0.25％测试误差，这在以前只能通过更深的网络才能达到。表1展示了在MNIST数据集上不同CapsNet设置的对应测试错误率，这显示了路由和正则化重构的重要性。添加正则化重构是在capsule向量中执行姿态编码来提高路由性能。baseline是通道为256,256,128的标准CNN。每层卷积核为5x5，步长为1。最后一层卷积后是分别为328和192的两层全连接。最后一层全连接通过dropout连接到10分类softmax层，采用交叉熵损失。baseline也使用Adam优化在2像素扩展的MNIST上进行训练。 baseline的设计是为了在MNIST上实现最佳性能，同时保持计算成本与CapsNet接近。 在参数方面，baseline有35.4M而CapsNet则为8.2M，没有重建子网络时只为6.8M。 capsule的个别维度代表了什么由于我们传递的只是一个数字的编码并且将其他数字置零，因此数字capsule的维度应该学习以这个类的数字被实例化的方式跨越变化空间。这些变化包括笔画粗细，歪斜和宽度。甚至包括特定数字的变化像2的尾部长度。通过使用解码网络，我们可以看到个别维度表示的内容。在计算正确的数字capsule的活动向量之后，我们可以将这个活动向量的扰动版本馈送到解码网络，并观察扰动是如何影响重建的。这些扰动的例子如图4所示。我们发现capsule的一个维度（16个）几乎总是代表数字的宽度。尽管一些维度代表全局变量的组合，但仍有其他维度代表数字的局部的变量。例如，不同的维度分别用于6的上行长度和环的尺寸。 仿射变换的鲁棒性实验表明，与传统的卷积网络相比，每个DigitCaps capsule对于每一类都可以学习更健壮的表示。由于在手写数字中存在自然的偏斜，旋转，风格等变化，训练好的CapsNet对于训练数据的小仿射变换具有适度的鲁棒性。为了测试CapsNet应对仿射变换的鲁棒性，我们在经过填充和转换的的MNIST训练集上训练了一个CapsNet和一个传统的卷积网络（含MaxPooling和DropOut），其中每个示例是一个MNIST数字随机放置在40×40的黑色背景上。 然后我们在affNIST数据集（其中每个例子都是一个随机的经过小仿射变换的一个MNIST数字）上测试了网络。 除了标准MNIST中看到的转换和一些自然变换之外，我们的模型从未受过仿射变换的训练。早期停止训练的CapsNet在扩展的MNIST测试集上达到了99.23％的准确性，在affNIST测试集上达到了79％的准确率。而传统卷积模型在相似参数下在扩展的MNIST测试集上获得相似精度（99.22％），但在专有测试集上仅达到66％准确性。 分割高度重叠数字动态路由可以被看作是允许一个级别的每个capsule关注下一级别的一些活动capsule而忽略其他capsule的并行注意机制。 这使得模型能在即使对象重叠的情况下也能识别图像中的多个对象。Hinton等人提出了分割和识别高度重叠数字的任务（Hinton等人[2000]和其他人已经在相似的领域测试了他们的网络）。路由协议应该能够使用关于对象形状的先验来帮助分割，并且可以避免在像素领域中做更高级分割决策的需要。 MultiMNIST数据集我们在相同集（训练或测试）不同类别的数字上叠加数字来生成MultiMNIST训练和测试数据集。每个数字在每个方向上扩展了4个像素，从而产生36×36的图像。考虑到28×28图像中的数字被限制在20×20的框中，两个数字的边界框平均具有80％的重叠。 对于MNIST数据集中的每个数字，我们生成1K个MultiMNIST examples。所以训练集大小为60M，测试集大小为10M。 MultiMNIST上结果我们在MultiMNIST训练数据上从头开始训练的3层CapsNet模型比baseline的卷积模型达到更高的分类准确性。在高重叠数字对上我们达到了5.0％的分类错误率，和Ba等人在重叠程度要小得多的任务上的顺序关注模型错误率一样（Ba等人的数字框的重叠度是4％）。在由测试集合中的图像对组成的测试图像上，我们将两个最活跃的数字capsule视为CapsNet产生的分类。在重建期间，我们一次选取一个数字，并使用选定的数字capsule的活动向量来重建所选数字的图像（因为我们使用它来生成合成图像，所以我们知道这个图像）。与我们的MNIST模型唯一的区别在于，我们增加了学习率的衰减步长时间到10倍，因为训练数据集较大。图5所示的重建显示CapsNet能够将图像分割成两个原始数字。由于这个分割不是在像素级别，所以我们观察到模型能够在考虑所有像素的情况下正确处理重叠（一个像素点在两个数字上）。每个数字的位置和样式以DigitCaps编码。解码器已经学会重建给定编码对应的数字。它能够不管重叠而去重建数字的事实表明每个数字capsule可以通过它从主capsule层接收的vote [译者注：这一词放在这里很难译]中选取样式和位置。表1强调了在这个任务中路由capsule的重要性。作为CapsNet准确性分类的基准，我们训练了一个先是两个卷积层再是两个全连接层的卷积网络。第一层有512个大小为9×9，步长为1的卷积核。第二层有256个大小为5×5，步长为1的卷积核。在每个卷积层后有一个大小为2×2，步长为2的池化层。第三层是一个1024维的全连接层。三层的激活函数都采用ReLU非线性。最后一层10个单元也是全连接。我们使用TensorFlow默认的Adam优化来训练最后一层输出的S形交叉熵损失。这个模型有24.56M参数，比参数为11.36M的CapsNet多2倍。我们从较小的CNN开始（前两层分别为32和64的5×5步长为1的卷积核，以及512维全连接层），并逐渐增加网络的宽度，直到我们在MultiMNIST数据的10K子集上达到最佳测试精度。我们还在10K验证集上寻找了正确的衰减步长。我们一次解码两个最活跃的数字capsule得到两张图像。然后通过给每个数字分配所有非零强度的像素，这样对于两个数字我们都得到了分割结果。 其他数据集我们在CIFAR10上测试了我们的模型，并且使用7个模型的集合来实现10.6％的误差，其中每个模型都是在24×24的图像块上进行3次路由迭代训练。每个模型与我们用于MNIST的简单模型具有相同的架构，除了有三个颜色通道和64个不同类型的主capsule。 我们还发现，对路由softmax输出引入“全都不是”这一类别很有用，因为我们不期望10个capsule的最后一层能解释图像中的所有内容。10.6％的测试误差是标准卷积网络在首次应用于CIFAR10时达到的。capsule与生成模型相同的一个缺点是，它喜欢考虑图像中的所有内容，所以对杂乱建模比在动态路由中使用额外的“孤立”类别更好。在CIFAR-10中，背景太多，不能在合理大小的网络中建模，这有助于解释性能较差的问题。我们还测试了与我们在smallNORB上用于MNIST数据集的完全相同的体系结构，并且获得了2.7％的测试错误率，这与现有技术水平相当。smallNORB数据集由96x96立体声灰度图像组成。我们将图像大小调整为48x48，并在训练过程中随机进行32x32的裁剪。测试期间，我们使用中心的32x32的块。我们还在仅有73257个图像的SVHN的小训练集（Netzer et al。[2011]）上训练了一个较小的网络。我们将第一卷积层通道的数量减少到64个，主capsule层减少到16个6D的capsule，最终capsule层为8D，在测试集上达到了4.3％的错误率。 讨论三十年来，语音识别技术的发展使用高斯混合的隐马尔可夫模型作为输出分布。这些模型在小型计算机上很容易学习，但是它们有最终致命的代表性限制：与使用分布式表示的递归神经网络相比，它们所使用的其中之一个代表是指数低效的。为了使HMM可以记住的信息数量增加一倍，我们需要对隐藏节点的数量进行平方增加。对于递归网络，我们只需要将隐藏层的神经元数量加倍。现在卷积神经网络成为目标识别的主要方法，那么问一问是否存在可能导致它们消亡的缺点是有意义的。一个很好的答案是卷积网络在推广到新颖的观点方面的困难。处理副本的能力是内置的，但是对于仿射变换的其他维度，我们必须选择在网格点上复制随维度指数增长的特征检测器，或者同样指数形式地增加标记训练集的大小。capsule网络采取将像素强度转换成识别片段的实例化参数向量，然后将转换矩阵应用于片段以预测较大片段的实例化参数的方式来避免这些指数形式的低效。学习编码部分和整体之间内在空间关系的变换矩阵构成了自动推广到新视点的视点不变知识。Hinton提出通过转换自编码器来生成主Capsule层的实例化参数及其系统所需要的转换矩阵以供给外部。我们提出了一个完整的系统，这同样回答了“通过使用活跃较低级别capsule预测的姿势协议可以识别多大多复杂的视觉实体”。胶囊做了一个非常强烈的代表性假设：在图像的每个位置，一个胶囊代表的实体类型至多有一个实例。这个来源于称为“拥挤”感知现象的假设消除了绑定问题，并允许胶囊使用分布式表示（活动向量）进行编码该类型实体在给定位置的实例化参数。这种分布式表示法指数地比通过激活高维网格上的一个点并利用正确的分布式表示来编码实例化参数更高效，然后capsule可以充分利用空间关系可以通过矩阵相乘建模这一事实。胶囊使用随视点变化而变化的神经活动，而不是试图消除活动中的视点变化。这使得它们比诸如空间变换网络的“标准化”方法更具优势：它们可以同时处理不同对象或对象部分的多个不同的仿射变换。正如我们在本文中所展示的那样，胶囊对于处理视觉中最棘手的问题之一的分割也是非常好的，因为实例化参数的向量允许它们使用路由协议。动态路由过程的重要性也得到了视觉皮层中不变模式识别的生物学合理模型的支持。 Hinton提出了动态连接和基于规范对象的参考框架来生成可以用于对象识别的形状描述。 奥尔斯豪森等人对Hinton动态连接进行了改进，并提出了一个生物学上可信的对象表示的位置和尺度不变模型。目前胶囊的研究处于本世纪初用于语音识别的递归神经网络研究的相似阶段。有一些基本的表现理由认为这是一种更好的方法，但是在它能够超越高度发展的技术之前，可能需要更多的小洞见。事实上，简单的胶囊系统已经在分割重叠数字方面提供了无与伦比的性能，这是胶囊值得探索的早期指示。 致谢致谢。]]></content>
      <categories>
        <category>Paper</category>
      </categories>
      <tags>
        <tag>capsnet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo渲染mathjax公式解决方案]]></title>
    <url>%2F2017%2F11%2F28%2Fhexo%E6%B8%B2%E6%9F%93mathjax%E5%85%AC%E5%BC%8F%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88%2F</url>
    <content type="text"><![CDATA[今天写论文翻译，里面有大量数学公式，用latex写的，然后生成博客发现渲染失败，Hexo内的数学公式问题终于引起了我的注意……于是，百度Hexo下latex数学公式显示不出来，结果出来第一个打开看太烦了，然后打开第二个，很整洁，于是我就照着做了，结果接连报错，不过我这么聪明肯定很快解决咯，那篇终究是没有大错的，应该是博主太马虎了……下面我就重新理一理思路，以后有问题解决起来也很方便。Go! 现在的问题是，渲染数学公式，有的可以，有的不可以，，这应该是大家都出现的问题，至于什么原因，大家看刚刚那篇。我只讲方法……首先，卸载当前渲染引擎，装个别的——$ npm uninstall hexo-renderer-marked --save$ npm install hexo-renderer-kramed --save 然后到根目录的node_modules\kramed\lib\rules\inline.js中，第11行左右，进行替换——// escape: /^\\([\\`*&#123;&#125;\[\]()#$+\-.!_&gt;])/, escape: /^\\([`*\[\]()#$+\-.!_&gt;])/, 同时，第20行左右的em变量也要做相应的修改——// em: /^\b_((?:__|[\s\S])+?)_\b|^\*((?:\*\*|[\s\S])+?)\*(?!\*)/, em: /^\*((?:\*\*|[\s\S])+?)\*(?!\*)/, 接下来，到terminal，$ hexo clean，然后$ hexo generate,好了完成，收工回家！ 注意主题里的配置文件要记得把mathjax使能像这样——# MathJax Supportmathjax: enable: true 好了，这下可以收工回家了！]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>MathJax</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[信息熵是什么]]></title>
    <url>%2F2017%2F11%2F16%2F%E4%BF%A1%E6%81%AF%E7%86%B5%E6%98%AF%E4%BB%80%E4%B9%88%2F</url>
    <content type="text"><![CDATA[信息熵是什么？记得之前上数据通信这门课时老师好像讲过，但是并没有理解。直到今天，看到论文中说到distribution statistics, entropy， 想起陆铖在MIIP中的报告，用熵来评估以及提取特征，所以特意查资料看了一下并做此总结。本文大部分知识来自知乎忆臻 信息量在搞懂信息熵之前，我们先来明白一下，信息量是什么？其实信息量就是我们平常说的信息量，对于信息的一种度量方式。我们用$h(x)$表示。 像我们平常说的某件事情信息量很小，其实就是那种不需要动脑子就知道的事情，什么事情不用动脑子？是那种我们可以几乎肯定的事情啊，换句话说， 就是事件发生概率很高。而如果一件事发生概率很低，那么就可以说该事件很难确定，那么’这件事发生’本身，其中包含着很大信息量。举我玩’达芬奇密码’的例子，如果13张牌已经亮出来12张，那么我们不需要考虑对方说过的猜牌语就可以知道剩余的那张牌是什么，也就是，’猜对’这个事件概率很大，包含信息量很小； 如果13张牌只亮出来3张，那么我们去猜某张牌时，要考虑自己有哪几张牌，猜牌附近亮出来的牌值是多少，对方说过什么话………也就是，’猜对’这个事件概率很小，包含信息量很大。所以仔细想想，其实，事件发生的概率和其中包含的信息量，不仅相关，而且是成一个负相关的规律，且不存在负值。后面会有说明。 再举个例子，如果现在有两件不相干的事情，数学术语叫，两独立事件；那么凭上面的理解我们就可以知道，两件事不相干，那么两件事都发生的信息量是各自发生的信息量之和，即$h(A,B) = h(A) + h(B)$；若从概率的角度表示都发生的概率就是$p(A,B) = p(A)\cdot p(B)$。很明显，概率角度的乘积形式，从信息量角度表示却是求和。这也就让我们想到了$log$，能够将乘转换成加；且上面说过，概率和信息量是有相关性的，因此，我们尝试定义： h(x) = -\ log_2\ p(x)这样将$h(x)$和$p(x)$联系起来，同时满足了由乘变加的过渡；至于为什么加负号，其实是为了让信息量保持正数 ( 概率值不大于1，则$log_2$后一定不大于0)。如此一来，也说明了事件发生的概率和包含的信息量的负相关规律，即，$h(x)$是一个递减函数。值得注意的是，信息量是’事件发生’的度量；但我们在不知道结果之前就包含的信息量其实是所有可能发生事件的信息量之和。 信息熵这时候，再来告诉你，信息熵，其实就是在结果出来之前对所有可能结果发生所产生的信息量的期望。就很容易理解了。用公式表示： H(x) = - \sum_{i=1}^n\ p(x_i)\ log_2\ p(x_i)$p(x_i)$代表随机事件$X$为$x_i$的概率公式表示，所有可能结果产生的信息量$-log_2\ p(x_i)$的期望，注意公式没有除以$n$，是因为信息量本身乘的是概率值$p(x_i)$。 反推现在我们来看，信息熵的作用：熵(entropy)是信息不确定性的一个测度。可根据信息熵和概率的函数关系，某事件发生的概率越低，则包含的信息熵越大；同时，发生概率越低，则该事件的不确定性越高。这也就解释了熵对信息不确定性的度量。 致谢Thank for Zhihu]]></content>
      <categories>
        <category>Maths</category>
      </categories>
      <tags>
        <tag>theory</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[十一月的前奏]]></title>
    <url>%2F2017%2F11%2F12%2F%E5%8D%81%E4%B8%80%E6%9C%88%E7%9A%84%E5%89%8D%E5%A5%8F%2F</url>
    <content type="text"><![CDATA[十月份的会议结束了，一直没有收心好好学习，只是在一点点啃书，结果也没啃个几页出来，然后月初就happily去厦门了——厦门是真的很美呐！风土人情咯，风景，气候都nice，虽然经常妖风阵阵，而且最重要的，人都非常nice，连卖花的老奶奶都有一套俘获你心的方式呢！奶奶祝你一路平安！嘿！虽然买了花，倒没有任何不情愿的样子，还挺高兴，这让我想起了双十一的故事——淘宝成交额达1682亿。要知道，这么大的一个交易额到目前，应该说几乎没有多少资金是直接付给了商家的，绝大部分都暂存在第三方平台即淘宝上，为什么？如果直接打给了商家，商家不发货该怎么办；如果不打，收到货后，买家突然失踪怎么办，这都是问题啊，所以淘宝应运而生，把钱存在我这里，钱不会跑，一旦确认收货立马把钱给卖家，这样买家卖家都放心啊。大家都心甘情愿把钱放在淘宝暂存。 是不是？这样，即便在平常，淘宝每天都会有高达亿级别的交易额，按照快递速度普遍在两天，买家确认收货需要一天（实际最多是十天），那么这些交易额三天内都在淘宝这里，这么大现金流，或者说，这么多钱，马爹如果不利用起来，那就不是浪费，是马爹傻了！何况，以马爹的脑子，他利用的地方绝对都让我们想不到！所以啊，我们平民老百姓，接触不到那么多资金，就不会往那方面想。还是naive啊！回到厦门，有关厦门的风土人情以及我美滋滋的旅行，可以去我女朋友博客走一波。。传送门 接下来做什么，双十一剁手啊，再说回淘宝，淘宝套路是真的深啊，商家也是！各种券啊，红包啊，津贴啊，想不到想不到，玩不了玩不了。吾等平民百姓，还是老老实实把钱交出去等收货吧，分析人家套路？只知其一难知其二，实难做到啊！ 愉快的双十一周末就这么过去了。接下来，收心学习。]]></content>
      <categories>
        <category>life</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[矩阵相乘的本质是什么(转)]]></title>
    <url>%2F2017%2F10%2F26%2F%E7%9F%A9%E9%98%B5%E7%9B%B8%E4%B9%98%E7%9A%84%E6%9C%AC%E8%B4%A8%E6%98%AF%E4%BB%80%E4%B9%88%2F</url>
    <content type="text"><![CDATA[开门见山，直接给答案：矩阵乘法的本质是线性空间运动的描述！为了解释清楚这个问题，我们需要补充一些线性代数学习过程中被忽略的基础知识。文章较长，大致分为两个部分。第一部分介绍线性代数里面最基础的部分：空间、坐标系和坐标。第二部分揭示矩阵乘法的本质。基础较好可以直接跳到第二部分。 第一部分，空间与坐标系的建立首先，线性代数里面的线性主要的意思就是线性空间里的线性变换。线性变换或线性映射是把中学的线性函数概念进行了重新定义，强调了函数的变量之间的变换的意义。线性函数的概念在初等数学和高等数学中含义不尽相同（高等数学常常把初等数学的关键概念进行推广或进一步抽象化，初等数学的概念就变成了高等数学概念的一个特例）。 在中学的初等数学里，我们知道，函数f(x)=kx+b（k和b是不变量），称为一元线性函数，因为在平面直角坐标系中这个函数的图形就是一条直线，就是变量（包括自变量和因变量）之间的关系描述为一条直线，所以把这种函数形象地称为“线性”函数；如果b=0，这个函数的外观就变成f(x)=kx的形式了，这是一条过原点的直线。显然，过原点的直线是最简单的线性函数。 在大学的代数里面，为了线性函数的进一步推广（如推广至双线性函数、多线性函数、线性空间、线性泛函…）的远大未来，我们忍痛割“尾”，把一元线性函数f(x)= kx + b的b割舍掉，成了f(x)=kx的形式。呵呵，简单点说，只有过原点的最简单的直线f(x)= kx才被称为一元线性函数。 为什么？只因为不过原点的直线不满足我们对线性函数的比例性的要求。 线性函数表现为直线，这只是几何意义。那么所谓“线性”的代数意义是什么呢？实际上，最基本的意义只有两条：可加性和比例性。用数学的表达来说就是：对加法和数乘封闭。 然后说说空间(space)，这个概念是现代数学的命根子之一。对于空间的理解需要更抽象一些，简单的说，能装东西的就是空间。比如计算机内有存储单元，那么就有内存空间；我们上课有课表，那么就有课表空间；有一个能装载梦境的东西，我们可以叫它盗梦空间。对于数学来说，数学家定义的空间里装载的当然是能运算的东西。从拓扑空间开始，一步步往上加定义，可以形成很多空间。线形空间其实还是比较初级的，如果在里面定义了范数，就成了赋范线性空间。赋范线性空间满足完备性，就成了巴那赫空间；赋范线性空间中定义角度，就有了内积空间，内积空间再满足完备性，就得到希尔伯特空间，如果空间里装载所有类型的函数，就叫泛函空间。 总之，空间有很多种。你要是去看某种空间的数学定义，大致都是“存在一个集合，在这个集合上定义某某概念，然后满足某些性质”，就可以被称为空间。这未免有点奇怪，为什么要用“空间”来称呼一些这样的集合呢？大家将会看到，其实这是很有道理的。 我们一般人最熟悉的空间，毫无疑问就是我们生活在其中的（按照牛顿的绝对时空观）的三维空间，从数学上说，这是一个三维的欧几里德空间，我们先不管那么多，先看看我们熟悉的这样一个空间有些什么最基本的特点。仔细想想我们就会知道，这个三维的空间： 由很多（实际上是无穷多个）位置点组成 这些点之间存在相对的关系 可以在空间中定义长度、角度 这个空间可以容纳运动 上面的这些性质中，最最关键的是第4条。这里我们所说的运动是从一个点到另一个点的移动（变换），而不是微积分意义上的“连续”性的运动。第1、2条只能说是空间的基础，不算是空间特有的性质，凡是讨论数学问题，都得有一个集合，大多数还得在这个集合上定义一些结构或关系，并不是说有了这些就算是空间。而第3条太特殊，其他的空间不需要具备，更不是关键的性质。只有第4条是空间的本质，也就是说：容纳运动是空间的本质特征。 认识到了这些，我们就可以把我们关于三维空间的认识扩展到其他的空间。事实上，不管是什么空间，都必须容纳和支持在其中发生的符合规则的运动（变换）。你会发现，在某种空间中往往会存在一种相对应的变换，比如拓扑空间中有拓扑变换，线性空间中有线性变换，仿射空间中有仿射变换，其实这些变换都只不过是对应空间中允许的运动形式而已。因此只要知道，“空间”是容纳运动的一个对象集合，而变换则规定了对应空间的运动。 下面我们来看看线性空间。线性空间的定义任何一本书上都有，但是既然我们承认线性空间是个空间，那么有两个最基本的问题必须首先得到解决，那就是： 空间是一个对象集合，线性空间也是空间，所以也是一个对象集合。那么线性空间是什么样的对象的集合？或者说，线性空间中的对象有什么共同点吗？ 线性空间中的运动如何表述的？也就是，线性变换是如何表示的？ 我们先来回答第一个问题，回答这个问题的时候其实是不用拐弯抹角的，可以直截了当的给出答案。线性空间中的任何一个对象，通过选取坐标系(基)和坐标的办法，都可以表达为向量的形式。只要你找到合适的坐标轴（也就是基），就建立了一个坐标系，就可以用坐标（表示成向量的形式）表示线性空间里任何一个对象。换句话说，给你一个空间，你就能用基和坐标来描述这个空间中的对象！这里头大有文章，因为向量表面上只是一列数，但是由于向量的有序性，除了这些数本身携带的信息之外，还在对应位置上携带信息。为什么在程序设计中数组最简单，却又威力无穷呢？根本原因就在于此。 由三维扩展到四维的空间的确难以想象，我想给出个人的几点认识供读者参考：对于笛卡儿坐标系，二维坐标系的两个坐标轴互相正交并构成一个平面空间；三维坐标系的坐标轴互相正交且第三个坐标轴垂直于其余两个坐标轴平面，三个坐标轴构成一个立体空间；则四维坐标系中的四个坐标轴互相正交，第四轴必然与其余的三维立体空间垂直，四个坐标轴构成一个超多面体空间… 四维空间的物理解释就是爱因斯坦的时空理论，三维物理空间之外增加了一个与之垂直的时间轴（垂直或正交的意思应理解为不相关，时间和空间在低于光速的尺度内就是没有关系的两个事物，这就是牛顿的世界）。你，作为一个有生命周期的高级动物实际上是个四维动物，因为你的肉体既占有了一个三维小空间同时又占有了另外一维的时间轴上的一段。N 维空间的出现实际上是人们在抽象他所观察到的宇宙事物时出现的概念。 实际上，在以后的线性代数学习中，坐标轴的正交不是必须的，取消了正交的要求后，我们在平面上就可以画出来大于四维以上的空间来了，你就理解了由n 个向量张成的n个空间的理论，进而想象高维空间的图像也就不是一个困难的事情了。 如果有一个线性空间，就能建立一个坐标系，选取一组基和坐标来描述这个空间里的对象。举一个通俗一点的例子，我们学习的教室就是一个空间，先定义它为“教室空间”好了，教室空间中有桌子、椅子、黑板、粉笔、黑板擦，为了说明问题我们就先假设只有这几样东西。那么我们怎么描述这个空间呢？选取基和坐标对吧：根据我们对空间中基的要求：完备性和无关性，我们选取的“坐标轴”为{桌子、椅子、黑板、粉笔、黑板擦}，基的维数是5维的。那么坐标怎么选取呢？坐标就是每件东西的数量，有了基和坐标，我们就能描述我们定义的“教室空间”了，就是这么简单。再举一个例子，鸡兔同笼的问题大家应该都听过，这里也可以定义一个空间，暂且取名“笼子空间”吧，在笼子空间中，“坐标轴”就是{鸡的两只脚和一个头，兔的四只脚和一个头}，基的维数是2维的。坐标就是鸡和兔的数目。再回头观察我们的“笼子空间”和“教室空间”，我们发现教室空间里面的基彼此不包含彼此，具体来说就是桌子不包含椅子的成分，粉笔不包含黑板的成分，我们说基彼此正交。而笼子空间中的两个基都有头和脚，但彼此不能线性表示对方。这时，基就不是正交的。下面回到我们线性代数中的线性空间。线性空间里面装的都是向量。那么这组基怎么选取呢？我们知道，线性空间里的基本对象是向量，所以，基一定也是向量。但有一些要求：对基的要求是数量要够，还要线性无关。 总结一下：我们首先确立一个叫空间的东西，空间里面装着要研究的对象，比如“教室空间”。为了描述对象和对象的运动，就要选取空间中最基本的元素作为基，把基前面的系数叫做坐标，于是建立了坐标系和坐标。注意：“坐标轴”，也就是基，可以是桌子椅子等任何东西，只是在线性代数中我们把基选成了向量。世界是物质的，物质是运动的，接下来肯定要研究一下线性空间中的运动是怎么实现的。 第二部分，矩阵乘法描述运动线性空间中的运动，被称为线性变换。也就是说，你从线性空间中的一个点运动到任意的另外一个点，都可以通过一个线性变化来完成。那么，线性变换如何表示呢？很有意思，在线性空间中，当你选定一组基之后，不仅可以用一个向量来描述空间中的任何一个对象，而且可以用矩阵乘法来描述该空间中的任何一个运动（变换）。而使某个对象发生对应运动的方法，就是用代表那个运动的矩阵，乘以代表那个对象的向量。 简而言之，在线性空间中选定基之后，向量（坐标）刻画对象，矩阵刻画对象的运动，用矩阵与向量的乘法施加运动。 如果以后有人问你矩阵乘法是什么，那么你就可以响亮地告诉他，矩阵乘法的本质是运动的施加。到现在为止，好像大家都还没什么意见。但是我相信早晚会有数学系出身的网友来拍板转。因为运动这个概念，在数学和物理里是跟微积分联系在一起的。我们学习微积分的时候，总会有人照本宣科地告诉你，初等数学是研究常量的数学，是研究静态的数学，高等数学是变量的数学，是研究运动的数学。大家口口相传，差不多人人都知道这句话。但是真知道这句话说的是什么意思的人，好像也不多。简而言之，在我们人类的经验里，运动是一个连续过程，从A点到B点，就算走得最快的光，也是需要一个时间来逐点地经过AB之间的路径，这就带来了连续性的概念。而连续这个事情，如果不定义极限的概念，根本就解释不了。古希腊人的数学非常强，但就是缺乏极限观念，所以解释不了运动，被芝诺的那些著名悖论（飞箭不动、飞毛腿阿喀琉斯跑不过乌龟等四个悖论）搞得死去活来。因为这篇文章不是讲微积分的，所以我就不多说了。有兴趣的读者可以去看看齐民友教授写的《重温微积分》。我就是读了这本书开头的部分，才明白“高等数学是研究运动的数学”这句话的道理。 不过在这个文章里，“运动”的概念不是微积分中的连续性的运动，而是瞬间发生的变化。比如这个时刻在A点，经过一个“运动”，一下子就“跃迁”到了B点，其中不需要经过A点与B点之间的任何一个点。这样的“运动”，或者说“跃迁”，是违反我们日常的经验的。不过了解一点量子物理常识的人，就会立刻指出，量子（例如电子）在不同的能量级轨道上跳跃，就是瞬间发生的，具有这样一种跃迁行为。所以说，自然界中并不是没有这种运动现象，只不过宏观上我们观察不到。但是不管怎么说，“运动”这个词用在这里，还是容易产生歧义的，说得更确切些，应该是“跃迁”。因此这句话可以改成：“矩阵乘法是线性空间里跃迁的描述”。 可是这样说又太物理，也就是说太具体，而不够数学，也就是说不够抽象。因此我们最后换用一个正牌的数学术语——变换，来描述这个事情。这样一说，大家就应该明白了，所谓变换，其实就是空间里从一个点（元素/对象）到另一个点（元素/对象）的跃迁。比如说，拓扑变换，就是在拓扑空间里从一个点到另一个点的跃迁。再比如说，仿射变换，就是在仿射空间里从一个点到另一个点的跃迁。附带说一下，这个仿射空间跟向量空间是亲兄弟。做计算机图形学的朋友都知道，尽管描述一个三维对象只需要三维向量，但所有的计算机图形学变换矩阵都是4 x 4的。说其原因，很多书上都写着“为了使用中方便”，这在我看来简直就是企图蒙混过关。真正的原因，是因为在计算机图形学里应用的图形变换，实际上是在仿射空间而不是向量空间中进行的。想想看，在向量空间里相一个向量平行移动以后仍是相同的那个向量，而现实世界等长的两个平行线段当然不能被认为同一个东西，所以计算机图形学的生存空间实际上是仿射空间。而仿射变换的矩阵表示根本就是4 x 4的。又扯远了，有兴趣的读者可以去看《计算机图形学——几何工具算法详解》。 一旦我们理解了“变换”这个概念，矩阵乘法就变成：“矩阵乘法是线性空间里的变换的描述。”到这里为止，我们终于得到了一个看上去比较数学的定义。不过还要多说几句。教材上一般是这么说的，在一个线性空间V里的一个线性变换T，当选定一组基之后，就可以表示为矩阵。线性变换的定义是很简单的，设有一种变换T，使得对于线性空间V中间任何两个不相同的对象x和y，以及任意实数a和b，有：T(ax + by) = aT(x) + bT(y)，那么就称T为线性变换。 定义都是这么写的，但是光看定义还得不到直觉的理解。线性变换究竟是一种什么样的变换？我们刚才说了，变换是从空间的一个点跃迁到另一个点，而线性变换，就是从一个线性空间V的某一个点跃迁到另一个线性空间V的另一个点的运动。这句话里蕴含着一层意思，就是说一个点不仅可以变换到同一个线性空间中的另一个点，而且可以变换到另一个线性空间中的另一个点去。不管你怎么变，只要变换前后都是线性空间中的对象，这个变换就一定是线性变换，也就一定可以用一个矩阵来描述。最后我们把矩阵乘法定义完善如下： “矩阵乘法是线性空间中的线性变换的一个描述。在一个线性空间中，只要我们选定一组基，那么对于任何一个线性变换，都能够用一个确定的矩阵乘法来加以描述。” 见下图： 对于矩阵乘法，主要是考察一个矩阵对另一个矩阵所起的变换作用。其作用的矩阵看作是动作矩阵，被作用的矩阵可以看作是由行或列向量构成的几何图形。同样，如果一连串的矩阵相乘，就是多次变换的叠加么。而矩阵左乘无非是把一个向量或一组向量（即另一个矩阵）进行伸缩或旋转。乘积的效果就是多个伸缩和旋转的叠加！比如S=ABCDEF会把所有的矩阵线性变化的作用力传递并积累下去，最终得到一个和作用力S。工业上的例子就是机器人的手臂，机械臂上的每个关节就是一个矩阵（比如可以是一个旋转矩阵），机械臂末端的位置或动作是所有关节运动的综合效果。这个综合效果可以用旋转矩阵的乘法得到。 本文是在整理孟岩老师的《理解矩阵》和任广千、胡翠芳老师的《线性代数的几何意义》基础上形成的，只是出于一种对数学的爱好！有兴趣的读者建议阅读原文。也欢迎下载《神奇的矩阵》和《神奇的矩阵第二季》最新版本了解更多有关线性代数和矩阵的知识。 再次感谢author阿狸，原文链接：https://www.zhihu.com/question/21351965/answer/176777987来源：知乎著作权归作者所有，转载请联系作者获得授权。]]></content>
      <categories>
        <category>Maths</category>
      </categories>
      <tags>
        <tag>theory</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[github+hexo+阿里云搭建个人博客]]></title>
    <url>%2F2017%2F09%2F30%2Fgithub%2Bhexo%2B%E9%98%BF%E9%87%8C%E4%BA%91%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%2F</url>
    <content type="text"><![CDATA[写在前面不得不说这真是一件开心的事情啊，趁着火热的功夫抓紧blog一波之前github+hexo已经搭建了一个个人静态博客，只不过是以gatsby2016.github.io的形式存在的，而且写的都是之前做硬件的时候写的几篇博客，后来一段时间没更新也就扔在那里了。结果这几天我女盆友自己电脑重装系统后又自己搭了个博客，让我十分感触；我想，年初辛辛苦苦搭建的博客怎么能放弃呢！而且，csdn的广告泰特么多了！所以，我又重新拾起来我的github博客了！ 具体步骤其实是没有什么具体步骤的，因为之前博客是怎么搭建起来我已经忘记了！很尴尬啊！这里只给大家指一条明路——百度github+hexo,然后就能搭建起来博客了，不行就删除了卸载了重新搭建，毕竟想要逼格是不可能有快捷键的！博客搭建起来后的第一件事，大概就是要换个nice的主题，因为最粗的主题简直，太大众了！怎么换，百度啊！hexo 主题，一堆就出来了，个人认为，解决问题的能力大学生一定是要有的！然后，血荐一个极简主义的主题——next，而且有官方文档，_config.yml里内容什么是什么，怎么配置，说得非常清楚！而且，还有个“常见问题”栏几乎解决了我所有痛点，你说气人不。链接。我已经爱上next的主题了！ 域名解析然而域名是gatsby2016.github.io，这是不是有点长了，而且比较绕啊，怎么解决，好办啊！阿里云上花钱买个域名，然后，解析域名，简单粗暴“新手引导设置”，然后，设置网站解析，填IP地址时，在windows下的cmd里ping -4 gatsby2016.github.io得到ipv4的ip地址，记住需要更改为自己的博客github名称啊，别ping到我这里了啊。。。ping -4 xxx.github.io是ping ipv4地址；ping -6 xxx.github.io是ping ipv6地址；然后有个来自xxx.xxx.xx.xxx的回复 中间有一串数字，ip就是它，到阿里云上，填进去，完事。（用ip4，ip6不可以）然后，到github上，放博客文件的repository里，进去，settings，然后，custom domin将域名填进去，完事收工。以此，直接可以通过新域名进入网站，yanng.cc是不是很刺激，刺激！精彩!本次域名绑定我主要参考这篇进行域名绑定，写的非常棒PS:top域名我买了一年，花了3块钱，不忍心不忍心。 后记一直都有个搭建自己博客有个家的打算，包括之前买域名也是有这个打算，但是苦于自己对wordpress不熟悉，服务器这一块更不熟悉，所以一直没有动手。现在好了，通过github+hexo搭建的博客用上了，而且换上了自己最喜欢的主题，如今又将买来的域名添加进去了，十分激动，exciting！当然，虽然我这么说十分简单，但是，就像那些去搭建一个云服务器版个人博客的他们一样，也许，他们也觉得那十分简单。 送给自己也送给你们：勇敢跨出去吧！后面怎么样，谁知道呢！定有办法的。 附录（增于2017.10.1）各位，更新一个问题，按照上述方式设置好域名后，正常可以访问，但是当再次hexo g生成静态网页后，hexo s保存到本地后，hexo d部署到github上后，问题出现了，再输入买的域名后，打开进不去了！为啥？因为我们只是在github上对域名进行了指向，本地的并没有任何操作，所以，需要在根目录的source文件夹里，新加一个CNAME的文件，不要后缀名，然后直接将不带www的域名（如nuistmi.top）添加进去，ok！接下来再部署后，就可以了！hexo d操作就不会clean custom domin了！完美~ 附录2各位，通告一个极好用的存储空间，七牛云，注册后新建一个空间，然后可以上传图片包括文件压缩包，可以说非常棒了，实名注册还有10g空间，足够用了啊。。当然，我没有打广告，好的东西就应该分享出来啊！]]></content>
      <categories>
        <category>Github</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>github</tag>
        <tag>阿里云</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python中对象序列化及tensorflow的命令行参数解析]]></title>
    <url>%2F2017%2F09%2F17%2Fpython%E4%B8%AD%E5%AF%B9%E8%B1%A1%E5%BA%8F%E5%88%97%E5%8C%96%E5%8F%8Atensorflow%E7%9A%84%E5%91%BD%E4%BB%A4%E8%A1%8C%E5%8F%82%E6%95%B0%E8%A7%A3%E6%9E%90%2F</url>
    <content type="text"><![CDATA[虽然这两天状态有些差，但是断断续续的看代码还是学到了一点东西，关于python中经常用到的对象序列化反序列化以及文件的读写操作；关于tensorflow中flags.py对命令行参数解析；以及对ubuntu系统安装的psensorCPU/GPU温度监控做一个总结，这种学到新东西的感觉真是圆满啊 序列化与反序列化做分类分割模型的训练时，都不可能通过一张张图片训练集直接操作的，大部分情况下是对图片的灰度值进行操作；灰度值数据存入文件中，我们不可能单纯通过file.read() 或者file.readline()读取，很慢还麻烦；更多时候，我们是通过cPickle.load()将文件数据反序列化后存入对象，然后对对象（list或者tuple或者dict）进行切片操作选取部分数据或者其他操作，这样大大提高速度。同样，当有些参数如weight/bias等需要保留至磁盘时，同样可以通过cPickle.dump()将数据对象序列化后存入文件。具体示例如下："""以下为将data对象序列化后存入test.txt文件"""&gt;&gt;&gt; import cPickle as cp # 导入cPickle模块&gt;&gt;&gt; import numpy as np # 导入numpy模块&gt;&gt;&gt; data = np.random.randn(2,4,4,3) # 随机初始化一个[2,4,4,3]的正态分布，可以当作是两张4*4像素的3 channels 图片的灰度值&gt;&gt;&gt; data # 显示该矩阵数据array([[[[-0.11245588, 0.46361267, -0.45861576], [ 1.08045987, -0.0912301 , 1.8826123 ], [-0.51326317, -1.2739741 , 0.96748474], [ 1.5503973 , 0.06333209, 0.89981012]], [[ 0.29903728, 1.9080101 , -1.06983402], [-0.9695859 , -1.47038233, 0.95528392], [-1.02076298, 1.28100534, 0.48845259], [ 1.04841292, -0.01142226, 0.10188068]], [[ 0.3883587 , 0.37042073, -0.0289424 ], [ 0.53593898, 1.71126688, 0.1262282 ], [ 0.92322512, 0.24725783, 0.8679894 ], [-0.36954911, 0.74044357, -0.44616424]], [[ 0.91045428, -0.24680013, -0.83070879], [-0.34094366, 1.7384725 , -0.33322178], [-0.75691129, -0.55346692, -0.06621144], [-0.18568211, -0.90673854, 0.25074295]]], [[[ 0.9069235 , 0.60922932, -0.29551031], [-1.34363931, 0.57989501, 0.18441912], [-0.29126267, -0.28990356, 0.68273307], [-0.13870684, -0.453214 , 1.02924452]], [[ 1.03025589, -0.20542393, 0.71855423], [-0.40312111, -0.09586122, 0.00740302], [ 0.54204646, 1.07633607, -0.23386144], [ 0.06289798, 0.02128197, -1.15631823]], [[ 1.12197064, -0.18423948, -0.61632741], [-1.38417149, -1.70282593, 0.42730971], [ 0.52566924, 0.24153171, 0.60613443], [-1.12691221, -1.67107417, -1.52135605]], [[-2.31999217, -0.57238608, -0.66098013], [ 1.72764717, -1.89471904, 0.36895925], [-1.67735118, 0.6438323 , -1.73313038], [-1.00737721, -0.60583479, 2.15804428]]]])&gt;&gt;&gt; file = open('./test.txt','wb') # 以二进制只写方式打开根目录下的test.txt文件，如果没有则创建一个test.txt文件&gt;&gt;&gt; cp.dump(data,file) # 调用cPickle中的dump()函数，将data序列化后传入test.txt文件&gt;&gt;&gt; file.close() # 数据写完后，关闭文件 经过上述操作后，根目录下出现一个test.txt文件，打开文件如下图2所示，由于以二进制方式写，所以通过文本方式打开为乱码；实际上，反序列化后查看对象，数据不会有任何问题图1 为在根目录创建的test.txt文件图2 为打开test.txt文件后显示内容 """以下为将文件中数据反序列化后存入对象并查看数据"""&gt;&gt;&gt; FILE = open('./test.txt','rb') # 以二进制只读方式打开根目录下的test.txt文件&gt;&gt;&gt; DATA = cp.load(FILE) # 调用cPickle的load函数反序列化test.txt文件中数据送入DATA对象&gt;&gt;&gt; FILE.close() #文件用完后，记得关闭&gt;&gt;&gt; DATA # 查看DATA对象数据array([[[[-0.11245588, 0.46361267, -0.45861576], [ 1.08045987, -0.0912301 , 1.8826123 ], [-0.51326317, -1.2739741 , 0.96748474], [ 1.5503973 , 0.06333209, 0.89981012]], [[ 0.29903728, 1.9080101 , -1.06983402], [-0.9695859 , -1.47038233, 0.95528392], [-1.02076298, 1.28100534, 0.48845259], [ 1.04841292, -0.01142226, 0.10188068]], [[ 0.3883587 , 0.37042073, -0.0289424 ], [ 0.53593898, 1.71126688, 0.1262282 ], [ 0.92322512, 0.24725783, 0.8679894 ], [-0.36954911, 0.74044357, -0.44616424]], [[ 0.91045428, -0.24680013, -0.83070879], [-0.34094366, 1.7384725 , -0.33322178], [-0.75691129, -0.55346692, -0.06621144], [-0.18568211, -0.90673854, 0.25074295]]], [[[ 0.9069235 , 0.60922932, -0.29551031], [-1.34363931, 0.57989501, 0.18441912], [-0.29126267, -0.28990356, 0.68273307], [-0.13870684, -0.453214 , 1.02924452]], [[ 1.03025589, -0.20542393, 0.71855423], [-0.40312111, -0.09586122, 0.00740302], [ 0.54204646, 1.07633607, -0.23386144], [ 0.06289798, 0.02128197, -1.15631823]], [[ 1.12197064, -0.18423948, -0.61632741], [-1.38417149, -1.70282593, 0.42730971], [ 0.52566924, 0.24153171, 0.60613443], [-1.12691221, -1.67107417, -1.52135605]], [[-2.31999217, -0.57238608, -0.66098013], [ 1.72764717, -1.89471904, 0.36895925], [-1.67735118, 0.6438323 , -1.73313038], [-1.00737721, -0.60583479, 2.15804428]]]])&gt;&gt;&gt; np.reshape(DATA,[-1,2,8,3]) # 对DATA数据进行reshape操作array([[[[-0.11245588, 0.46361267, -0.45861576], [ 1.08045987, -0.0912301 , 1.8826123 ], [-0.51326317, -1.2739741 , 0.96748474], [ 1.5503973 , 0.06333209, 0.89981012], [ 0.29903728, 1.9080101 , -1.06983402], [-0.9695859 , -1.47038233, 0.95528392], [-1.02076298, 1.28100534, 0.48845259], [ 1.04841292, -0.01142226, 0.10188068]], [[ 0.3883587 , 0.37042073, -0.0289424 ], [ 0.53593898, 1.71126688, 0.1262282 ], [ 0.92322512, 0.24725783, 0.8679894 ], [-0.36954911, 0.74044357, -0.44616424], [ 0.91045428, -0.24680013, -0.83070879], [-0.34094366, 1.7384725 , -0.33322178], [-0.75691129, -0.55346692, -0.06621144], [-0.18568211, -0.90673854, 0.25074295]]], [[[ 0.9069235 , 0.60922932, -0.29551031], [-1.34363931, 0.57989501, 0.18441912], [-0.29126267, -0.28990356, 0.68273307], [-0.13870684, -0.453214 , 1.02924452], [ 1.03025589, -0.20542393, 0.71855423], [-0.40312111, -0.09586122, 0.00740302], [ 0.54204646, 1.07633607, -0.23386144], [ 0.06289798, 0.02128197, -1.15631823]], [[ 1.12197064, -0.18423948, -0.61632741], [-1.38417149, -1.70282593, 0.42730971], [ 0.52566924, 0.24153171, 0.60613443], [-1.12691221, -1.67107417, -1.52135605], [-2.31999217, -0.57238608, -0.66098013], [ 1.72764717, -1.89471904, 0.36895925], [-1.67735118, 0.6438323 , -1.73313038], [-1.00737721, -0.60583479, 2.15804428]]]]) tensorflow中的命令行解析我们经常看到，在ubuntu的terminal下python xxx.py 时后面有时候会跟一些参数，如python xxx.py --name= &#39;yann&#39;，事实上，这种在命令行下给入参数的方式是通过tensorflow.flags的方式进行操作的。test_flag.py文件如下：#encoding:utf-8 """由于添加了中文注释，所以必须有上面那一句且要在中文注释之前""""""这是一个test_flag.py文件用于测试命令行参数解析"""import tensorflow as tfflags = tf.flags #flags是文件flags.py，用于处理命令行参数的解析工作FLAGS = flags.FLAGS #FLAGS是flags中一个对象，保存了解析后的命令行参数#以下为调用flags的DEFINE_string函数，变量为flag类型不可更改，但可通过命令行传入,变量保存在flags.FLAGS对象中flags.DEFINE_string("model_name","default","name") #model_name为变量名，default为值，name为文档说明，用于解释model_name的使用flags.DEFINE_string("model_url","www.baidu.com","url")def main(argv=None): # argv=None必须要，表示可以传入参数 print 'name:',FLAGS.model_name,'url:',FLAGS.model_url #调用命令行输入的参数,如果没有输入则为定义时默认参数if __name__ == "__main__": tf.app.run() #解析命令行参数，调用main函数 main(sys.argv)"""如果我们是直接执行某个.py文件的时候，该文件中那么"__name__ == '__main__'"是True,则执行下面语句；但是我们如果从另外一个.py文件通过import导入该文件的时候，这时__name__的值就是我们这个py文件的名字而不是__main__。这个功能还有一个用处：调试代码的时候，在if __name__ == '__main__':中加入一些我们的调试代码，我们可以让外部模块调用的时候不执行我们的调试代码，但是如果我们想排查问题的时候，直接执行该模块文件，调试代码能够正常运行！""" 在terminal下执行：yann@Y:~$ python test_flag.py name: default url: www.baidu.comyann@Y:~$ python test_flag.py --model_name=&apos;yann&apos; --model_url=&apos;www.bing.com&apos;name: yann url: www.bing.com 具体实施还是看代码注释，解释比较清楚了。不懂的东西就要多看两遍。关于tensorflow中命令行参数解析就是这样，其实可以理解为c语言中，# define name &#39;yann&#39; psensor温度监控安装安装psensor不仅仅是在安装psensor，还有m-sensors和hddtemp，各种硬件的温度监控包；安装时在进行sudo sensors-detect时有询问的一律yes就可以；最后全部完成最好重启一下系统。yann@Y:~$ sudo apt-get install lm-sensors hddtempyann@Y:~$ sudo sensors-detectyann@Y:~$ sudo service module-init-tools startyann@Y:~$ sudo apt-get install psensor OK，今天的总结就这些；状态不好的时候就要耐住性子看东西，很快状态就好了！]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python面向对象编程]]></title>
    <url>%2F2017%2F09%2F16%2Fpython%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%E7%BC%96%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[限制访问类成员类成员中，变量前面加上__(两个下划线)，这时变量变成私有变量；只有内部可以访问，而外部代码无法直接读取或者更改，这样以确保外部代码与内部数据的无关性。但是，可以通过定义内部类方法的方式，对内部数据进行获取和更改，举例如下："""以下为不限制外部代码对类成员的访问"""&gt;&gt;&gt; class Student():... def __init__(self,name,age):... self.name = name # name为公有变量... self.age = age # age为公有变量... &gt;&gt;&gt; a = Student('yann',23) #实例化一个Student类&gt;&gt;&gt; a.name # 外部代码直接获取实例a的内部数据'yann'&gt;&gt;&gt; a.age23&gt;&gt;&gt; a.name = 'xu' #外部代码直接更改实例a的内部数据&gt;&gt;&gt; a.age = 20&gt;&gt;&gt; a.name'xu'&gt;&gt;&gt; a.age20 """以下为限制对类成员的访问，通过限制外部代码访问内部数据，使代码更加健壮"""&gt;&gt;&gt; class Student():... def __init__(self,name,age):... self.__name = name... self.__age = age... def print_inform(self): # 定义print_inform()方法... print self.__name... print self.__age... def set_name(self,name): # 定义set_name()方法... self.__name = name... def set_age(self,age): # 定义set_age()方法... self.__age = age... &gt;&gt;&gt; x = Student('yann',23) # 实例化Student&gt;&gt;&gt; x.__name #试图通过外部代码直接获取x的私有变量__nameTraceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;AttributeError: Student instance has no attribute '__name'&gt;&gt;&gt; x.__age #试图通过外部代码直接获取x的私有变量__ageTraceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;AttributeError: Student instance has no attribute '__age'&gt;&gt;&gt; x.print_inform() # 只能通过对象内部print_inform()方法获取数据yann23&gt;&gt;&gt; x.__name = 'xu' # 试图通过外部代码直接更改私有变量&gt;&gt;&gt; x.__age = 20&gt;&gt;&gt; x.print_inform() # 通过内部方法查看数据，并未改变，说明外部代码并不能查看内部数据yann23&gt;&gt;&gt; x.set_name('xu') # 通过内部方法更改变量 __name&gt;&gt;&gt; x.print_inform()xu23 总结以前用C语言的时候，会自定义很多子函数，然后需要的时候就调用子函数，面向过程就是这样一步步来；但python是面向对象，通过对象本身带有的属性去解决问题，这种属性包含了数据和方法；而python中类就是自定义的一种对象类型，类是抽象的，实例才是类的具体。 就先这些了。python就到这里。]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python中基本简洁函数笔记]]></title>
    <url>%2F2017%2F09%2F13%2Fpython%E4%B8%AD%E5%9F%BA%E6%9C%AC%E7%AE%80%E6%B4%81%E5%87%BD%E6%95%B0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[高阶函数变量可以指向函数，变量可以作为函数的参数传入，a=b,b=c,so a=c 因此，一个函数可以作为另一个函数的参数传入，这种函数就是高阶函数。&gt;&gt;&gt; def add(x,y,f):... return f(x) + f(y)... &gt;&gt;&gt; x = 5&gt;&gt;&gt; y = -3&gt;&gt;&gt; add(x,y,abs)8 map函数map()函数接收两个参数，第一个参数是函数，第二个参数是序列；执行操作是：将传入的函数应用于序列中每一个元素，结果返回一个list。&gt;&gt;&gt; test = [1,-2,3,-4,5,-6,7,-8,9,-10]&gt;&gt;&gt; map(abs,test)[1, 2, 3, 4, 5, 6, 7, 8, 9, 10] 当需要对某一序列中所有元素进行遍历式的函数操作时，可以使用map()函数。当然也可以使用列表生成式操作：&gt;&gt;&gt; [abs(n) for n in test][1, 2, 3, 4, 5, 6, 7, 8, 9, 10] reduce函数reduce()函数和map()一样接收两个分别是函数和序列的参数；但reduce接收的函数需要序列中的两个元素，然后将结果和下一个元素再次作为参数传入函数中进行累积操作，直至元素遍历完。&gt;&gt;&gt; test = [4,3,2,1]&gt;&gt;&gt; reduce(pow,test)4096&gt;&gt;&gt; pow(pow(pow(4,3),2),1)4096 map()和reduce()都是为了方便快速而应运而生的，毕竟python追求代码简洁高效性；如果用for语句也不是不可以。 filter函数filter()函数和map()一样接收两个参数，第一个为函数，第二个为序列，将函数应用至所有序列，如果返回值为False则删除该元素，返回值为True则保留。&gt;&gt;&gt; def pos(x):... if x&gt;0:... return True... else:... return False... &gt;&gt;&gt; test[1, -2, 3, -4, 5, -6, 7, -8, 9, -10]&gt;&gt;&gt; filter(pos,test)[1, 3, 5, 7, 9] sorted函数sorted()用于直接对序列进行排序；数字按照数字大小排序；字符串按照首字母的ASCII码大小排列；从小到大；当然，sorted可以传入函数用于更改排序方法；如从大到小或者忽略大小写。其中x.lower() 是将元素x的大写字符全部换为小写；x.upper() 则相反。&gt;&gt;&gt; x = ['a','A','bab','BAB']&gt;&gt;&gt; [n.lower() for n in x]#全部改变为小写['a', 'a', 'bab', 'bab']&gt;&gt;&gt; [n.upper() for n in x]#全部改变为大写['A', 'A', 'BAB', 'BAB']&gt;&gt;&gt;#以下为对字符串分别进行区分和不区分大小写排序&gt;&gt;&gt; x = ['a','Ba','cd','E']&gt;&gt;&gt; sorted(x)['Ba', 'E', 'a', 'cd']&gt;&gt;&gt; def low(x,y):... x1= x.lower()... y1= y.lower()... if x1 &lt; y1:... return -1... elif x1 &gt; y1:... return 1... else:... return 0... &gt;&gt;&gt; sorted(x,low)['a', 'Ba', 'cd', 'E']&gt;&gt;&gt;# 因为low函数没有改变输入的元素值，所以只在排序上方式有变化元素未改变 匿名函数lambdalambda表示匿名函数，其实lambda还是python为了追求代码简洁性而出的关键字，通过一个关键字lambda，省去了函数定义时需要的函数名以及def和return关键字。不过尽管其简洁，但并不能定义过多表达式，只能定义一个表达式；如：&gt;&gt;&gt; def function(x):... return x*x... &gt;&gt;&gt; function(5)25&gt;&gt;&gt; function = lambda x: x*x # 通过lambda定义一个函数，function变量指向该函数&gt;&gt;&gt; function(5)25 偏函数functools.partial偏函数实际上就是借助一个module，把某一函数的默认值更改后确定下来从而方便调用，举个例子就很清楚了：&gt;&gt;&gt; int('100',base=2) #权为2，2的2次方*1+2的1次方*0+2的0次方*0"""4&gt;&gt;&gt; int('100',base=4) #权为4，4的2次方*1+4的1次方*0+4的0次方*016&gt;&gt;&gt; int('100',base=8) #权为8，8的2次方*1+8的1次方*0+8的0次方*064&gt;&gt;&gt; int('100',base=16) #权为16，16的2次方*1+16的1次方*0+16的0次方*0256&gt;&gt;&gt; int('100',base=10) #权为10，10的2次方*1+10的1次方*0+10的0次方*0100&gt;&gt;&gt; import functools &gt;&gt;&gt; a = functools.partial(int,base=16)&gt;&gt;&gt; a('100') """此时相当于默认值base=16"""256 总结一下午系统学习这些函数其实感觉没有太大作用，一般情况这些函数还是很少接触，权当作为学习python过程的必要吧！有时候很多东西只有接触了才知道这个东西好不好哪里好需要花多少时间。听别人说什么什么好什么什么不好，自己永远抓不住关键点。这也就是实践与理论最大的区别啊！学习python的一点感悟。 python学习来自廖雪峰老师 &gt;&gt;&gt;这里]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python中序列及用户输入]]></title>
    <url>%2F2017%2F09%2F12%2Fpython%E4%B8%AD%E5%BA%8F%E5%88%97%E5%8F%8A%E7%94%A8%E6%88%B7%E8%BE%93%E5%85%A5%2F</url>
    <content type="text"><![CDATA[python的listlist是形如a=[&#39;name&#39;,&#39;age&#39;,&#39;senior&#39;] 这样的以[]表示的数组，list是有序可变的，——列表，可以添加删除更改获取其中元素； 元素索引python中索引是从0开始的，而且支持倒序获取；如’name’是a[0]；’senior’是a[2]同时也是a[-1]； 元素操作&gt;&gt;&gt; a.append(5) #在list末尾添加数字元素5；&gt;&gt;&gt; a.insert(2,'gatsby') #将字符串元素'gatsby'插入到索引为2的位置；&gt;&gt;&gt; a.pop() #删除list末尾的元素；&gt;&gt;&gt; a.pop(1) #删除list中索引为1的元素；&gt;&gt;&gt; a[0]='chaoyang' #将索引为0的元素更改为'chaoyang' 这时a == [&#39;chaoyang&#39;, &#39;gatsby&#39;, &#39;senior&#39;]当然，list中的元素也可以是一个list，如b = [3,&#39;test&#39;,a] 其中a是上面的list。 python的tupletuple是形如a = (&#39;name&#39;,&#39;age&#39;,&#39;senior&#39;) 这样以（）表示的数组，tuple是有序不可变的，——元组，虽然和list很相像，但是不能添加删除元素只能通过a[0] 索引方式获取元素；或者可以理解为，tuple只有读权限没有写权限；而list具有读写权限；这样来看tuple比list的安全性要高一些。当然tuple可以直接删除整个tuple。 因为tuple只有读权限，所以一个tuple一旦使用，其中元素必须在初始化时就确定下来；可以有多个元素可以是一个空元组也可以只有一个元素；值得注意的是只有一个元素时元素后须有逗号以防解释器认为这是一个数学公式或变量；&gt;&gt;&gt; a = ('name','age') # a是一个两元素的tuple&gt;&gt;&gt; b = () # b是一个空tuple&gt;&gt;&gt; c = ('name',) # c是一个一元素的tuple&gt;&gt;&gt; d = ('name') # d是一个字符串变量 tuple指向不变性tuple中的不可变指的是，每一个索引对应的位置不变，而不是具体的数值不变；如：若索引为1的对应位置为具体的数值或者字符串，则该元素不变；若对应位置为一个list，此时list中的元素可以添加删除更改，但其实索引指向的位置仍旧是没有改变的。 python的dictionarydictionary是形如a = {&#39;name&#39;:&#39;chaoyang&#39;,&#39;age&#39;:23,&#39;senior&#39;:&#39;forth&#39;}这样以{}表示的无序数组——字典；通过键-值对应存储数据，其中，值value才是需要的数据，键key只是由coder自定义的显式的“索引”；因此其最大的好处是通过key快速查找对应value； 引自www.liaoxuefeng.com—— 为什么dict查找速度这么快？因为dict的实现原理和查字典是一样的。假设字典包含了1万个汉字，我们要查某一个字，一个办法是把字典从第一页往后翻，直到找到我们想要的字为止，这种方法就是在list中查找元素的方法，list越大，查找越慢。第二种方法是先在字典的索引表里（比如部首表）查这个字对应的页码，然后直接翻到该页，找到这个字，无论找哪个字，这种查找速度都非常快，不会随着字典大小的增加而变慢。dict就是第二种实现方式，给定一个名字，比如’name’，dict在内部就可以直接计算出name对应的存放名字的“页码”，也就是’chaoyang’这个字符串存放的内存地址，直接取出来，所以速度非常快。 dictionary操作字典是具有读写操作的，可以添加删除更改获取其中键-值；另外，python提供了一种判断某个key是否存在的操作；操作如下：&gt;&gt;&gt; dict = &#123;'name':'chaoyang','age':23,'senior':'forth'&#125;&gt;&gt;&gt; dict['major'] = 'AI' # 添加一对键-值 'major':'AI'&gt;&gt;&gt; dict.pop('senior') #删除key'senior'及其值&gt;&gt;&gt; dict['age'] = 24 # 更改'age'的值为24&gt;&gt;&gt; dict['name'] # 获取'name'对应值&gt;&gt;&gt; 'major' in dict #判断键'major'是否存在于dict字典，是返回true 否返回false 值得一提的是，在字典中，键一般不能重复；如果键重复则默认以后一个键的值为对应，相当于更改了键的值。另外，键必须为不可变对象如数字字符串或者元组而不能是list这类具有读写权限的可变数组。 python的setset，集合，是和字典一样用{}界定，但是只有key而没有value的无序数组，形如a = {3,4} ,且set中每个元素都是唯一的，不可重复。和字典一样的是，其中元素必须是不可变对象，且无法像list那样通过索引直接获取元素。 set操作set（） 可以将list或者tuple转换成集合并过滤重复元素。另外可以添加删除元素，也可以像数学集合运算那样进行交集并集求解。&gt;&gt;&gt; a = &#123;3,4&#125; # 创建集合a&gt;&gt;&gt; b = [1,2,3,4] # 创建list b&gt;&gt;&gt; b_set = set(b) # 将list b 转化为集合b_set&gt;&gt;&gt; c = (1,2,2,4) # 创建 tuple c&gt;&gt;&gt; c_set = set(c) # 将tuple c 转化为集合c_set&gt;&gt;&gt; a.add(2) # 添加元素2至集合a&gt;&gt;&gt; a.remove(4) # 从集合a删除元素4&gt;&gt;&gt; x = a &amp; b_set # 求a和b_set的交集&gt;&gt;&gt; y = a | c_set # a和c_set的并集 关于序列总结list[] tuple() dictionary{} set{}四种序列方式，其中list和tuple都是有序的，所谓有序，就是可以直接通过索引的方式，如a[0]这样获取该索引指向的元素值；而像dictionary和set这样的无序序列，就无法通过索引获取；但是dictionary有键-值对，我个人认为其中键就像是自定义的“索引”，因此可以通过键获取对应的值。在使用中，list可读可写，操作性强，因此应用较多；tuple可读不能写，安全性高可作为封装使用；dictionary可用于通过键快速获取对应值；set特点则是元素唯一性及其可操作的并集交集。 （用户输入）输入输出其实是用户和解释器交互的一种直接方式，代码里有什么信息可以print；同样，代码需要什么信息可以通过input或raw_input读取用户输入。通过x = input() 或者x = raw_input() 的方式，用户输入信息存放至 x 变量中；当然为了交互性可以x = input(&quot;please give a input:&quot;) ，不至于再多写个print。注意两个不同的是，通过x = input() 的方式，数字可以直接输入，如果输入是字符或者字符串，需要用户显式地用引号说明，否则系统会报错的；如：&gt;&gt;&gt; x = input("give a input:")give a input:'chaoyang'&gt;&gt;&gt; y = input("give a input again:")give a input again:123&gt;&gt;&gt; x'chaoyang'&gt;&gt;&gt; y123 但是如果通过x = raw_input() 的方式，字符或字符串不需要引号说明，直接输入，包括数字也是；之所以可以这样，是因为raw_input()获得的数据全部以字符的方式存储；如：&gt;&gt;&gt; x = raw_input("give a input:")give a input:chaoyang&gt;&gt;&gt; y = raw_input("give a input again:")give a input again:123&gt;&gt;&gt; z = raw_input("give the third input:")give the third input:'test'&gt;&gt;&gt; x'chaoyang'&gt;&gt;&gt; y'123'&gt;&gt;&gt; z"'test'"]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一周疯狂装机史]]></title>
    <url>%2F2017%2F08%2F07%2F%E4%B8%80%E5%91%A8%E7%96%AF%E7%8B%82%E8%A3%85%E6%9C%BA%E5%8F%B2%2F</url>
    <content type="text"><![CDATA[以下是一个三天前对装机一窍不通的人的血泪记录史 Abstract三天以前，我还是个自由自在无忧无虑的青春少年～～ 而如今，我仍旧自由自在却不再无忧无虑，而经过三天装机的璀璨？？摧残？？，捋一把青春的小胡子，我好像就成了一个满面风霜的大叔。。。 Introduction作为一个已经将升学相关事宜搞定的三年级本科生，现在的事情无非就是按照导师安排reading coding 以及doing 。。然而，事情总是不会静水流深，也许哪天导师兴致来了就需要检验你的工作情况，甚至为了你的工作，会对你配置更好的工作环境，然后对你提出更严格的要求。这不，一个“秋高气爽”的空调房的下午，导师进来了，在与几个师兄沟通最近工作状况完成之后，看见你正认真边看资料边coding，考虑到大家最近做的不错，这小伙看起来也学得不错，给他工作电脑升个级吧！于是，随手就是一块GTX960显卡，并满怀期待地说:”有时间把显卡装上，先用一段时间，后期需要再上1080ti，接下来好好干。”，”行的老师”。 接下来，你的机子的人生就此改变了。 Related Process关于装显卡这件事，我其实是一点准备也没有的，甚至，作为一个电脑小白，我个人觉得，装显卡应该就和前段时间换硬盘是一样的吧！带着这种连枪都不磨的心理，我拆开了机箱，”师兄，这显卡怎么装啊？”角落传来一阵声音”不是有PCI插槽么，插上去就行，别忘了供电”。经过大约良久的摸索——“师兄，装不上啊，还有，哪有电源啊”这时，师兄过来了，简单看了一下，”哦，你这种机箱啊，电源没有插口，得换电源，而且你这功率320w也不够，和老师说换电源。” “哦行” 。隔天，电源到了，800大洋的海盗船CS750M，在短暂的暗喜之后，我再次打开机箱打算将电源换上，然而人生总不是一帆风顺四马平川六六大顺的，我再次发现了一个问题，gg这电源尺寸和我这hp大机尺寸不一样啊，，”师兄，我这个电源怎么不一样大啊”走过来的师兄看了看，”哦你这个hp还有点独啊，现在都是ATX尺寸了，你这个还这样，不行这样接两个螺丝吧，悬在那，我们都这样”。”好吧 我先这样放着，看看有没有合适尺寸的吧”，我一脸不情愿的表情。就在我专心在京东上浏览电源时，老师进来了，”小伙子怎么样电源装好了么”，”还没，它这个电源尺寸不一样”， “那显卡放着没问题吧”，”没问题可以的”。过了一会，另一个师兄过来了，看了一眼我的机子，”他这机子不行，我以前试过，连显卡都放不下”，这时我的脸很疼，”得换个机箱”，”行那买个机箱，你们给他看看什么机箱好”。又换机箱了，心里美滋滋的笑了。隔天，ATX的机箱到了，这次，我满怀期待的拆开老机箱，在经历了主板拿不下来散热器拿不下来等一系列问题之后，我终于将老机箱东西都拿下来了，开始装机，突然，我发现，这CPU散热器，老机箱有固定孔，新机箱竟然没有，怎么办怎么办，”师兄，你过来看看，散热器不好装啊”，”哎呦，还真不好弄”，”这样，我明天找个东西给你固定住后面，小伙别急”。第二天，师兄好像今天挺忙的。一整天我都把新旧机箱放在那，用着我自己的Lenovo，我笔记本电脑屏幕是真小啊。隔天，我等不及了，师兄还没回来，我得自己搞试试了，于是，我一不做二不休将hp的铁片硬生生给剪下来了。大约花费了一上午，我把散热器装上了。完美～～然后我要接电源线，power sw power led hdd led，safari一查，说主板上都有标记字，但是我怎么都没找到，无奈，我按照师兄电脑上的接线，重新把线一根根捋，终于算是对应上了。解决，完美～～然而，就在我的笑容还么有散去之际，新的问题出现了——这teme电源是能放上去，可是它这个线这么多，我这个主板就是6pin的电源接口啊，而且主板上这个也没有power supply control 的接线啊，神坑啊。。我问遍了实验室所有师兄，实验室所有师兄帮我查遍了网上所有资料，无果。甚至，我致电了电源厂家，他说，”市面上所有主板，全部20+4pin或者20+8pin，没有6pin”，加了微信发给他主板图，他说，”哦这个主板啊，那这个电源你用不了了” 。继而，我联系了hp售后，他说我机子还没过保。。。内心OS:”你能帮我装么，不能的话，给你个五星好评你走吧”。。我崩溃了，这就是常说的世事多磨难吧！未顷，师兄再次带来了希望，Lenovo的主板，14pin，可是还是要用转接线啊。。晚上，老师再次过来了，看了看，凝重地说，”不行就换主板，搞多久了还没搞好，把原来的全部再装回去”，内心os：老机箱都被我剪了。。。”品牌机很多东西都不兼容，那我去网上看看主板吧”，刚打开京东就要买b250，可是这次我犹豫了，买来真的不会再出问题了么，万一，CPU，这，，，保守起见，我看了看参数，LGA1151？这是什么东西？ safari一下，指适合的cpu针脚；紧接着，我去看了看我的CPU，特意拍给卖家，”老板，这个b250的板子，我这cpu能用吧？”，”不能，亲可以试下这款”，随后将b85的板子推荐给我。what！？ 不能？ 我这CPU不是1151？ safari，结果，1150，就差一只脚，好吧，那我找个1150吧，b85，华硕技嘉的板子b85没有自营，自营的只有华擎，搜遍了所有京东，最终买了自营华擎板。隔天，新的主板到了，反正我是对机箱构造都一清二楚了，三下五除二装好主板，插各种线，装好，没有散热器，没关系，先开机试试，能开再买散热器，激动人心的时刻到了，按下开机按钮的那一刻，我觉得我是一个战士，疲惫但却仍旧充满杀气的战士！灯亮了！屏幕亮了!进入ubuntu启动画面了！yes！关机关机，保护CPU安全撤退！刚刚，我已经买下了应该是整个装机过程的最后一步的CPU散热器。明天，告别敲敲打打，也许一切就要重回安宁了。 Discussion在这里，我讲述了我作为装机小白是如何一步步装机的，或者说，一步步走上这条路的；现在，我不敢自诩我是个装机高手，不能给你说出什么配件什么品牌最好什么性价比最高，也无法说出什么配件与什么搭配效果最好，但是，给我一堆配件我能装出一台机子，也能自己买主板买硬盘买电源组装一台机子。这就是我这三天学习到的。虽然这几天很疲倦但是很值得，花费的时间总是能够取得一定的效果这不正是我们的期待么？而且，我发现，带着问题去学习是最快的学习方式，也是最有效的。 Conclusion 不要对未知产生畏惧感 （大概的意思就是，不要怂就是干） 投入，才能把事情做好 问题驱动研究。而这正是我导师当前的理念 Acknowledgents 谢谢老师给我装显卡的机会，尽管我对此毫无了解但是仍旧给我极大信任；谢谢师兄们对我的帮助，这使我受益匪浅；谢谢商家客服以及售后支持；谢谢网上五花八门的资料，虽然有时使人困惑，但总体使人进步；以及谢谢其他在我装机过程中帮助我的一切。 Reference 网上有关装机资料以及京东]]></content>
      <categories>
        <category>life</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[git的repository笔记]]></title>
    <url>%2F2017%2F07%2F21%2Fgit%E7%9A%84repository%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[终于搞好了github，这里对两台电脑将本地库与github远程库的关联做一个总结性教程吧。这么做的目的，自己想捣鼓是一方面，主要还是想工作电脑和家用电脑能随时对代码文件保持一致性，而且随时能对代码的修改能有个清晰直观的了解。 基本配置：两台电脑 都是系统ubuntu14.04 所有操作都是在terminal下执行(另外，推荐liaoxuefeng.com 廖老师讲的git教程真的太好了) 首先，要确保系统装了git，直接在terminal下输入命令$ git --version，安装了会出现诸如git version 1.9.1这样的；若未安装会提示安装，就按照提示安转就行了$ sudo apt-get install git然后输入密码回车安装好之后，$ cd进入根目录，建立一个文件夹（最好是空的,不至于太乱）, 并且进入该目录$ mkdir gitwork$ cd gitwork 然后，初始化建立一个本地库$ git init 接着，通过在gitwork下新建一个文件$ vim test.md （这里说一句 vim没装就按照指示$ sudo apt-get install vim安装一下，如果提示因为什么依赖关系导致安装fail 极有可能是安装版本和依赖的vim-common版本不统一原因，可以先$ sudo apt-get remove vim-common卸载它依赖的这个版本 然后再$ sudo apt-get install vim让它自己装它依赖的软件，一般这种问题都这么解决，也许） vim 下怎么edit我就不说了 随便打个test内容保存退出就可以 接着，将gitwork目录下的文件加入本地库$ git add test.md 按理说是没有反应的，继续输入指令$ git commit -m &quot;create a new file named test.md&quot; 这时会出来什么找不到用户，没关系，没有就添加用户， 执行下列指令$ git config --global user.name &quot;xiaoming&quot;$ git config --global user.email &quot;xiaoming@gmail.com&quot; 其中，xiaoming以及xiaoming@gmail.com 是需要换成自己的github用户名和github登录邮箱的config好之后，再次执行$ git commit -m &quot;create a new file named test.md&quot;这一步，会出现诸如以下:[master 010e2a1] add some words and delete some words1 file changed, 3 insertions(+), 1 deletion(-) 以上，就算是本地库搞好了 接下来，搞本地库和远程库的关联 在这之前，需要先登录github，create a new repository，命名最好是gitwork，和本地库一致，（不一样的话应该也可以）（其他不要更改不然后面需要多一步操作），然后create repository接着，到terminal下$ cd确保进入根目录，为了生成ssh key用于github远程关联，具体操作：$ ssh-keygen -t rsa -C &quot;xiaoming@gmail.com&quot; 其中，邮箱换成自己的。然后一路回车，okk。接下来需要将ssh key pub添加至github中，这一步是相当关键的。先说一下我自己吧，之前因为不知道什么原因死活ssh key添加进入但就是无法推送到远程库，最后google的方法突然就行了，也是奇怪。但是我另一台机子就是按照我最初的方法做的而且成功了，所以，ubuntu装东西很陶醉。。进入正题上面一路回车之后，我们执行$ eval &quot;$(ssh-agent -s)&quot; 出现Agent pid 15924 然后，$ ssh-add ~/.ssh/id_rsa 出现以下：Identity added: /home/gatsby/.ssh/id_rsa (/home/gatsby/.ssh/id_rsa) （上面两步我没太懂 大概就是什么add ssh key to ssh-agent有需求的可以看这篇的讲解） 然后 通过terminal下安装xclip剪切板，用command将ssh的id_rsa.pub里的内容复制下来,命令如下 $ sudo apt-get install xclip$ xclip -sel clip &lt; ~/.ssh/id_rsa.pub 然后直接ctrl+v加入到github的settings下的ssh and gpg keys的new ssh key的key下，然后add ssh key确认按钮点一下，ok。 接下来，就是关联至远程库的命令$ git remote add origin git@github.com:xiaoming/gitwork.git (xiaoming换成自己的github名称)这时会出现什么我忘记了 大概命令不输入错误是不会有问题的吧然后$ git push -u origin master 出现诸如：Counting objects: 8, done.Delta compression using up to 8 threads.Compressing objects: 100% (3/3), done.Writing objects: 100% (7/7), 620 bytes | 0 bytes/s, done.Total 7 (delta 1), reused 0 (delta 0)remote: Resolving deltas: 100% (1/1), done.To git@github.com:gatsby2016/gitwork.git 38f95ed..cf56b61 master -&gt; master 一堆，就可以了。如果出现什么：To git@github.com:gatsby2016/gitwork.git ! [rejected] master -&gt; master (fetch first)error: 无法推送一些引用到 &apos;git@github.com:gatsby2016/gitwork.git&apos;提示：更新被拒绝，因为远程版本库包含您本地尚不存在的提交。这通常是因为另外提示：一个版本库已向该引用进行了推送。再次推送前，您可能需要先整合远程变更提示：（如 &apos;git pull ...&apos;）。提示：详见 &apos;git push --help&apos; 中的 &apos;Note about fast-forwards&apos; 小节。 一堆，说明你在github远程库的gitwork上做了改动，导致本地库和远程库不一致，这时$ git pull origin master先将远程库数据拉下来就行。当出现诸如来自 github.com:gatsby2016/gitwork*branch master -&gt; FETCH_HEAD已经是最新的！Merge made by the &apos;recursive&apos; strategy. 一堆，就可以了。 然后可以更改文件，提交，或者对其他电脑推到远程库的数据拉下来再提交都可以。 以上就是关联本地库和远程库以及推送的基本总结，鄙人还是菜鸟，有问题还是多baidu多google。 关于git其他操作可以看廖老师的网站，liaoxuefeng.com 干货满满啊！]]></content>
      <categories>
        <category>Github</category>
      </categories>
      <tags>
        <tag>github</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[蓝牙遥控船存记]]></title>
    <url>%2F2017%2F02%2F24%2F%E8%93%9D%E7%89%99%E9%81%A5%E6%8E%A7%E8%88%B9%E5%AD%98%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[费时三天的蓝牙遥控船基本告一段落了。原本想做个小车，无奈风力不足，万向轮不合适的原因，加之自己不想耗费太多时间在机械结构的处理上，所以决定到此为止。 设计耗材 89C51最小系统板一个 HC-05蓝牙模块一个 712空心杯电机两个 S9013三极管四个 USB-TO-TTL模块一个 杜邦线若干 万能板9cm 15cm、5cm 9cm各一块 空纯牛奶盒三个 （小的移动电源一个） 实现功能 数字0：停止 数字1：启动 数字2：加速 数字3：减速 数字4：左转 数字5：右转 数字6：加大转向幅度 数字7：降低转向幅度 整体设计思路手机通过蓝牙发送不同数字（01234567），蓝牙模块接收到数据后分析并对应不同动作指令使51单片机P1.0和P1.1两端口产生模拟PWM信号（采用简单延时解决）控制三极管开关从而控制电机转动，通过同时对两端口PWM占空比的调整来改变小船直行的速度；通过对两端口占空比分别调整，采用差速法实现左转右转；另外，左转右转的幅度调节单独由发送数字决定。硬件设计上，通过在大的万能板底部粘贴空牛奶盒实现基本船体搭建；船板上放置各模块，小万能板竖直粘贴在船尾，电机旋桨平行小的万能板垂直大的万能板。（本设计采用旋桨悬空的方式避免直接与水体接触，这也是为何开头处提到风力不足的原因）另外，硬件上一大难点为三极管电路。因为51单片机输出电流极其微弱，高电平输出仅不到10mA，所以必须用到三极管放大以满足电机电流需要；该处各采用两个NPN三极管搭建电流放大电路（此处说的电流放大实际上是三极管的开关状态）。 另外，采用双H桥电路可以实现电机反转，图中分别对AD和BC分别控制开关，实现电机反向。感兴趣可以尝试。 （因为博主电路设计能力极其薄弱，在这个三极管放大电路设计上，博主花了整个设计的一半时间，深为惭愧）软件设计上，对蓝牙接受到的数据进行简单的switch判断，对应不同动作，其中直行、左转、右转都使用一个函数，但以函数的第二个参数决定。 具体代码实现#include&lt;reg51.h&gt;//全局变量及子函数声明****************************void UsartConfiguration();void delay1ms(unsigned int x);void moving(unsigned int bili,unsigned int fangxiang);unsigned int value=0,extra=1,FLAG=1;//端口定义**************************************sbit L1=P1^0;sbit L2=P1^1;//主函数*****************************************void main()&#123; unsigned int T=50; L1=0;L2=0; UsartConfiguration(); while(1) &#123; FLAG=1; switch(value) &#123; case 0:L1=0;L2=0;T=50;break; case 1:while(FLAG)&#123;moving(T,2);&#125;break; case 2:T=T+5;while(FLAG)&#123;moving(T,2);&#125;break; case 3:T=T-5;while(FLAG)&#123;moving(T,2);&#125;break; case 4:while(FLAG)&#123;moving(T,0);&#125;break; case 5:while(FLAG)&#123;moving(T,1);&#125;break; default: break; &#125; &#125;&#125;//小船移动函数**************************************void moving(unsigned int bili,unsigned int fangxiang)&#123; L1=1; L2=1; delay1ms(bili); if(fangxiang == 0) &#123; L1=1; L2=0; delay1ms(extra*10); L1=0; L2=0; delay1ms(100-extra*10-bili); &#125; else if(fangxiang == 1) &#123; L1=0; L2=1; delay1ms(extra*10); L1=0; L2=0; delay1ms(100-extra*10-bili); &#125; else &#123; L1=0; L2=0; delay1ms(100-bili); &#125;&#125;//延时函数**************************************void delay1ms(unsigned int x)&#123;unsigned int i,j;for(i=0;i&lt;x;i++) for(j=0;j&lt;120;j++);&#125;//串口配置**************************************void UsartConfiguration()&#123; SCON=0X50; //设置为工作方式1 TMOD=0X20; //设置计数器工作方式2 TH1=0XFD; //9600 TL1=0XFD; ES=1; //打开接收中断 EA=1; //打开总中断 TR1=1; //打开计数器&#125;//串口中断*****************************************void Usart() interrupt 4&#123; unsigned char Data; Data=SBUF; //出去接收到的数据 RI = 0; //清除接收中断标志位 FLAG=0; switch(Data) &#123; case 0x00: value=0;break;//停止 case 0x01: value=1;break;//启动 case 0x02: value=2;break;//加速 case 0x03: value=3;break;//减速 case 0x04: value=4;break;//左转 case 0x05: value=5;break;//右转 case 0x06: extra++;break;//幅度加 case 0x07: extra--;break;//幅度减 default: break; &#125;&#125; 以上。写于2017年2月24日]]></content>
      <categories>
        <category>Hardware</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[电机调试笔记]]></title>
    <url>%2F2017%2F02%2F19%2F%E7%94%B5%E6%9C%BA%E8%B0%83%E8%AF%95%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[今日总结： 电机调试总体思路，stm32端口推挽输出电平，高电平3.3v,通过sn7406n反相器,将单片机高电平翻转为0，低电平翻转为5V后，对电调进行初始化，后续对电调给pwm信号控制转速。 型号 电机：FMS模型飞机配件70mm12叶涵道动力组 KV2750电机 电调：FMS捕食者70A航模电调5A开关BEC配置无刷电调固定翼 涵道飞机配件 电调配置参数：PWM信号300HZ标准5V 最大油门2.0ms（占空比60%），最小油门1.0ms（占空比30%），注意共地问题 ○初始化过程 首先给定最大油门，然后对电调供电，在电机连续四次滴滴两声时间内，按下按键降至最小油门，这时候马达也会“滴滴”两声响，然后又一声音乐声，这时表明电调已获取您的发射机油门信号范围；此时表示初始化已完成。电调断电。 ○设置速度过程 首先给定最小油门，然后对电调供电，当电调第一次通电时，会连续发出两组可听见的声音，说明电调进入工作的状态，第一组声音表示与电调相连接的Lipo电池组中电池的数量（三声滴滴响表示有三节电池，四声滴滴响表示有四节电池），第二组声音表示刹车的状态（一声滴滴响表示刹车为开，二声滴滴响表示刹车为关）然后按键设置提高油门，这时候电 机开始转动，提高油门电机速度相应提高；注意，在此过程中，若电机吹出的风往两边发散，可调整电机与电调连线顺序，以此改变旋转方向。 疑惑： 32单片机端口开漏输出外接上拉电阻至5V后，使端口输出高电平，这时候电平是否为5V。 2017年3月13日解决疑惑开漏输出只能输出低电平，接上拉电阻时才能输出高电平，而高电平的实际电压由上拉电源电压决定，因此，上拉至5V时，由于电阻本身分压原因，高电平电压无法达到5v 以上。写于2017年2月19日]]></content>
      <categories>
        <category>Hardware</category>
      </categories>
      <tags>
        <tag>电调</tag>
        <tag>stm32</tag>
      </tags>
  </entry>
</search>
